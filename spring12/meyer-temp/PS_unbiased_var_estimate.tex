        \problemdata       % Takes 5 *mandatory* arguments
        {variance-6}             % latex-friendly label for the prob.
        {variance}               % The topic of the problem content.
        {Velleman}               % Source (if known)
        {S98 PS11-7; F97 PS11-8} % Usage (list ones you are aware of).
        {Theory Pig, S02}        % Last revision info (author, date).
% fall 01, problem set 12, problem 4

\begin{problem}
{\bf An Unbiased Estimator}

Suppose we are trying to estimate some physical parameter $p$.  When
we run our experiments and process the results, we obtain an estimator
of $p$, call it $p_e$.  But if our experiments are probabilistic, then
$p_e$ itself is a random variable which has a pdf over some range of
values.  We call the random variable $p_e$ an \emph{unbiased}
estimator if $\expect{p_e}=p$.

For example, say we are trying to estimate the height, $h$, of Green
Hall.  However, each of our measurements has some noise that is, say,
Gaussian with zero mean.  So each measurement can be viewed as a
sample from a random variable $X$.  The expected value of each
measurement is thus $\expect{X} = h$, since the probabilistic noise
has zero mean.  Then, given $n$ independent trials, $x_1, ..., x_n$,
an unbiased estimator for the height of Green Hall would be
$$h_e = \frac{x_1+...+x_n}{n},$$
since
$$\expect{h_e} = \expect{\frac{x_1+...+x_n}{n}} = \frac{\expect{x_1} + ... + \expect{x_n}}{n} = \expect{x_1}= h.$$

Now say we take $n$ independent observations of a random variable $Y$.  Let 
the true (but unknown) variance of $Y$ be $\variance{Y} = \sigma^2$.
Then (see section 6.4 in the
\href{http://theory.lcs.mit.edu/classes/6.042/fall01/lectures/l13.pdf}{notes}),
we can define the following estimator $\sigma_e^2$ for $\variance{Y}$ 
using the data from our observations:

$$\sigma_e^2 =  \frac{y_1^2 + y_2^2 + \ldots + y_n^2}{n} -
                \left(\frac{y_1 + y_2 + \ldots + y_n}{n}\right)^2.$$

%\begin{eqnarray*}
%\variance{Y}
%%        & = & \expect{Y^2} - \expectsq{Y} \\
%%        & \approx & \frac{y_1^2 + y_2^2 + \ldots + y_n^2}{n} -
%                \left(\frac{y_1 + y_2 + \ldots + y_n}{n}\right)^2.
%\end{eqnarray*}

Is this an unbiased estimator of the variance?   In
other words, is $\expect{\sigma_e^2} = \sigma^2$?   If not, can you
suggest how to modify this estimator to make it unbiased?

\solution{

Let $\sigma^2=\variance{X}, \mu=\expect{X}$.  Then our estimator
$\sigma^2_e$ is given by
\begin{eqnarray*}
\sigma^2_e&=&\frac{\sum
  y_i^2}{n} - \left(\frac{\sum y_i}{n}\right)^2\\
\expect{\sigma^2_e}&=&\expect{\frac{\sum
  y_i^2}{n} - \left(\frac{\sum y_i}{n}\right)^2}\\
&=&\frac{\sum
  \expect{y_i^2}}{n} - \frac{\expect{(\sum y_i)^2}}{n^2}\\
&=&\frac{\sum(\sigma^2+\mu^2)}{n} - \frac{\variance{\sum
    y_i}+\expectsq{\sum y_i}}{n^2}\\ 
&=&\frac{n(\sigma^2+\mu^2)}{n} - \frac{n\sigma^2+n^2\mu^2}{n^2}\\
&=&\sigma^2\left(1-\frac{1}{n}\right)
\end{eqnarray*}
So this gives a biased estimator, but we can make it
unbiased simply by multiplying by $\frac{n}{n-1}$.
\begin{eqnarray*}
\expect{\frac{n\sigma^2_e}{n-1}}&=&\sigma^2
\end{eqnarray*}
}

\end{problem}
