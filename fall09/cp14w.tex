\documentclass[handout]{mcs}

\begin{document}

\inclassproblems{14, Wed.}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Problems start here
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\pinput{CP_size_of_sample_vs_population}
\pinput{CP_drug_confidence}
\pinput{CP_gallup_poll}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Problems end here
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%\instatements{\newpage}

\appendix

\subsection{Binomial Variables}
A random variable, $J$, is $(n,p)$-\emph{binomial} for
$n \in \naturals$ and $0 \leq p \leq 1$, if
\[
J = \sum_{k=1}^n H_k
\]
where $H_1,H_2,\dots,H_n$ are mutually independent indicator variables
with $\pr{H_i = 1} = p$ for all $i$.

Equivalently, $J$ is $(n,p)$-\emph{binomial} iff $\pdf_J$ has the
$(n,p)$-\emph{binomial distribution}:
\[
\pdf_J(k) \eqdef \binom{n}{k} p^k (1-p)^{n-k},
\]
for $0 \leq k \leq n$.


\subsection{Binomial Sampling}

\begin{theorem*}
Let $K_1, K_2, \dots$, be a sequence of mutually independent 0-1-valued
random variables with the same expectation, $p$, and let
\[
S_n \eqdef \sum_{i=1}^n K_i.
\]
Then, for $1/2 > \epsilon > 0$,
\begin{equation}\label{delta bound}
\pr{\abs{\frac{S_n}{n} - p} \geq \epsilon}
\leq 
\frac{1 + 2\epsilon}{2\epsilon} \cdot
        \frac{2^{-n (1 - H((1/2) - \epsilon))}}{\sqrt{2 \pi (1/4 - \epsilon^2) n}},
\end{equation}
\end{theorem*}
where $H(x)$ is the \term{entropy} function
\[
H(\alpha) \eqdef \alpha \log_2 \frac{1}{\alpha} +
                (1 - \alpha) \log_2 \frac{1}{1 - \alpha}.
\]

\end{document}
