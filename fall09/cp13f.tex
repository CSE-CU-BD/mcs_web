\documentclass[handout]{mcs}

\begin{document}

\inclassproblems{13, Fri.}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Problems start here
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\pinput{CP_bigger_number_game}

\pinput{CP_3_random_variables}

\pinput{CP_carnival_dice_fair}

\pinput{CP_sixteen_desks}

\pinput{CP_consecutive_coin_flips}

\pinput{CP_independent_variables}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Problems end here
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\instatements{\newpage}
\section{Random Variables}

\iffalse

\subsection*{Stirling's Approximation}
\[
n! \sim \sqrt{2\pi n}\paren{\frac{n}{e}}^n
\]
\fi

%\subsection{Random Variables}

A \term{random variable} over a given sample space, $\sspace$, is a total
function whose domain is $\sspace$.  A random variable, $R$ such that
$\range{R} \subseteq \reals$ is called a \term{real-valued random
  variable}.

The \term{probability density function (pdf)} for a random variable, $R$,
is the function $\pdf_R : \range{R} \to [0,1]$ defined by:
\[
\pdf_R(x) \eqdef \pr{R = x}.
\]

\iffalse

The \emph{cumulative distribution function}, $\cdf_R: \range{R} \to
[0,1]$, is defined by:
\[
\cdf_R(x) \eqdef \pr{R \le x}.
\]
\fi

For any event $A$, its \emph{indicator variable}, $I_A$, is the 0-1 valued
variable such that the event $[I_A = 1]$ is the same as the event $A$.  It
follows that $[I_A = 0]$ is the same as $\bar{A}$.

A random variable, $U$, is \emph{uniform} iff all its values are equally
likely.  That is
\[
\pr{U = a} = \pr{U=b}
\]
for all $a,b \in \range{U}$.   % In other words, its pdf is constant.

Random variables $R_0, R_1, \dots$ are \emph{mutually independent} iff
\[
\pr{\lgintersect_{i} [R_i = x_i]} = \prod_{i} \pr{R_i = x_i},
\]
for all $x_i \in \range{R_i}$.
They are \emph{$k$-wise
independent} iff $\set{R_i \suchthat i\in J}$ are mutually independent for
all subsets $J \subseteq \naturals$ with $\card{J} = k$.

\iffalse
\subsection*{Binomial Variables}
A random variable, $J$, is $(n,p)$-\emph{binomial} for
$n \in \naturals$ and $0 < p < 1$, if
\[
J = \sum_{k=1}^n H_k
\]
where $H_1,H_2,\dots,H_n$ are mutually independent indicator variables
with $\pr{H_i = 1} = p$ for all $i$.

Equivalently, $J$ is $(n,p)$-\emph{binomial} iff $\pdf_J$ has the
$(n,p)$-\emph{binomial distribution}:
\[
\pdf_J(k) \eqdef \binom{n}{k} p^k (1-p)^{n-k},
\]
for $0 \leq k \leq n$.



\subsection*{Binomial bounds}
If $J$ is an $(n,p)$-binomial variable, the following formula gives a
fairly tight upper bound on $\pdf_J$.
\begin{equation}\label{pdfapprox}
\pdf_J(\alpha n) \leq
        \frac{2^{n H(\alpha)}}{\sqrt{2 \pi \alpha (1 - \alpha) n}}
        \cdot p^{\alpha n} (1-p)^{(1 - \alpha) n}
\end{equation}
where $H$ is the entropy function,
\[
H(\alpha) \eqdef - \paren{\alpha\log_2 \alpha + (1-\alpha)\log_2 (1-\alpha)}.
\]
The bounding formula is also asymptotically equal to $\pdf_J$.


\begin{theorem*} [Binomial Sampling]
Let $B_1, B_2, \dots, B_n$, be a sequence of mutually independent
0-1-valued random variables with the same probability, $p$, of being equal
to 1, and let
\[
A \eqdef \frac{\sum_{i=1}^n B_i}{n}.
\]
Then, for any $\epsilon > 0$,
\begin{equation}\label{binsam}
\pr{\abs{A - p} \geq \epsilon}
\leq
\frac{1 - 2\epsilon}{2\epsilon} \cdot
 \frac{2^{-n (1- H((1/2) - \epsilon))}}{\sqrt{2 \pi (1/4 - \epsilon^2) n}}
\end{equation}
where $H$ is the entropy function,
\[
H(\alpha) \eqdef - \paren{\alpha\log_2 \alpha + (1-\alpha)\log_2 (1-\alpha)}.
\]
\end{theorem*}
\fi

\iffalse

\bparts

\ppart Show that if discrete random variables are $k$-wise independent
for $k>2$, they are also $(k-1)$-wise independent.

\solution{Let $X_1,\ldots X_{k-1},X_k$ be any $k$ of the $k$-wise
  independent random variables.  We wish to show that
  $\pr{X_1=i_1,\ldots,X_{k-1} = i_{k-1}} =
  \pr{X_1=i_1}\cdots\pr{X_{k-1} = i_{k-1}}$.  We have
  $\pr{X_1=i_1,\ldots,X_{k-1} = i_{k-1},X_k = i_k} =
  \pr{X_1=i_1}\cdots\pr{X_{k-1} = i_{k-1}}\pr{X_k = i_k}$.  Sum this
  over all values $i_k$ that $X_k$ can take on to get
  \[\pr{X_1=i_1,\ldots,X_{k-1} = i_{k-1}} = \sum_{i_k}
  \pr{X_1=i_1,\ldots,X_{k-1} = i_{k-1},X_k = i_k}\]
  \[ = \sum_{i_k}\pr{X_1=i_1}\cdots\pr{X_{k-1} = i_{k-1}}\pr{X_k =
    i_k}\]
  \[ = \pr{X_1=i_1}\cdots\pr{X_{k-1}\ = i_{k-1}} \sum_{i_k}\pr{X_k =
    i_k}\]
  \[ = \pr{X_1=i_1}\cdots\pr{X_{k-1} = i_{k-1}}.\]
}
\fi

\section{Expectation}
The \term{expected value} of a random variable $R$ defined on a sample
space, $\sspace$, is:
\[
\expect{R} = \sum_{w \in \sspace} R(w) \pr{w}
\]

\iffalse

A 0-1-valued random variable is called an \term{indicator variable}.  If
$I$ is an indicator, then
\[
\expect{I} = \pr{I=1}.
\]

Another helpful formula for expected values is:
\[
\expect{R} = \sum_{x \in \range{R}} x \cdot \pr{R = x}
\]
\fi

\subsection{Mean Time to Failure}

If a biased coin with probability, $p$, of
Heads is repeatedly flipped until a Head comes up, where the flips are
mutually independent, then the expected number of flips is $1/p$.

\subsection{Conditional Expectation}

The expected value of a random variable, $R$, \emph{given event $A$}, is
\[
\expcond{R}{A} \eqdef \sum_{x \in \range{R}} x \cdot \prcond{R = x}{A}
\]
So $\expect{R} = \expcond{R}{\sspace}$ where $\sspace$ is the sample space
for $R$.

\subsection{Law of Total Expectation}
Let $A_1,A_2,\dots$ be a partition of the sample space.  Then
\[
\expect{R} = \sum_i \expcond{R}{A_i} \pr{A_i}.
\]

\subsection{Linearity of Expectation}
For any random variables $R_1, \dots, R_k$ and constants $a_1, \dots, a_k
\in \reals$,
\[
\expect{\sum_{i=1}^k a_iR_i} = \sum_{i=1}^k a_i\expect{R_i}.
\]
(The $R_i$ do \emph{not} have to be independent.)

\subsection{Product of Expectations} If random
variables $R_1, R_2, \dots, R_k$ are \emph{mutually} independent, then
\[
\expect{\prod_{i=1}^k R_i} = \prod_{i=1}^k \expect{R_i}.
\]

\end{document}
