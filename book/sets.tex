\hyperdef{sets}{informal}{\chapter{Mathematical Data Types}}\label{sets_chap}

\section{Sets}

We've been assuming that the concepts of sets, sequences, and functions are
already familiar ones, and we've mentioned them repeatedly.  Now we'll do a
quick review of the definitions.

\begin{staffnotes}
Propositions of the sort we've considered so far are good for
reasoning about individual statements, but not so good for reasoning
about a collection of objects.  Let's first review a couple
mathematical tools for grouping objects and then extend our logical
language to cope with such collections.
\end{staffnotes}

Informally, a \term{set} is a bunch of objects, which are called the
\term{elements} of the set.  The elements of a set can be just about
anything: numbers, points in space, or even other sets.  The conventional
way to write down a set is to list the elements inside curly-braces.  For
example, here are some sets:

\[
\begin{array}{rcll}
%\naturals & = & \set{0, 1, 2, 3, \ldots} & \text{the} \text{nonnegative integers} \\
A & = & \set{\text{Alex}, \text{Tippy}, \text{Shells}, \text{Shadow}} & \text{dead pets} \\
B & = & \set{\text{red}, \text{blue}, \text{yellow}} & \text{primary colors} \\
C & = & \set{ \set{a, b}, \set{a, c}, \set{b, c}} & \text{a set of sets}
\end{array}
\]
This works fine for small finite sets.  Other sets might be defined by
indicating how to generate a list of them:
\begin{align*}
D & =  \set{1,2,4,8,16,\dots} & \text{the powers of 2}
\end{align*}

The order of elements is not significant, so $\set{x, y}$ and $\set{y, x}$
are the same set written two different ways.  Also, any object is, or is
not, an element of a given set ---there is no notion of an element
appearing more than once in a set.\footnote{It's not hard to develop a
notion of \term{multisets} in which elements can occur more than once, but
multisets are not ordinary sets.}  So writing $\set{x,x}$ is just
indicating the same thing twice, namely, that $x$ is in the set.  In
particular, $\set{x,x} = \set{x}$.

The expression $e \in S$ asserts that $e$ is an element of set $S$.  For
example, $32 \in D$ and $\text{blue} \in B$, but $\text{Tailspin}
\not\in A$ ---yet.

Sets are simple, flexible, and everywhere.  You'll find
some set mentioned in nearly every section of this text.

\subsection{Some Popular Sets}

Mathematicians have devised special symbols to represent some common
sets.

\begin{center}
\begin{tabular}{lll}
\textbf{symbol} & \textbf{set} & \textbf{elements} \\
\term{$\emptyset$} & the empty set & \text{none}\\
\term{$\naturals$} & nonnegative integers & $\set{0, 1, 2, 3, \ldots}$ \\
\term{$\integers$} & integers & $\set{\ldots, -3, -2, -1, 0, 1, 2, 3, \ldots}$ \\
\term{$\rationals$} & rational numbers & $\frac{1}{2},\ -\frac{5}{3},\ 16,\ \text{etc.}$ \\
\term{$\reals$} & real numbers & $\pi,\ e,\ -9,\ \sqrt{2},\ \text{etc.}$ \\
\term{$\complexes$} & complex numbers & $i,\ \frac{19}{2},\ \sqrt{2} - 2i,\ \text{etc.}$
\end{tabular}
\end{center}
A superscript ``$^+$'' restricts a set to its positive elements; for
example, \term{$\reals^+$} denotes the set of positive real numbers.  Similarly,
\term{$\reals^-$} denotes the set of negative reals.

\subsection{Comparing and Combining Sets}

The expression $S \subseteq T$ indicates that set $S$ is a \term{subset}
of set $T$, which means that every element of $S$ is also an element of
$T$ (it could be that $S=T$).  For example, $\naturals \subseteq
\mathbb{Z}$ and $\mathbb{Q} \subseteq
\reals$ (every rational number is a real number), but $\complexes
\not\subseteq \mathbb{Z}$ (not every complex number is an integer).

As a memory trick, notice that the \term{$\subseteq$} points to the
smaller set, just like a $\leq$ sign points to the smaller number.
Actually, this connection goes a little further: there is a symbol
\term{$\subset$} analogous to $<$.  Thus, $S \subset T$ means that $S$
is a subset of $T$, but the two are \emph{not} equal.  So $A \subseteq
A$, but $A \not\subset A$, for every set $A$.

There are several ways to combine sets.  Let's define a couple of sets for
use in examples:
\begin{align*}
X & \eqdef \set{1, 2, 3} \\
Y & \eqdef \set{2, 3, 4}
\end{align*}

\begin{itemize}

\item The \term{union} of sets $X$ and $Y$ (denoted $X$ \term{$\union$} $Y$)
contains all elements appearing in $X$ or $Y$ or both.  Thus, $X \union
Y = \set{1, 2, 3, 4}$.

\item The \term{intersection} of $X$ and $Y$ (denoted $X$
  \term{$\intersect$} $Y$) consists of all elements that appear in
  \textit{both} $X$ and $Y$.  So $X \intersect Y = \set{2, 3}$.

\item The \term{set difference} of $X$ and $Y$ (denoted $X$ \index{$-$,
    set difference}$-$ $Y$) consists of all elements that are in $X$, but not in $Y$.
  Therefore, $X - Y = \set{1}$ and $Y - X = \set{4}$.

\end{itemize}

\subsection{Complement of a Set}

Sometimes we are focused on a particular domain, $D$.  Then for any
subset, $A$, of $D$, we define \term{$\overline{A}$} to be the set of all
elements of $D$ \textit{not} in $A$.  That is, $\overline{A} \eqdef D-A$.
The set $\overline{A}$ is called the \term{complement} of $A$.

For example, when the domain we're working with is the real numbers,
the complement of the positive real numbers is the set of negative real
numbers together with zero.  That is,
\[
\overline{\reals^+} = \reals^- \union \set{0}.
\]

It can be helpful to rephrase properties of sets using complements.  For
example, two sets, $A$ and $B$, are said to be \term{disjoint} iff they
have no elements in common, that is, $A \intersect B = \emptyset$.  This
is the same as saying that $A$ is a subset of the complement of $B$, that
is, $A \subseteq \overline{B}$.

\subsection{Power Set}

The set of all the subsets of a set, $A$, is called the \term{power
  set}, \term{$\power(A)$}, of $A$.  So $B \in \power(A)$ iff $B
\subseteq A$.  For example, the elements of $\power( \set{1, 2})$ are
$\emptyset, \set{1}, \set{2}$ and $\set{1, 2}$.

More generally, if $A$ has $n$ elements, then there are $2^n$ sets in
$\power(A)$.  For this reason, some authors use the notation $2^A$ instead
of $\power(A)$.

\subsection{Set Builder Notation}

An important use of predicates is in \term*{set builder notation}.  We'll
often want to talk about sets that cannot be described very well by
listing the elements explicitly or by taking unions, intersections,
etc., of easily-described sets.  Set builder notation often comes to the
rescue.  The idea is to define a \textit{set} using a \textit{predicate};
in particular, the set consists of all values that make the predicate
true.  Here are some examples of set builder notation:

\begin{align*}
A & \eqdef \set{n \in \naturals \suchthat \text{$n$ is a prime and $n =
    4k+1$ for some integer $k$}} \\
B & \eqdef \set{x \in \reals \suchthat x^3 - 3 x + 1 > 0} \\
C & \eqdef \set{a + b i \in \complexes \suchthat a^2 + 2 b^2 \leq 1}
\end{align*}

The set $A$ consists of all nonnegative integers $n$ for which the
predicate
\begin{center}
``$n$ is a prime and $n = 4k+1$ for some integer $k$''
\end{center}
is true.  Thus, the smallest elements of $A$ are:
\[
5, 13, 17, 29, 37, 41, 53, 57, 61, 73, \ldots.
\]
Trying to indicate the set $A$ by listing these first few elements
wouldn't work very well; even after ten terms, the pattern is not
obvious!  Similarly, the set $B$ consists of all real numbers $x$ for
which the predicate
\[
x^3 - 3x + 1 > 0
\]
is true.  In this case, an explicit description of the set $B$ in
terms of intervals would require solving a cubic equation.  Finally,
set $C$ consists of all complex numbers $a + b i$ such that:
\[
a^2 + 2 b^2 \leq 1
\]
This is an oval-shaped region around the origin in the complex plane.

\subsection{Proving Set Equalities}

Two sets are defined to be equal if they contain the same elements.  That
is, $X = Y$ means that $z \in X$ if and only if $z \in Y$, for all
elements, $z$.  (This is actually the first of the ZFC axioms.)  So set
equalities can be formulated and proved as ``iff'' theorems.  For
example:

\begin{theorem}[\term{Distributive Law} for Sets]
Let $A$, $B$, and $C$ be sets.  Then:
\begin{equation}\label{set-distrib}
A \intersect (B \union C) = (A \intersect B) \union (A \intersect C)
\end{equation}
\end{theorem}

\begin{proof}
The equality~\eqref{set-distrib} is equivalent to the assertion that
\begin{equation}\label{set-distrib-z}
  z \in A \intersect (B \union C) \qiff z \in (A \intersect B)
  \union (A \intersect C)
\end{equation}
for all $z$.  Now we'll prove~\eqref{set-distrib-z} by a chain of iff's.

First we need a rule for distributing a propositional $\QAND$ operation
over an $\QOR$ operation.  It's easy to verify by truth-table that
\begin{lemma}\label{prop-distrib}
The propositional formula
\[
P \QAND (Q \QOR R)
\]
and
\[
(P \QAND Q) \QOR (P \QAND R)
\]
are equivalent.
\end{lemma}

Now we have
\begin{align*}
\lefteqn{z \in A \intersect (B \union C)}\\
& \qiff (z \in A) \QAND (z \in B \union C) & \text{(def of $\intersect$)}\\
& \qiff (z \in A) \QAND (z \in B \QOR z \in C)
                & \text{(def of $\union$)}\\
& \qiff (z \in A \QAND z \in B) \QOR (z \in A \QAND z \in C)
                & \text{(Lemma~\ref{prop-distrib})}\\
& \qiff (z \in A \intersect B) \QOR (z \in A \intersect C)
                & \text{(def of $\intersect$)}\\
& \qiff z \in (A \intersect B) \union (A \intersect C)
                & \text{(def of $\union$)}
\end{align*}

\end{proof}
  
\begin{problems}
\homeworkproblems
\pinput{PS_set_union}

\end{problems}

\section{Sequences}

Sets provide one way to group a collection of objects.  Another way is
in a \term{sequence}, which is a list of objects called \term{terms}
or \term{components}.  Short sequences are commonly described by
listing the elements between parentheses; for example, $(a, b, c)$ is
a sequence with three terms.

While both sets and sequences perform a gathering role, there are
several differences.
\begin{itemize}

\item The elements of a set are required to be distinct, but terms in a
sequence can be the same.  Thus, $(a, b, a)$ is a valid sequence of length
three, but $\set{a, b, a}$ is a set with two elements ---not three.

\item The terms in a sequence have a specified order, but the elements
of a set do not.  For example, $(a, b, c)$ and $(a, c, b)$ are
different sequences, but $\set{a, b, c}$ and $\set{a, c, b}$ are the
same set.

\item Texts differ on notation for the \term{empty sequence}; we use
  \term{$\lambda$} for the empty sequence.
\end{itemize}

The product operation is one link between sets and sequences.  A
\term{product of sets}, $S_1 \times S_2 \times \cdots \times S_n$, is a
new set consisting of all sequences where the first component is drawn
from $S_1$, the second from $S_2$, and so forth.  For example, $\naturals
\times \set{a,b}$ is the set of all pairs whose first element is a
nonnegative integer and whose second element is an $a$ or a $b$:
\[
\naturals \times \set{a,b}
    = \set{(0,a), (0,b), (1,a), (1,b), (2,a), (2, b), \dots}
\]
A product of $n$ copies of a set $S$ is denoted $S^n$.  For example,
$\set{0, 1}^3$ is the set of all $3$-bit sequences:
\[
\set{0, 1}^3 = \set{ (0,0,0), (0,0,1), (0,1,0), (0,1,1),
                     (1,0,0), (1,0,1), (1,1,0), (1,1,1) }
\]

\section{Functions}\label{funcsubsec}

A \term{function} assigns an element of one set, called the
\term{domain}, to elements of another set, called the \term{codomain}.
The notation
\[
f: A \to B
\]
indicates that $f$ is a function with domain, $A$, and codomain, $B$.  The
familiar notation ``$f(a) = b$'' indicates that $f$ assigns the element $b
\in B$ to $a$.  Here $b$ would be called the \term*{value} of $f$ at
\term*{argument} $a$.

Functions are often defined by formulas as in:
\[
f_1(x) \eqdef \frac{1}{x^2}
\]
where $x$ is a real-valued variable, or
\[
f_2(y,z) \eqdef y\mathtt{10}yz
\]
where $y$ and $z$ range over binary strings, or
\[
f_3(x, n) \eqdef \text{ the pair } (n, x)
\]
where $n$ ranges over the nonnegative integers.

A function with a finite domain could be specified by a table that shows
the value of the function at each element of the domain.  For example, a function
$f_4(P,Q)$ where $P$ and $Q$ are propositional variables is specified by:
\[\begin{array}{|cc|c|}
\hline
P & Q & f_4(P,Q)\\
\hline \true & \true & \true\\
\hline \true & \false & \false\\
\hline \false & \true & \true\\
\hline \false & \false & \true\\
\hline
\end{array}\]
Notice that $f_4$ could also have been described by a formula:
\[
f_4(P,Q)  \eqdef [P \QIMPLIES Q].
\]

A function might also be defined by a procedure for computing its value at
any element of its domain, or by some other kind of specification.  For
example, define $f_5(y)$ to be the length of a left to right search of the
bits in the binary string $y$ until a \texttt{1} appears, so
\begin{eqnarray*}
f_5(0010) & = &  3,\\
f_5(100)  & = & 1,\\
f_5(0000) & \text{is} & \text{undefined}.
\end{eqnarray*}

Notice that $f_5$ does not assign a value to any string of just \texttt{0}'s.
This illustrates an important fact about functions: they need not assign a
value to every element in the domain.  In fact this came up in our first
example $f_1(x)=1/x^2$, which does not assign a value to $0$.  So in
general, functions may be \term{partial functions}, meaning that there may be domain
elements for which the function is not defined.  If a function is defined
on every element of its domain, it is called a \term{total function}.

It's often useful to find the set of values a function takes when applied
to the elements in \emph{a set} of arguments.  So if $f:A \to B$, and $S$
is a subset of $A$, we define $f(S)$ to be the set of all the values that
$f$ takes when it is applied to elements of $S$.  That is,
\[
f(S) \eqdef \set{b \in B \suchthat f(s) = b \text{ for some } s
  \in S}.
\]
For example, if we let $[r,s]$ denote the interval from $r$ to $s$ on the
real line, then $f_1([1,2]) = [1/4,1]$.

For another example, let's take the ``search for a \texttt{1}''
function, $f_5$.  If we let $X$ be the set of binary words which
start with an even number of \texttt{0}'s followed by a
\texttt{1}, then $f_5(X)$ would be the odd nonnegative integers.

Applying $f$ to a set, $S$, of arguments is referred to as
\hyperdef{mapping}{pointwise}{``applying $f$ \idx{pointwise} to $S$''}, and the
set $f(S)$ is referred to as the \term{image} of $S$ under
$f$.\footnote{There is a picky distinction between the function $f$ which
  applies to elements of $A$ and the function which applies $f$ pointwise
  to subsets of $A$, because the domain of $f$ is $A$, while the domain of
  pointwise-$f$ is $\power(A)$.  It is usually clear from context whether
  $f$ or pointwise-$f$ is meant, so there is no harm in overloading the
  symbol $f$ in this way.}  The set of values that arise from applying $f$
to all possible arguments is called the \term{range} of $f$.  That is,
\[
\range{f} \eqdef f(\domain{f}).
\]
Some authors refer to the codomain as the range of a function, but they
shouldn't.  The distinction between the range and codomain will be
important in Sections~\ref{surj_sec} and~\ref{mappingrule_sec} when we
relate sizes of sets to properties of functions between
them.

\subsection{Function Composition}\label{func_compose_subsec}

Doing things step by step is a universal idea.  Taking a walk is a literal
example, but so is cooking from a recipe, executing a computer program,
evaluating a formula, and recovering from substance abuse.

Abstractly, taking a step amounts to applying a function, and going step
by step corresponds to applying functions one after the other.  This is
captured by the operation of \term{composing} functions.  Composing the
functions $f$ and $g$ means that first $f$ applied is to some argument,
$x$, to produce $f(x)$, and then $g$ is applied to that result to produce
$g(f(x))$.

\begin{definition}\label{func_compose_def}
  For functions $f:A \to B$ and $g:B \to C$, the \term{composition},
  $g \compose f$, of $g$ with $f$ is defined to be the function
  $h:A \to C$ defined by the rule:
\begin{displaymath}
(g \compose f)(x) = h(x) \eqdef g(f(x)),
\end{displaymath}
for all $x \in A$.
\end{definition}

Function composition is familiar as a basic concept from elementary
calculus, and it plays an equally basic role in discrete mathematics.

\section{Binary Relations}

\term{Relations} are another fundamental mathematical data type.  Equality
and ``less-than'' are very familiar examples of mathematical relations.
These are called \term{binary relations} because they apply to a pair
$(a,b)$ of objects; the equality relation holds for the pair when $a=b$,
and less-than holds when $a$ and $b$ are real numbers and $a < b$.

In this chapter we'll define some basic vocabulary and properties of binary
relations.

\hyperdef{func}{rel}{\section{Binary Relations and Functions}}
Binary relations are far more general than equality or less-than.
Here's the official definition:
\begin{definition}\label{reldef}
A \term{binary relation}, $R$, consists of a set, $A$, called
the \term{domain} of $R$, a set, $B$, called the \term{codomain} of $R$, and
a subset of $A \cross B$ called the \term{graph of $R$}.
\end{definition}

Notice that Definition~\ref{reldef} is exactly the same as the definition
in Section~\ref{funcsubsec} of a {\emph{function}}, except that it doesn't
require the functional condition that, for each domain element, $a$, there
is \emph{at most} one pair in the graph whose first coordinate is $a$.  So
a function is a special case of a binary relation.

A relation whose domain is $A$ and codomain is $B$ is said to be
``between $A$ and $B$'', or ``from $A$ to $B$.''  When the domain and
codomain are the same set, $A$, we simply say the \index{relation on a
set} relation is ``on $A$.''  It's common to use \idx{infix notation}
``$a \mrel{R} b$'' to mean that the pair $(a,b)$ is in the graph of
$R$.

For example, we can define an ``in-charge of'' relation, $T$, for MIT in
Spring '10 to have domain equal to the set, $F$, of names of the faculty
and codomain equal to all the set, $N$, of subject numbers in the current
catalogue.  The graph of $T$ contains precisely the pairs of the form
\[
(\ang{\text{instructor-name}}, \ang{\text{subject-num}})
\]
such that the faculty member named $\ang{\text{instructor-name}}$ is
in charge of the subject with number $\ang{\text{subject-num}}$ in Spring '10.
So $\graph{T}$ contains pairs like

\[\begin{array}{ll}
(\texttt{A. R. Meyer}, & \texttt{6.042}),\\
(\texttt{A. R. Meyer}, & \texttt{18.062}),\\
(\texttt{A. R. Meyer}, & \texttt{6.844}),\\
(\texttt{T. Leighton}, & \texttt{6.042}),\\
(\texttt{T. Leighton}, & \texttt{18.062}),\\
(\texttt{G, Freeman}, & \texttt{6.011}),\\
(\texttt{G, Freeman}, & \texttt{6.UAT}),\\
(\texttt{G. Freeman}, & \texttt{6.881})\\
(\texttt{G. Freeman}, & \texttt{6.882})\\
(\texttt{T. Eng},      & \texttt{6.UAT})\\
(\texttt{J. Guttag},  & \texttt{6.00})\\
\qquad \vdots
\end{array}\]

This is a surprisingly complicated relation: Meyer is in charge of
subjects with three numbers.  Leighton is also in charge of subjects with
two of these three numbers ---because the same subject, Mathematics for
Computer Science, has two numbers: 6.042 and 18.062, and Meyer and
Leighton are co-in-charge of the subject.  Freeman is in-charge of even
more subjects numbers (around 20), since as Department Education Officer,
he is in charge of whole blocks of special subject numbers.  Some
subjects, like 6.844 and 6.00 have only one person in-charge.  Some
faculty, like Guttag, are in charge of only one subject number, and no one
else is co-in-charge of his subject, 6.00.

Some subjects in the codomain, $N$, do not appear in the list ---that is,
they are not an element of any of the pairs in the graph of $T$; these are
the Fall term only subjects.  Similarly, there are faculty in the domain,
$F$, who do not appear in the list because all their in-charge subjects
are Fall term only.

\section{Images and Inverse Images}

%drop the two-sided inverse-image, image notation.
%revise to use function-like notation $R(D)$ for image of $D \subseteq \domain{R}$
%explain $R(D)$: take the endpoints of all the arrows that cone from $D$.
%Define \inv{R} and get inverse images as \inv{R}(C) for $C \subseteq \codomain{R}$

The faculty in charge of 6.UAT in Spring '10 can be found by taking the
pairs of the form
\[
(\ang{\text{instructor-name}}, 6.UAT)
\]
in the graph of the teaching relation, $T$, and then just listing the left
hand sides of these pairs; these turn out to be just Eng and Freeman.

The introductory course 6 subjects have numbers that start with
\emph{6.0}\,.  So we can likewise find out all the instructors in-charge
of introductory course 6 subjects this term, by taking all the pairs of
the form $(\ang{\text{instructor-name}}, 6.0\dots)$ and list the left hand
sides of these pairs.  For example, from the part of the graph of $T$
shown above, we can see that Meyer, Leighton, Freeman, and Guttag are
in-charge of introductory subjects this term.

These are all examples of taking an \term{inverse image} of a set under a
relation.  If $R$ is a binary relation from $A$ to $B$, and $X$ is any
set, define the inverse image of $X$ under $R$, written simply as
$RX$ to be the set elements of $A$ that are related to something in $X$.

For example, let $D$ be the set of introductory course 6 subject numbers.
So $TD$, the inverse image of the set $D$ under the relation, $T$, is the
set of all faculty members in-charge of introductory course 6 subjects in
Spring '10.  Notice that in inverse image notation, $D$ gets written to
the right of $T$ because, to find the faculty members in $TD$, we're
looking pairs in the graph of $T$ whose right hand sides are subject
numbers in $D$.

Here's a concise definition of the inverse image of a set $X$ under
a relation, $R$:
\[
RX \eqdef \set{a \in A \suchthat aRx \text{ for some } x \in X}.
\]

Similarly, the \term{image} of a set $Y$ under $R$, written $YR$, is the
set of elements of the codomain, $B$, that are related to some element in
$Y$, namely,
\[
YR \eqdef \set{b \in B \suchthat yRb \text{ for some } y \in Y}.
\]

So, $\set{\text{A. Meyer}}T$ gives the subject numbers that Meyer is in
charge of in Spring '09.  In fact, $\set{\text{A. Meyer}}T = \set{6.042,
  18.062, 6.844}$.  Since the domain, $F$, is the set of all in-charge
faculty, $FT$ is exactly the set of \emph{all} Spring '09 subjects being
taught.  Similarly, $TN$ is the set of people in-charge of a Spring '09
subject.

It gets interesting when we write composite expressions mixing images,
inverse images and set operations.  For example, $(TD)T$ is the set of
Spring '09 subjects that have people in-charge who also are in-charge of
introductory subjects.  So $(TD)T - D$ are the advanced subjects with
someone in-charge who is also in-charge of an introductory subject.
Similarly, $TD \intersect T(N-D)$ is the set of faculty teaching both an
introductory \emph{and} an advanced subject in Spring '09.

\textbf{Warning:} When $R$ happens to be a function, the pointwise
application, $R(Y)$, of $R$ to a set $Y$ described in
Section~\ref{funcsubsec} is exactly the same as the image of $Y$ under
$R$.  That means that when $R$ is a function, $R(Y) = YR$ ---\emph{not}
$RY$.  Both notations are common in math texts, so you'll have to live
with the fact that they clash.  Sorry about that.

\section{Surjective and Injective Relations}\label{surj_sec}

There are a few properties of relations that will be useful when we take
up the topic of counting because they imply certain relations between the
\emph{sizes} of domains and codomains.  We say a binary relation $R : A
\to B$ is:

\begin{itemize}

\item \term{total} when every element of $A$ is assigned to some element of
  $B$; more concisely, $R$ is total iff $A=RB$.

\item \term{surjective} when every element of $B$ is mapped to \textit{at
least once}\footnote{
The names ``surjective'' and ``injective'' are unmemorable and
nondescriptive.  Some authors use the term \term{onto} for surjective and
\emph{one-to-one} for injective, which are shorter but arguably no more
memorable.}; more concisely, $R$ is surjective iff $AR=B$.

\item \term{injective} if every element of $B$ is mapped to \textit{at
most once}, and

\item \term{bijective} if $R$ is total, surjective, and injective
  \emph{function}.

\end{itemize}

Note that this definition of $R$ being total agrees with the definition in
Section~\ref{funcsubsec} when $R$ is a function.

If $R$ is a binary relation from $A$ to $B$, we define $AR$ to to be the
\emph{range} of $R$.  So a relation is surjective iff its range equals its
codomain.  Again, in the case that $R$ is a function, these definitions of
``range'' and ``total'' agree with the definitions in
Section~\ref{funcsubsec}.

\subsection{Relation Diagrams}
We can explain all these properties of a relation $R:A \to B$ in terms of
a diagram where all the elements of the domain, $A$, appear in one column
(a very long one if $A$ is infinite) and all the elements of the codomain,
$B$, appear in another column, and we draw an arrow from a point $a$ in
the first column to a point $b$ in the second column when $a$ is related
to $b$ by $R$.  For example, here are diagrams for two functions:

\begin{center}
\begin{tabular}{ccc}

\unitlength = 2pt
\begin{picture}(50,60)(-10,-5)
\thinlines
\put(-5,50){\makebox(0,0){$A$}}
  \put(35,50){\makebox(0,0){$B$}}
\put(-5,40){\makebox(0,0){a}}
  \put(0,40){\vector(1,0){28}}
  \put(35,40){\makebox(0,0){1}}
\put(-5,30){\makebox(0,0){b}}
  \put(0,30){\vector(3,-1){28}}
  \put(35,30){\makebox(0,0){2}}
\put(-5,20){\makebox(0,0){c}}
  \put(0,20){\vector(3,-1){28}}
  \put(35,20){\makebox(0,0){3}}
\put(-5,10){\makebox(0,0){d}}
  \put(0,10){\vector(3,2){28}}
  \put(35,10){\makebox(0,0){4}}
\put(-5,0){\makebox(0,0){e}}
  \put(0,0){\vector(3,2){28}}
\end{picture}

& \hspace{0.5in} &

\unitlength = 2pt
\begin{picture}(50,60)(-10,-5)
\thinlines
\put(-5,50){\makebox(0,0){$A$}}
  \put(35,50){\makebox(0,0){$B$}}
\put(-5,40){\makebox(0,0){a}}
  \put(0,40){\vector(1,0){28}}
  \put(35,40){\makebox(0,0){1}}
\put(-5,30){\makebox(0,0){b}}
  \put(0,30){\vector(3,-1){28}}
  \put(35,30){\makebox(0,0){2}}
\put(-5,20){\makebox(0,0){c}}
  \put(0,20){\vector(3,-2){28}}
  \put(35,20){\makebox(0,0){3}}
\put(-5,10){\makebox(0,0){d}}
  \put(0,10){\vector(3,2){28}}
  \put(35,10){\makebox(0,0){4}}
\put(35,0){\makebox(0,0){5}}
\end{picture}

\end{tabular}
\end{center}

Here is what the definitions say about such pictures:
\begin{itemize}

\item ``$R$ is a function'' means that every point in the domain column,
  $A$, has \emph{at most one arrow out of it}.

\item ``$R$ is total'' means that \emph{every} point in the $A$ column has
  \emph{at least one arrow out of it}.  So if $R$ is a function, being
  total really means every point in the $A$ column has
  \emph{exactly one arrow out of it}.

\item ``$R$ is surjective'' means that \emph{every} point in the codomain
  column, $B$, has \emph{at least one arrow into it}.

\item ``$R$ is injective'' means that every point in the codomain column,
  $B$, has \emph{at most one arrow into it}.

\item ``$R$ is bijective'' means that \emph{every} point in the $A$ column
      has exactly one arrow out of it, and \emph{every} point in the $B$ column
      has exactly one arrow into it.

\end{itemize}

So in the diagrams above, the relation on the left is a total, surjective
function (every element in the $A$ column has exactly one arrow out, and
every element in the $B$ column has at least one arrow in), but not
injective (element 3 has two arrows going into it).  The relation on the
right is a total, injective function (every element in the $A$ column has
exactly one arrow out, and every element in the $B$ column has at most one
arrow in), but not surjective (element 4 has no arrow going into it).

%Define $\inv{R}$ and explain with reversed arrows.  Deduce that that
%$R$ is total iff $\inv{R} is surjective, $R$ is function iff
%$\inv{R}$ is injective.

Notice that the arrows in a diagram for $R$ precisely correspond to the
pairs in the graph of $R$.  But $\graph{R}$ does not determine by itself
whether $R$ is total or surjective; we also need to know what the domain
is to determine if $R$ is total, and we need to know the codomain to tell
if it's surjective.
\begin{example}
  The function defined by the formula $1/x^2$ is total if its domain is
  $\reals^+$ but partial if its domain is some set of real numbers
  including 0.  It is bijective if its domain and codomain are both
  $\reals^+$, but neither injective nor surjective if its domain and
  codomain are both $\reals$.
\end{example}


\hyperdef{mapping}{rule}{\section{The Mapping Rule}}\label{mappingrule_sec}

The relational properties above are useful in figuring out the relative
sizes of domains and codomains.

If $A$ is a finite set, we let $\card{A}$ be the number of elements in
$A$.  A finite set may have no elements (the empty set), or one element,
or two elements,\dots or any nonnegative integer number of elements.

Now suppose $R:A \to B$ is a function.  Then every arrow in the diagram
for $R$ comes from exactly one element of $A$, so the number of arrows is
at most the number of elements in $A$.  That is, if $R$ is a function,
then
\[
\card{A} \geq \#\text{arrows}.
\]
Similarly, if $R$ is surjective, then every element of $B$ has an arrow
into it, so there must be at least as many arrows in the diagram as the
size of $B$.  That is,
\[
\#\text{arrows} \geq \card{B}.
\]
Combining these inequalities implies that if $R$ is a surjective function,
then $\card{A} \geq \card{B}$.  In short, if we write $A \surj B$ to mean
that there is a surjective function from $A$ to $B$, then we've just
proved a lemma: if $A \surj B$, then $\card{A} \geq \card{B}$.  The
following definition and lemma lists include this statement and three
similar rules relating domain and codomain size to relational properties.

\begin{definition}\label{bigger}
  Let $A,B$ be (not necessarily finite) sets.  Then
  \begin{enumerate}
  \item $A$ \term{$\surj$} $B$ iff there is a surjective \emph{function} from $A$ to $B$.  

  \item $A$ \term{$\inj$} $B$ iff there is a total injective \emph{relation} from $A$ to $B$.

  \item $A$ \term{$\bij$} $B$ iff there is a bijection from $A$ to $B$.  

  \item $A$ \term{$\strict$} $B$ iff $A \surj B$, but not $B \surj A$.  

  \end{enumerate}
\end{definition}


\begin{lemma}\label{mapruldef}
\hyperdef{mapping-rule}{lemma}{[Mapping Rules]} \mbox{}
Let $A$ and $B$ be finite sets.

\begin{enumerate}

\item\label{mapping-sur} If $A \surj B$, then $\card{A} \geq \card{B}$.

\item\label{mapping-inj} If $A \inj B$, then $\card{A} \leq \card{B}$.

\item\label{mapping-bij} If $R \bij B$, then $\card{A} = \card{B}$.

\item\label{mapping-strict} If $R \strict B$, then $\card{A} > \card{B}$.

\end{enumerate}

\end{lemma}

Mapping rule~\ref{mapping-inj} can be explained by the same kind of
``arrow reasoning'' we used for rule~\ref{mapping-sur}.
Rules~\ref{mapping-bij} and ~\ref{mapping-strict} are immediate
consequences of these first two mapping rules.

\section{The sizes of infinite sets}

Mapping Rule~\ref{mapping-sur} has a converse:
if the size of a finite set, $A$, is greater than or equal to the size of
another finite set, $B$, then it's always possible to define a
surjective function from $A$ to $B$.  In fact, the surjection can be a
total function.  To see how this works, suppose for example that
\begin{align*}
A & =\set{a_0,a_1,a_2,a_3,a_4,a_5}\\
B & =\set{b_0,b_1,b_2,b_3}.
\end{align*}
Then define a total function $f:A\to B$ by the rules
\[
f(a_0) \eqdef b_0,\  f(a_1) \eqdef b_1,\  f(a_2) \eqdef b_2,\  f(a_3)=
f(a_4)=f(a_5) \eqdef b_3.
\]

\begin{staffnotes}

\[
f(a_i) \eqdef b_{\min(i,3)},
\]
for $i=0, \dots, 5$.  Since $5 \geq 3$, this $f$ is a surjection.

\end{staffnotes}
In fact, if $A$ and $B$ are finite sets of the same size, then we could also
define a bijection from $A$ to $B$ by this method.

In short, we have figured out if $A$ and $B$ are finite sets, then
$\card{A} \geq \card{B}$ \emph{if and only if} $A \surj B$, and similar
iff's hold for all the other Mapping Rules:
\begin{lemma}\label{finbig}
For \emph{finite} sets, $A,B$,
\begin{align*}
\card{A} \geq \card{B} & \qiff A \surj B,\\
\card{A} \leq \card{B} & \qiff A \inj B,\\
\card{A} = \card{B} & \qiff A \bij B,\\
\card{A} > \card{B} & \qiff A \strict B.
\end{align*}
\end{lemma}

This lemma suggests a way to generalize size comparisons to infinite sets,
namely, we can think of the relation $\surj$ as an ``\emph{at least as big
  as}'' relation between sets, even if they are infinite.  Similarly, the
relation $\bij$ can be regarded as a ``\term{same size}'' relation between
(possibly infinite) sets, and $\strict$ can be thought of as a
``\term{strictly bigger} than'' relation between sets.

\textcolor{red}{\textbf{Warning}}: We haven't, and won't, define what the
``size'' of an infinite is.  The definition of infinite ``sizes'' is
cumbersome and technical, and we can get by just fine without it.  All we
need are the ``as big as'' and ``same size'' relations, $\surj$ and
$\bij$, between sets.

But there's something else to \textcolor{red}{watch out for}.  We've
referred to $\surj$ as an ``as big as'' relation and $\bij$ as a ``same
size'' relation on sets.  Of course most of the ``as big as'' and ``same
size'' properties of $\surj$ and $\bij$ on finite sets do carry over to
infinite sets, but \emph{some important ones don't} ---as we're about to
show.  So you have to be careful: don't assume that $\surj$ has any
particular ``as big as'' property on \emph{infinite} sets until it's been
proved.

Let's begin with some familiar properties of the ``as big as'' and ``same
size'' relations on finite sets that do carry over exactly to infinite
sets:
\begin{lemma}\label{translem}
For any sets, $A,B,C$,
\begin{enumerate}

\item \label{bigtrans}
$A \surj  B \text{ and } B \surj C, \qimplies  A \surj C$.

\item \label{sametrans} $A \bij B \text{ and } B \bij C, \qimplies A \bij C$.

\item\label{sameABA}
$A \bij B \qimplies B \bij A$.
\end{enumerate}
\end{lemma}

Lemma~\ref{translem}.\ref{bigtrans} and~\ref{translem}.\ref{sametrans}
follow immediately from the fact that compositions of surjections are
surjections, and likewise for bijections, and
Lemma~~\ref{translem}.\ref{sameABA} follows from the fact that the inverse
of a bijection is a bijection.  We'll leave a proof of these facts to
Problem~\ref{CP_surj_relation}.

Another familiar property of finite sets carries over to infinite sets,
but this time it's not so obvious:
\begin{theorem} [\idx{Schr\"oder-Bernstein}] For any sets $A,B$, if $A \surj B$
  and $B \surj A$, then $A \bij B$.
\end{theorem}

That is, the Schr\"oder-Bernstein Theorem says that if $A$ is at least as
big as $B$ and conversely, $B$ is at least as big as $A$, then $A$ is the
same size as $B$.  Phrased this way, you might be tempted to take this
theorem for granted, but that would be a mistake.  For infinite sets $A$
and $B$, the Schr\"oder-Bernstein Theorem is actually pretty technical.
Just because there is a surjective function $f:A\to B$ ---which need not
be a bijection ---and a surjective function $g:B \to A$ ---which also need
not be a bijection ---it's not at all clear that there must be a bijection
$e:A \to B$.  The idea is to construct $e$ from parts of both $f$ and $g$.
We'll leave the actual construction to
Problem~\ref{CP_Cantor_Schroeder_Bernstein_theorem}.

\subsubsection{Infinity is different}

A basic property of finite sets that does \emph{not} carry over to
infinite sets is that adding something new makes a set bigger.  That is,
if $A$ is a finite set and $b \notin A$, then $\card{A \union \set{b}} =
\card{A}+1$, and so $A$ and $A \union \set{b}$ are not the same size.  But
if $A$ is infinite, then these two sets \emph{are} the same size!

\begin{lemma}\label{AUb}
  Let $A$ be a set and $b \notin A$.  Then $A$ is infinite iff $A \bij A
  \union \set{b}$.
\end{lemma}
\begin{proof}
  Since $A$ is \emph{not} the same size as $A \union \set{b}$ when $A$ is
  finite, we only have to show that $A \union \set{b}$ \emph{is} the same
  size as $A$ when $A$ is infinite.

That is, we have to find a bijection between $A \union \set{b}$ and $A$
when $A$ is infinite.  Here's how: since $A$ is infinite, it certainly has
at least one element; call it $a_0$.  But since $A$ is infinite, it has at
least two elements, and one of them must not be equal to $a_0$; call this
new element $a_1$.  But since $A$ is infinite, it has at least three
elements, one of which must not equal $a_0$ or $a_1$; call this new
element $a_2$.  Continuing in the way, we conclude that there is an
infinite sequence $a_0,a_1,a_2,\dots,a_n,\dots$ of different elements of
$A$.  Now it's easy to define a bijection $e: A \union \set{b} \to A$:
\begin{align*}
e(b) & \eqdef a_0,\\
e(a_n) & \eqdef a_{n+1}  &\text{ for } n \in \naturals,\\
e(a) & \eqdef a & \text{ for } a \in A - \set{b,a_0,a_1,\dots}.
\end{align*}
\end{proof}

A set, $C$, is \term{countable} iff its elements can be listed in order,
that is, the distinct elements is $A$ are precisely
\[
c_0, c_1, \dots, c_n, \dots.
\]
This means that if we defined a function, $f$, on the nonnegative integers
by the rule that $f(i) \eqdef c_i$, then $f$ would be a bijection from
$\naturals$ to $C$.  More formally,

\begin{definition}
  A set, $C$, is \term{countably infinite} iff $\naturals \bij C$.  A set
  is \term{countable} iff it is finite or countably infinite.
\end{definition}

A small modification\footnote{See Problem~\ref{CP_smallest_infinite_set}}
of the proof of Lemma~\ref{AUb} shows that countably infinite sets are
the ``smallest'' infinite sets, namely, if $A$ is a countably infinite
set, then $A \surj \naturals$.

Since adding one new element to an infinite set doesn't change its
size, it's obvious that neither will adding any \emph{finite} number
of elements.  It's a common mistake to think that this proves that you
can throw in countably infinitely many new elements.  But just because
it's ok to do something any finite number of times doesn't make it OK
to do an infinite number of times.  For example, starting from 3, you
can add 1 any finite number of times and the result will be some
integer greater than or equal to 3.  But if you add add 1 a countably
infinite number of times, you don't get an integer at all.

It turns out you really can add a countably infinite number of new
elements to a countable set and still wind up with just a countably
infinite set, but another argument is needed to prove this:

\begin{lemma}\label{countable-union}
If $A$ and $B$ are countable sets, then so is $A \union B$.
\end{lemma}

\begin{proof}
Suppose the list of distinct elements of $A$ is $a_0,a_1,\dots$ and the
list of $B$ is $b_0,b_1, \dots$.  Then a list of all the elements in $A
\union B$ is just
\begin{equation}\label{a0b0list}
a_0,b_0,a_1,b_1, \dots a_n,b_n, \dots.
\end{equation}
Of course this list will contain duplicates if $A$ and $B$ have elements
in common, but then deleting all but the first occurrences of each element in
list~\eqref{a0b0list} leaves a list of all the distinct elements of $A$
and $B$.
\end{proof}

\subsection{Infinities in Computer Science}

We've run into a lot of computer science students who wonder why they
should care about infinite sets: any data set in a computer memory is
limited by the size of memory, and since the universe appears to have
finite size, there is a limit on the possible size of computer memory.

\iffalse need to learn all this abstract theory of infinite sets, and this
is a good question.  \fi

The problem with this argument is that universe-size bounds on data
items are so big and uncertain (the universe seems to be getting
bigger all the time), that it's simply not helpful to make use of
possible bounds.  For example, by this argument the physical sciences
shouldn't assume that measurements might yield arbitrary real numbers,
because there can only be a finite number of finite measurements in a
universe of finite lifetime.  What do you think scientific theories
would look like without using the infinite set of real numbers?

Similary, in computer science, it simply isn't plausible that writing a
program to add nonnegative integers with up to as many digits as, say, the
stars in the sky (billions of galaxies each with billions of stars), would
be any different than writing a program that would add any two integers
no matter how many digits they had.

That's why basic programming data types like integers or strings, for
example, can be defined without imposing any bound on the sizes of
data items.  Each datum of type \idx{\texttt{string}} has only a
finite number of letters, but there are an infinite number of data items
of type \texttt{string}.  When we then consider string procedures of
type \idx{\texttt{string-->string}}, not only are there
an infinite number of such procedures, but each procedure generally
behaves differently on different inputs, so that a single
\texttt{string-->string} procedure may embody an infinite number of
behaviors.

In short, an educated computer scientist can't get around having to
understand infinite sets.

\begin{problems}

\classproblems
\pinput{CP_surj_relation}

\pinput{CP_smallest_infinite_set}

\pinput{CP_mapping_rule}

\pinput{CP_set_product_bijection}

\pinput{CP_rationals_are_countable}

\pinput{CP_Cantor_Schroeder_Bernstein_theorem}

\homeworkproblems

\pinput{PS_composition-of-jections}
\pinput{PS_unit_interval}

\end{problems}

\newpage
\section{Glossary of Symbols}
\begin{center}
\begin{tabular}{ll}
symbol &  meaning\\
\hline%
\iffalse
$\eqdef$ & is defined to be\\
$\land$ & and\\
$\lor$ & or\\
$\implies$ & implies\\
$\neg$    & not\\
$\neg{P}$ & not $P$\\
$\bar{P}$ & not $P$\\
$\iff$    & iff\\
$\iff$    & equivalent\\
$\oplus$   & xor (exclusive-or)\\
$\exists$ & exists\\
$\forall$ & for all\\
\fi%
$\in$   &  is a member of\\
$\subseteq$ & is a subset of\\
$\subset$ & is a proper subset of\\
$\union$  & set union\\
$\intersect$ & set intersection\\
$\bar{A}$ & complement of a set, $A$\\
$\power(A)$ & powerset of a set, $A$\\
$\emptyset$ & the empty set, $\set{}$\\
$\naturals$ & nonnegative integers \\
$\integers$ & integers\\
$\integers^+$ & positive integers\\
$\integers^-$ & negative integers\\
$\rationals$ & rational numbers\\
$\reals$ & real numbers\\
$\complexes$ & complex numbers\\
$\emptystring$ & the empty string/list
\end{tabular}
\end{center}

\endinput
