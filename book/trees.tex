\newcommand{\brnch}{\text{BBTr}}
\newcommand{\leafp}[1]{\text{leaf?}#1}
\newcommand{\brnchng}{\text{Branching}}
\newcommand{\leafset}{\text{Leaves}}
\newcommand{\rightsub}[1]{\text{right}#1}
\newcommand{\leftsub}[1]{\text{left}#1}
\newcommand{\rslt}[2]{\text{subtree}_{#1}#2}
\newcommand{\subbrn}[1]{\text{Subtrs}(#1)}
\newcommand{\propbrn}[1]{\text{PropSubtrs}(#1)}

\newcommand{\fintr}{\text{FinTr}}

\newcommand{\rectr}{\text{RecTr}}

\newcommand{\trdpth}[1]{\text{recdepth}#1}
\newcommand{\trsize}[1]{\text{recsize}#1}

\newcommand{\nlbl}[1]{\text{num}#1}
\newcommand{\rmin}[1]{\text{recmin}#1}

%\newcommand{\freetr}{\text{FreeTr}}
%\newcommand{\subtr}[1]{\text{subtrs}(#1)}
%\newcommand{\psubtr}[1]{\text{psubtrs}(#1)}
\newcommand{\trsrch}[1]{\text{srch}#1}

%\newcommand{\selpath}[1]{\vec{#1}}

\section{Search Trees}

Searching through data may be the most common of all computations, and
\term{search trees} are a widely used data structure that supports
efficient search.

The basic idea behind search trees is simple.  Assuming the data to be
searched have some kind of order relation on them---like numerical order
for numbers or alphabetical order for words---the data is laid out in
a kind of branching ``tree'' structure illustrated in
Figure~\ref{searchtree1}.  In this example, there are two branches at
each branch point, with all the values in the left branch less than
the value at the branch point, and all the values in the right branch
greater than the value at the branch point.  This makes it easy to
search for a given value in the tree: starting at the topmost
branchpoint, compare the given value to the value at the branchpoint.
If the given value---call it $g$---is less than the value at the
branchpoint---call it $b$---then continue searching in the left
branch; if $g$ is greater than $b$ continue searching in the right
branch; if $g = b$ then the given value has been found, and the search
ends successfully.  Finally, if there are no more branches to search,
then $g$ is not there, so the search ends unsuccessfully.

For example, to search for 3.7 in the tree in Figure~\ref{searchtree1},
we compare 3.7 to the value 6.8 at the topmost branch.  Since $3.7 <
6.8$, we go left and compare $3.7$ to the new top value $\pi$.  Since
$3.7 > \pi$, go right comparing 3.7 to 4.9, go left again, and arrive
at 3.7. So the search ends successfully.

\begin{figure}

%\graphic{}
\begin{center}
\begin{verbatim}
                6.8
          pi             8  
     3          4.9    7   9 
  0.5  3.1    3.7  5
.4  .6       3.2 4 
\end{verbatim}
\end{center}

\caption{A binary search tree.}

\label{searchtree1}

\end{figure}

Organizing the data in this way means you can search for a value
without having to scan through all the values in the data set.  In
fact, if the all the top-to-bottom paths through the tree don't differ
too much in length, then with maximum path length $n$, the search tree
will hold up to $2^n$ items.  Since each search simply follows some
path, this means that searching for a value will take
\emph{exponentially less time} than it would to search through all the
data.  That's why search trees are a big win.

Of course it is possible that the tree has paths that are not short.
An extreme example is illustrated in Figure~\ref{unbalanced} where
there is a path containing half the values.  For this tree, the
average search for a value requires searching halfway down the path,
and the exponential savings evaporates.

\begin{figure}

%\graphic{}
\begin{center}
\begin{verbatim}
                0
            .5     1
               1.5    2
                  2.5    3
                     3.5   .
                        .    .
                          .    .
\end{verbatim}   
\end{center}

\caption{An unbalanced search tree.}

\label{unbalanced}

\end{figure}

By treating search trees as a recursive data type, we will be able to
describe simple recursive procedures for tree management, and we also
will have a powerful inductive proof method for proving properties of
these trees and procedures.  This will allow simple explanations of
how to grow search trees whose top-to-bottom paths are close to the
same length, and how to maintain this property as the data evolves
through adding and removing values.

\subsection{Binary Branching}

The trees in the examples above have either a single ``leaf'' with no
branching, or a binary (two-way) branch to the left and right.
Allowing more than two branches is important in some applications, but
binary branching is most common and is enough to illustrate all the
important ideas.

There are lots of ways to represent trees as a computer data
structure---or a mathematical set---but we will take an abstract
approach that will enable us, largely, to understand binary trees
without worrying about how they are represented.

We can begin by assuming only that we have some set \brnch\ of things
that represent \emph{binary branching trees}.  A crucial property of a
binary branching tree is whether or not it is a leaf.  In any
representation, there has to be way to test this.  So there has to be
a \term{leaf predicate} $\leafp{}$ that does the job.  That is, a
binary branching tree $T \in \brnch$ is a leaf when $\leafp{(T)}$ is
true.

\iffalse
 We define the
\term{leaves} to be elements that $\leafp{}$ picks out:
\[
\leafset \eqdef \set{T \in \brnch \suchthat \leafp{(T)}}.
\]

So the official definition of ``leaf'' is ``element of the set \leafset.''
\fi

Let \brnchng\ be the trees in \brnch\ that actually branch to the left
and right.  In any representation there has to be a way to select
these branches.  So there have to be \term{selector functions}
$\leftsub{}$ and $\rightsub{}$ that produce the left and right
branches of non-leaf elements of \brnch.  That is, let
\[
\brnchng \eqdef \set{T \in \brnch \suchthat \QNOT(\leafp{(T)})}.
\]
Then there have to be total functions
\begin{align*}
\leftsub{}: & \brnchng \to \brnch,\\
\rightsub{}:& \brnchng \to \brnch,
\end{align*}
where $\leftsub{(T)}$ is the tree on the left branch of $T$ and
$\rightsub{(T)}$ is the one on the right.

\subsubsection{Subtrees}

In the example search trees we started with, if you start at the top
and follow some path down, you arrive at a subtree that is also a
search tree.  Any such path can be described by the sequence of left
and right branches you select.  For example in the tree given in
Figure~\ref{searchtree1}, the sequence
\begin{equation}\label{lslsrs}
(\leftsub{},\leftsub{},\rightsub{}).
\end{equation}
describes the path that starts at the top (6.8).  The selectors are
applied in right-to-left order, so we first go to $\rightsub{(6.8)} =
\pi$.  Then we take the left branch to $\leftsub{(\pi)} = 3$, and take a
final left branch to end with the subtree $\leftsub{(3)} = 0.5$.  That
is, the subtree at the end of the path given by the sequence~\eqref{lslsrs} is
\[
\leftsub{(\leftsub{(\rightsub{(6.8)})})} = 0.5.
\]

Let $\vec{P}$ be a finite sequence of selector functions.  For $T \in
\brnch$, let $\rslt{T}{(\vec{P})}$ be the subtree at the end of the
path described by $\vec{P}$.  Notice that if the path tries to
continue past a leaf of $T$, there won't be any subtree at the end.
That is, $\rslt{T}{(\vec{P})}$ may not exist.

To formalize this idea of following a path $\vec{P}$ down a tree, we
can give an inductive definition of $\rslt{T}{(\vec{P})}$.  To do
this, we'll use the notation
\[
f \cdot \vec{P}
\]
for the sequence that starts with $f$ followed by the successive
elements of $\vec{P}$.  For example, if $\vec{P}$ is the length three
sequence in~\eqref{lslsrs}, then
\[
\rightsub{} \cdot \vec{P}
\]
is the length four sequence
\[
(\rightsub{}, \leftsub{},\leftsub{},\rightsub{}).
\]

Likewise for $\vec{P} \cdot f$, so for example
\[
 \vec{P} \cdot \rightsub{}
\]
would be the length four sequence
\[
(\leftsub{},\leftsub{},\rightsub{},\rightsub{}).
\]

Supppose $T \in \brnch$ and $\vec{P}$ is a finite sequence of selector
functions.  Let $\rslt{T}{(\vec{P})}$ be the subtree of $T$ at the end
of the path given by $\vec{P}$.  So in the example in Figure~\ref{searchtree1},
\[
\rslt{(6.8)}{((\leftsub{},\leftsub{},\rightsub{}))} = 0.5.
\]

The formal definition of the subtree at the end of the path is by
induction on path length:

\begin{definition}
The tree $\rslt{T}{(\vec{P})} \in \brnch$ will be defined by induction
on the length of the sequence~$\vec{P}$:

\inductioncase{Base case}: ($\vec{P} = \emptystring$).
Then
\[
\rslt{T}{(\vec{P})} \eqdef T.
\]

\inductioncase{Constructor case}: ($\vec{P} = f \cdot \vec{Q}$).  If
$\rslt{T}{(\vec{Q})}$ is defined and equals a tree in \brnchng, then
\[
\rslt{T}{(\vec{P})} \eqdef f(\rslt{T}{(\vec{Q})}).
\]
Otherwise $\rslt{T}{(\vec{P})}$ is undefined.\footnote{An alternative
  Constructor case could have been

\inductioncase{Constructor case}: ($\vec{P} = \vec{Q} \cdot f$).  If
$T \in \brnchng$, then
\[
\rslt{T}{(\vec{P})} \eqdef \rslt{f(T)}{(\vec{Q})}.
\]}

\end{definition}
\medskip

Let $\subbrn{T}$ be all the trees you can get to by following paths
starting at $T$.  More precisely,
\begin{definition}\label{def:subtree}
\[
\subbrn{T} \eqdef \set{S \in \brnch \suchthat S =
  \rslt{T}{(\vec{P})}\ \text{for some selector sequence}\ \vec{P}}.
\]
\end{definition}

The \term{proper subtrees} $\propbrn{T}$ are the subtrees that are
actually ``below'' $T$.
\[
\propbrn{T} \eqdef \set{S \in \brnch \suchthat S =
  \rslt{T}{(\vec{P})}\ \text{for some \emph{nonempty} selector
    sequence}\ \vec{P}}.
\]
So we have by definition that
\begin{equation}\label{subtree-proptree}
\subbrn{T} = \propbrn{T} \union \set{T}.
\end{equation}

There are a couple of obvious facts about paths and subtrees that we
might have just taken for granted but are worth saying explicitly.
The first is that a proper subtree of $T$ must be a subtree of its
left subtree or its right subtree:

\begin{corollary}\label{unionLR}
For $T \in \brnch$,
\[
\propbrn{T} =
\begin{cases}
  \emptyset, & \text{if $T$ is a leaf},\\
  \subbrn{\leftsub{(T)}} \union \subbrn{\rightsub{(T)}},
            & \text{if}\ T \in \brnchng.
\end{cases}
\]
\end{corollary}

Corollary~\ref{unionLR} follows from the fact when the first step in a
path goes to a subtree, the rest of the path is in that subtree.

The second obvious fact is that a subtree of a subtree is a subtree:

\begin{corollary}\label{propsubbranch}
If  $S \in \propbrn{T}$ and $R \in \subbrn{S}$, then $R \in \propbrn{T}$.
\end{corollary}

Corollary~\ref{propsubbranch} follows from the fact that if $\vec{P}$
is a path in $T$ to a subtree $S$ of $T$, and $\vec{Q}$ is a path in
$S$ to a subtree $R$ of $S$, as in Figure~\ref{PQdown}, then the
\idx{concatenation} $\vec{Q} \cdot \vec{P}$ is a path from $T$ to
$R$.\footnote{Path sequences are applied right-to-left, so $\vec{Q}
  \cdot \vec{P}$ is the path that starts with~$\vec{P}$ and then
  follows~$\vec{Q}$.}

%\footnote{(Definition~\ref{concat_def}).}

\begin{figure}

%\graphic{}

\begin{center}
\begin{verbatim}
              T
               \
               /    } P
              /
             S
            /
     Q {   /
           \
            R
\end{verbatim}   
\end{center}

\caption{A subbranch $S$ of $T$ and  a subbranch $R$ of $S$.}

\label{PQdown}

\end{figure}

\subsection{Binary Trees}
Although we've called the elements of \brnch\ binary branching
``trees,'' the abstract way we defined \brnch\ allows some weird stuff
to happen.  One weirdness is that nothing forbids ``circular'' trees
that are proper subtrees of themselves.
\iffalse
For example, it is entirely
possible that there is a structure $T \in \brnch$ such that
\textcolor{red}{
\[
\leftsub{(T)} = T = \rightsub{(T)}.
\]}

This weird binary branching structure doesn't even have any leaves.
\fi
And even if we forbid circular trees, there are still infinite
trees.  For example, suppose we choose \brnch\ to be the nonnegative
integers and define $\leftsub{(n)} \eqdef 2n+1$ and $\rightsub{(n)} \eqdef
2n+2$.  Now we get the infinite binary tree indicated in
Figure~\ref{inftree123}.

%, which is another example with no leaves.

\begin{figure}

%\graphic{}

\begin{verbatim}
                  0
           1            2
        3     4      5      6
       7 8   9 10  11 12  13 14
       .          .             . 
      .           .              .
     .            .               . 

\end{verbatim}

\caption{An infinite tree with no leaves.}

\label{inftree123}

\end{figure}

The problem with both circular structures and infinite ones is that
they will have infinite paths that you might keep searching down
without ever finishing.  By forbidding infinite paths we ensure that
searches will end, and we also stop all the circular and infinite
weirdness.

Paths that result in subtrees are by definition finite, so we should
be clear about the definition of an infinite path.

\begin{definition}
A infinite path in $T \in \brnch$ is an infinite sequence
\[
\dots,f_n,\dots,f_1,f_0
\]
of selector functions such that
\[
\rslt{T}{(\vec{P})} \in \brnchng
\]
for every finite suffix
\[
\vec{P} = (f_n,\dots, f_1,f_0).
\]
\end{definition}

But there's still a problem.  Even if they
have no infinite paths, a tree in \brnch\ may have some unexpected
\emph{sharing} of subtrees.  For example, it is possible to have a
tree in \brnch\ whose left and right subtrees are the same.

% or another whose top to bottom paths all end at the \emph{same} leaf.

To be precise
\begin{definition}
A tree $T \in \brnch$ \emph{shares subtrees} if there are two finite
sequences $\vec{P} \neq \vec{Q}$ of selector functions such that
$\rslt{T}{(\vec{P})}$ and $\rslt{T}{(\vec{Q})}$ are both defined, and
\[
\rslt{T}{(\vec{P})}=\rslt{T}{(\vec{Q})}.
\]
\end{definition}
Sharing of subtrees can be very useful in getting a compact
representation of a tree, but it seriously complicates adding or
removing values from search trees, so we will forbid sharing as well.
This leads us to \fintr, the familiar trees that underlie search
trees.
\begin{definition}
\[
\fintr \eqdef \set{T \in \brnch \suchthat T\ \text{has no infinite path and
  does not share subtrees}}.
\]
\end{definition}

Now it is both very useful, very cool, and initially surprising to
realize that $\fintr$ has a simple recursive definition.

\begin{definition}
The set \rectr\ of \term{recursive trees} is defined recursively.  For
$T \in \brnch$,

\inductioncase{Base case}: ($T$ is a leaf).  $T \in \rectr$.

\inductioncase{Constructor case}: ($T \in \brnchng$).
If $\leftsub{(T)}, \rightsub{(T)} \in \rectr$, and these two trees
have no subtree in common, that is,
\[
\subbrn{\leftsub{(T)}} \intersect \subbrn{\rightsub{(T)}} = \emptyset,
\]
and $T$ is not a subtree of itself, that is,
\[
T \notin \subbrn{T},
\]
then $T$ is in \rectr.
\end{definition}

The trees with no infinite paths and no shared subtrees are actually
the same as the recursively defined trees.

\begin{theorem}\label{fundthmrec}(Fundamental Theorem for Recursive Trees)
\[
\fintr = \rectr.
\]
\end{theorem}

We first prove
\begin{equation}\label{rectrsubfintr}
\rectr \subseteq \fintr,
\end{equation}
that is, no recursive tree $T$ has an infinite path or a shared subtree.

\begin{proof}
The proof follows by structural induction on the
definition of \rectr:

\inductioncase{Base case}: ($T$ is a leaf).  $T$ has no infinite path
or shared subtree because it has no proper subtrees at all.

\inductioncase{Constructor case}: ($T \in \brnchng$).  In this case,
$\leftsub{(T)}$ and $\rightsub{(T)}$ are defined and in \rectr.  By
induction hypothesis, neither $\leftsub{(T)}$ nor $\rightsub{(T)}$ has an
infinite path nor sharing of subtrees.

We begin with a proof by contradiction that $T$ has no infinite path.
Namely, suppose that $T$ had an infinite path
\[
\dots,f_n,\dots, f_2,f_1,f_0.
\]
Now if $f_0 = \leftsub$, then
\begin{equation}\label{dfnf2f1}
\dots,f_n,\dots,f_2,f_1
\end{equation}
would be an infinite path in $\leftsub{(T)}$, a contradiction.  Likewise
if $f_0 = \rightsub$, then~\eqref{dfnf2f1} would be an infinite path
in $\rightsub{(T)}$, a contradiction.  So we get a contradiction in any
case, which implies that $T$ cannot have an infinite path.

Next we show that $T$ has no shared subtree.

Since $T$ has no infinite path, $T$ cannot be a subtree of
$\leftsub{(T)}$ or $\rightsub{(T)}$, and so will not itself be a
shared subtree.  Since there are no shared subtrees within
$\leftsub{(T)}$ or $\rightsub{(T)}$, the only remaining possibility
would be a subtree that was in $\leftsub{(T)}$ and $\rightsub{(T)}$,
which is not possible since $\subbrn{\leftsub{(T)}}$ and
$\subbrn{\rightsub{(T)}}$ have no tree in common by definition of
\rectr.
\end{proof}

Second, we prove
\begin{equation}\label{fintrsubrectr}
\fintr \subseteq \rectr.
\end{equation}
Taking the contrapositive, we want to prove that if $T \in \brnch$ is
\emph{not} a recursive tree, then it has an infinite path or it has a
shared subtree.

\begin{proof}
If $T \notin \rectr$, then $T$ cannot be a leaf.  If $\leftsub{(T)}$
and $\rightsub{(T)}$ have a subtree in common, then this subtree is
shared in $T$, proving the claim.  So we may assume that
$\leftsub{(T)}$ and $\rightsub{(T)}$ have no subtree in common.  In
this case, if both $\leftsub{(T)}$ and $\rightsub{(T)}$ were in
\rectr, then $T$ would be in \rectr\ by definition of \rectr.
Therefore $\leftsub{(T)}$ and $\rightsub{(T)}$ cannot both be in
\rectr.

Let $f_0$ be a selector such that $f_0(T) \notin \rectr$.  Now if
$\leftsub{(f_0(T))}$ and $\rightsub{(f_0(T))}$ have a shared subtree,
then by Corollary~\ref{propsubbranch}, this will also be a shared
subtree of $T$, proving the claim.  So by the same reasoning as for
$T$, $\leftsub{(f_0(T))}$ and $\rightsub{(f_0(T))}$ cannot both be in
\rectr.  So let $f_1$ be a selector such that $f_1(f_0(T)) \notin
\rectr$.  Similarly, if there is no sharing in $f_1(f_0(T))$, there is
a selector $f_2$ such that $f_2(f_1(f_0(T))) \notin \rectr$.
Continuing in this way, we either find a shared subtree of $T$ or we
find an infinite path
\[
\dots,f_n,\dots, f_2,f_1,f_0
\]
in $T$.
\end{proof}

\subsection{Properties of Recursive Trees}

With the recursive definition of trees, we can define some important,
basic functions on trees recursively.  For example, the \term{size} of
a tree is the number of subtrees it has:
\begin{equation}\label{treesizedef}
\text{size}(T) \eqdef \card{\subbrn{T}}.
\end{equation}

Now we give a recursive definition of size:
\begin{definition}
We define a function $\trsize{}: \rectr \to \nngint$ as follows:

\inductioncase{Base case}: ($T$ is a leaf).
\[
\trsize{(T)} \eqdef 1.
\]

\inductioncase{Constructor case}: ($T \in \brnchng$).
\[
\trsize{(T)} \eqdef 1 + \trsize{(\leftsub{(T)})} + \trsize{(\rightsub{(T)})}.
\]
\end{definition}

A simple proof by structural induction confirms that the definition of
\trsize{}\ is correct, namely:
\begin{lemma}\label{}
\[
\trsize{(T)} = \text{size}(T)
\]
for all $T \in \rectr$.

\begin{proof}
The proof is by structural induction on the definition of \rectr.

\inductioncase{Base case}: ($T$ is a leaf).
By Corollary~\ref{unionLR}, $\propbrn{T} = \set{T}$, so
\[
\trsize{(T)} \eqdef 1 = \card{\set{T}}.
\]

\inductioncase{Constructor case}: ($T \in\brnchng$).

We have by induction hypothesis that
\begin{equation}\label{sizefsub}
\text{size}(f(T)) = \trsize{(f(T))}, \qquad  \text{for}\ f \in \set{\leftsub{}, \rightsub{}}.
\end{equation}
By Corollary~\ref{unionLR},
\[
\subbrn{T} = \set{T} \union \subbrn{\leftsub{(T)}} \union \subbrn{\rightsub{(T)}},
\]
for $T \in \brnch$.  Now if $T \in \rectr$, then these three sets are disjoint, so
\begin{equation}\label{sT1sl}
\text{size}(T) = 1 + \text{size}(\leftsub{(T)}) + \text{size}(\rightsub{(T)}).
\end{equation}
Now we have
\begin{align*}
\text{size}(T)
 & = 1 + \trsize{(\leftsub{(T)})} + \trsize{(\rightsub{(T)})}
   & \text{(by~\eqref{sizefsub} and~\eqref{sT1sl})}\\
 & = \trsize{(T)} 
   & \text{(def. of \trsize{})}.
\end{align*}
\end{proof}
\end{lemma}

Similarly, the \term{depth} of a recursive tree is the length of its
longest path.  The formal definition is:
\begin{equation}\label{defdepth}
\term{depth}(T) \eqdef \max \set{\lnth{\vec{P}} \suchthat
  \rslt{T}{(\vec{P})}\ \text{is a leaf}}.
\end{equation}

Here is the recursive definition:
\begin{definition}
\inductioncase{Base case}: ($T$ is a leaf).
\[
\trdpth{(T)} \eqdef 0.
\]

\inductioncase{Constructor case}: ($T \in\brnchng$).
\[
\trdpth{(T)} \eqdef 1+\max\set{\trdpth{(\leftsub{(T)})}, \trdpth{(\rightsub{(T)})}}.
\]
\end{definition}

It follows immediately by structural induction (Problem~\ref{TBA})
that
\begin{equation}\label{recdepth=depth}
\trdpth{(T)} = \text{depth}(T).
\end{equation}

\begin{problem}
Prove that
\[
\trdpth{(T)} = \text{depth}(T)
\]
for all $T \in \rectr$.

\begin{solution}
 \TBA{TBA}
\end{solution}
\end{problem}

We can now prove a basic relation between size and depth of recursive
trees:
\begin{theorem}\label{szT2d1}
For all $T \in \rectr$,
\[
\text{size}(T) \leq 2^{\text{depth}(T)+1} - 1.
\]
\end{theorem}

There is an easy way to understand this inequality: if there were
paths of different lengths that ended at leaves, then there would be a
bigger tree with the same depth---just attach two new leaves to the
shorter path.  So the biggest tree with depth $d$ will be the
\term{complete tree} $T$ in which all paths have depth $d$.  In this
tree there are two paths of length one that branch into four
paths of length two, continuing into $2^d$ paths of length $d$.  So
\[
\text{size}(T) = 2^0 + 2^1 + 2^2 + \cdots + 2^d = 2^{d+1}-1.
\]

On the other hand, there is a a cumbersome but routine proof by
structural induction that requires no ingenuity:

\begin{proof} 
It is enough to prove Theorem~\ref{szT2d1} for \trsize\ and \trdpth.
The proof is by structural induction on the definition of \rectr:

\inductioncase{Base case}: ($T$ is a leaf).
\[
\trsize{(T)} \eqdef 1 = 2^1 - 1 = 2^{\trdpth{(T)}+1} - 1.
\]

\inductioncase{Constructor case}: ($T \in \brnchng$).
\begin{align*}
\trsize{T}
& = 1 + \trsize{(\leftsub{(T)})} + \trsize{(\rightsub{(T)})}
              & \text{(def. \trsize{})}\\
& \leq 1 + (2^{\trdpth{(\leftsub{(T)})}+1} -1)  + (2^{\trdpth{(\rightsub{(T)})}+1} -1)
              & \text{(ind. hypothesis)}\\
& \leq  2\cdot 2^{\max\set{\trdpth{(\leftsub{(T)})}+1,\trdpth{(\rightsub{(T)})}+1}} - 1\\
& = 2\cdot 2^{1+\max\set{\trdpth{(\leftsub{(T)})},\trdpth{(\rightsub{(T)})}}} - 1\\
& = 2\cdot 2^{\trdpth{(T)}} - 1
              & \text{(def of $\trdpth{(T)}$)}\\
& =  2^{\trdpth{(T)}+1} - 1.
\end{align*}
\end{proof}

\begin{problem}
For $T \in \brnch$, define
\begin{align*}
\text{leaves}(T)   & \eqdef \set{S \in \subbrn{T} \suchthat S\ \text{is a leaf}}\\
\text{internal}(T) & \eqdef \set{S \in \subbrn{T} \suchthat S \in \brnchng}
\end{align*}

Prove that in a recursive tree, there is always one more leaf than
there are internal subtrees:

\begin{lemma*}
If $T \in \rectr$, then
\[
\card{\text{leaves}(T)} = 1 + \card{\text{internal}(T)}.
\]
\end{lemma*}
\begin{solution}

\begin{proof}
The proof is by structural induction on the definition of \rectr.

\TBA{rest of proof}
\end{proof} 
\end{solution}
\end{problem}

A $T \in \rectr$ is \term{balanced} when all the paths that end at
leaves have length at least $\text{depth}(T) - 1$.

\begin{lemma}\label{b>2d}
If $B \in \rectr$ is balanced, then
\[
\text{size}(B) > 2^{\text{depth}(B)}.
\]
\end{lemma}

\begin{problem}
Prove Lemma~\ref{b>2d} by structural induction on the definition of \rectr.

\begin{solution}
\TBA{TBA}
\end{solution}
\end{problem}

\begin{problem}
Prove Lemma~\ref{b>2d} by reasoning about the complete tree as in the
argument following Theorem~\ref{szT2d1}.

\begin{solution}
\TBA{TBA}
\end{solution}
\end{problem}

An \term{AVL tree} is a tree $T\in \rectr$ such that
\[
\abs{\text{depth}(\leftsub{(S)}) - \text{depth}(\rightsub{(S)})} \leq 1
\]
for all subtrees $S \in \subbrn{T}$.

\begin{lemma}\label{}
If $T$ is AVL, then
\[
\text{size}(T) \geq 2^{\text{depth}(T/3)}???
\]

\begin{proof}
\TBA{TBA}
\end{proof}
\end{lemma}

\subsection{Search Trees}

The search tree examples above have a numerical label on each subtree.
Abstractly, this means there is a total function
\[
\nlbl{}:\brnch \to \reals.
\]

Let $\text{min}(T)$ be the minimum label in $T$ and likewise for
$\text{max}(T)$.  More formally,
\begin{definition}
\begin{align}
\text{nums}(T) & \eqdef \set{\nlbl{(S)} \suchthat S \in \subbrn{T}},\\
\text{min}(T) &\eqdef \min \text{nums}(T), \label{defminT}\\
\text{max}(T) &\eqdef \max \text{nums}(T).\label{defmaxT}
\end{align}
\end{definition}

\begin{definition}\label{defsearchtree}
A recursive tree $T \in \rectr$ is a \term{Search tree} when
\begin{equation}\label{}
\text{max}(\leftsub{(S)}) < \nlbl{(S)} < \text{min}(\rightsub{(S)}).
\end{equation}
for every nonleaf $S \in \subbrn{T}$.
\end{definition}

\begin{problem}
Suppose every tree $T$ has a numerical label $\nlbl{(T)}$. The function
\[
\rmin{}: \rectr \to \reals
\]
is recursively defined as follows:

\inductioncase{Base case}: ($T$ is a leaf).
\[
\rmin{(T)} \eqdef \nlbl{(T)}.
\]

\inductioncase{Constructor case}: ($T \in \brnchng$)
\[
\rmin{(T)} \eqdef \min\set{\nlbl{(T)}, \rmin{(\leftsub{(T)})}, \rmin{(\rightsub{(T)})}}
\]

Prove that
\[
\rmin{(T)} = \text{min}(T)
\]
for all $T \in \rectr$.
\begin{solution}

\begin{proof}
The proof is by structural induction on the definition of \rmin{}.
\TBA{TBA}
\end{proof}

\end{solution}

\end{problem}

We can also give a simple recursive definition of Search tree:
\begin{definition}
The Search trees $T \in \brnch$ are defined recursively as follows:

\inductioncase{Base case}: ($T$ is a leaf).  $T$ is a Search tree.

\inductioncase{Constructor case}: ($T \in \brnchng$).
If $\leftsub{(T)}$ and $\rightsub{(T)}$ are both Search trees, and
\[
\text{max}(\leftsub{(T)}) < \nlbl{(T)} < \text{min}(\rightsub{(T)}),
\]
then $T$ is a Search tree.
\end{definition}

\begin{problem}
Verify that every Search tree is a recursive tree.

\hint Show that Search trees can't have shared subtrees.

\begin{solution}
\TBA{}
\end{solution}
\end{problem}

There is a simple procedure to search through a Search tree for any
given number.  The procedure can be described by defining a function
\trsrch{} recursively on the recursive definition of Search tree.  The
value of $\trsrch{(T,r)}$ will be the path in a Search tree $T$ that
leads to the number $r$, assuming $r$ appears in $T$.  Otherwise the
value will be a path that ends in\textbf{fail}.

\begin{definition}
The function
\[
\trsrch{}: \text{(Search trees)} \times \reals \to
\strings{\set{\leftsub{},\rightsub{}}} \union \set{\textbf{fail}}
\]
is defined as follows:

If $r = \nlbl{(T)}$, then
\[
\trsrch{(T,r)} \eqdef \emptystring.
\]

If $r \neq \nlbl{(T)}$, then $\trsrch{(T,r)}$ is defined recursively
on the definition~\ref{defsearchtree} of Search tree:

\inductioncase{Base case}: ($T$ is a leaf).
\[
\trsrch{(T,r)} \eqdef \textbf{fail}.
\]

\inductioncase{Constructor case}: ($T \in \brnchng$).
\[
\trsrch{(T,r)} \eqdef
 \begin{cases} 
\trsrch{(\leftsub{(T)},r)} \cdot \leftsub{} & \text{if $r < \nlbl{(T)$}},\\
\trsrch{(\rightsub{(T)},r)}\cdot \rightsub{}& \text{if $r > \nlbl{(T)}$}.
\end{cases}
\]
\end{definition}

\begin{theorem}
If $r \in \text{nums}(T)$, then $\trsrch{(T,r)}$ is a sequence of
selectors and
\[
\nlbl{(\rslt{T}{(\trsrch{(T,r)})})} = r.
\]

Otherwise, $\trsrch{(T,r)}$ will be a sequence of the form
$\textbf{fail}\cdot \vec{P}$ for some sequence $\vec{P}$.
\end{theorem}

\begin{proof}

\TBA{By induction on the definition of \trsrch{}.}

\end{proof}

\subsection{Insertions \& Deletions}

Rotations of AVL trees.


  \endinput
