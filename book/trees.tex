\newcommand{\brnch}{\text{Brnch}}
\newcommand{\leafp}[1]{\text{leaf?}(#1)}
\newcommand{\leafset}{\text{Leaves}}
\newcommand{\rightsub}[1]{\text{right}(#1)}
\newcommand{\leftsub}[1]{\text{left}(#1)}
\newcommand{\subbrn}[1]{\text{brchs}(#1)}
\newcommand{\propbrn}[1]{\text{prbrchs}(#1)}

\newcommand{\fintr}{\text{FinTr}}

\newcommand{\rectr}{\text{RecTr}}

\newcommand{\trdpth}[1]{\text{depth}(#1)}
\newcommand{\sizetr}[1]{\text{size}(#1)}

\newcommand{\freetr}{\text{FreeTr}}

%\newcommand{\subtr}[1]{\text{subtrs}(#1)}
%\newcommand{\psubtr}[1]{\text{psubtrs}(#1)}
\newcommand{\srchtr}{\text{Srch}}

%\newcommand{\selpath}[1]{\vec{#1}}


\section{Search Trees}

Searching through data may be the most common of all computations, and
\term{search trees} are a widely used data structure that supports
efficient search.

The basic idea behind search trees is simple.  Assuming the data to be
searched have some kind of order relation on them---like numerical order
for numbers or alphabetical order for words---the data is laid out in
a kind of branching ``tree'' structure illustrated in
Figure~\ref{searchtree1}.  In this example, there are two branches at
each branch point, with all the values in the left branch less than
the value at the branch point, and all the values in the right branch
greater than the value at the branch point.  This makes it easy to
search for a given value in the tree: starting at the topmost
branchpoint, compare the given value to the value at the branchpoint.
If the given value---call it $g$---is less than the value at the
branchpoint---call it $b$---then continue searching in the left
branch; if $g$ is greater than $b$ continue searching in the right
branch; if $g = b$ then the given value has been found, and the search
ends successfully.  Finally, if there are no more branches to search,
then $g$ is not there, so the search ends unsuccessfully.

For example, to search for 3.7 in the tree in Figure~\ref{searchtree1},
we compare 3.7 to the value 6.8 at the topmost branch.  Since $3.7 <
6.8$, we go left and compare $3.7$ to the new top value $\pi$.  Since
$3.7 > \pi$, go right comparing 3.7 to 4.9, go left again, and arrive
at 3.7. So the search ends successfully.

\begin{figure}

%\graphic{}
\begin{center}
\begin{verbatim}
                6.8
          pi            8  
     3        4.9     7   9 
   1  3.1   3.7  5
          3.2  4 
\end{verbatim}
\end{center}

\caption{A binary search tree.}

\label{searchtree1}

\end{figure}

Organizing the data in this way means you can search for a value
without having to scan through all the values in the data set.  In
fact, if the all the top-to-bottom paths through the tree don't differ
too much in length, then with maximum path length $n$, the search tree
will hold up to $2^n$ items.  Since each search simply follows some
path, this means that searching for a value will take
\emph{exponentially less time} than it would to search through all the
data.  That's why search trees are a big win.


Of course it is possible that the tree has paths that are not short.
An extreme example is illustrated in Figure~\ref{unbalanced} where
there is a path containing half the values.  For this tree, the
average search for a value requires searching halfway down the path,
and the exponential savings evaporates.

\begin{figure}

%\graphic{}
\begin{center}
\begin{verbatim}
                0
            .5     1
               1.5    2
                  2.5    3
                     3.5   .
                        .    .
                          .    .
\end{verbatim}   
\end{center}

\caption{An unbalanced search tree.}

\label{unbalanced}

\end{figure}

By treating search trees as a recursive data type, we will be able to
describe simple recursive procedures for tree management, and we also
will have a powerful inductive proof method for proving properties of
these trees and procedures.  This will allow simple explanations of
how to grow search trees whose top-to-bottom paths are close to the
same length, and how to maintain this property as the data evolves
through adding and removing values.

\subsection{Binary Branching Structures}

The \emph{binary} branching structures \brnch\ illustrated in the
examples above have either a single ``leaf'' with no branching, or a
branching structure connected to a left branch and a right branch.
Allowing more than two branches is important in some applications, but
two-way branching is most common and is enough to illustrate all the
important ideas.

There are lots of ways to represent \brnch\ as a computer data
structure---or a mathematical set---but we will take an abstract
approach that will enable us, largely, to understand \brnch\ without
worrying about how its elements are represented.

We can begin by assuming only that \brnch\ is some arbitrary domain
whose elements are called a \term{binary branching structures}.  Since
there must be a way to test whether or not an element is a leaf, we
assume there is a \term{leaf predicate} $\leafp{}$ that does the job.
We define the \term{leaves} to be elements that $\leafp{}$ picks out:
\[
\leafset \eqdef \set{T \in \brnch \suchthat \leafp{T}}.
\]
So the official definition of ``leaf'' is ``element of the set \leafset.''

The remaining things in \brnch\ are the structures that actually
branch into left and right substructures.  Any representation of
\brnch\ has to allow a way to select these branches, and we capture
this abstractly by assuming there are \term{selector functions} that
produce the left and right branches of non-leaf elements of \brnch.
That is, we assume there are total functions
\begin{align*}
\leftsub{}: & (\brnch - \leafset) \to \brnch,\\
\rightsub{}:& (\brnch - \leafset) \to \brnch.
\end{align*}

In the example search trees we started with, if you start at the top and
follow some path down, you arrive at a subtree that is also a search
tree.  Any such path can be described by the sequence of left and
right branches you select, like
\[
(\leftsub{},\leftsub{},\rightsub{}).
\]
We can formulate this abstractly as follows::

\begin{definition}
For $T \in \brnch$, a \term{selector path} $\vec{P}$ \emph{from $T$}
and its \term{result} is defined recursively:

\inductioncase{Base case}: The empty sequence\emptystring\ is a
selector path for $T$, and its result is $T$.

\inductioncase{Constructor case}: If $\vec{P}$ is a selector path
for $T$ with result $S \in \brnch$, and $S$ is not a leaf, then
\begin{itemize}
\item $\leftsub{} \cdot \vec{P}$ is a selector path
for $T$ with result $\leftsub{S}$.

\item $\rightsub{} \cdot \vec{P}$ is a selector path
for $T$ with result $\rightsub{S}$.
\end{itemize}

The \term{subbranches} $\subbrn{T}$ of $T$ and \term{proper subbranches}
$\propbrn{T}$ of $T$ are defined by:
\begin{align*}
\propbrn{T} & \eqdef \set{R \suchthat R\ \text{is the result of some
    \textbf{positive length} selector path of}\ T},\\
\subbrn{T} & \eqdef \set{R \suchthat R\ \text{is the result of some possibly empty
    selector path of}\ T}\\
          & = \propbrn{T} \union \set{T}.
\end{align*}
\end{definition}

Now if you go down from the top of $T$ to some subbranch $S$, and then
continue down from $S$ to some subbranch $R$, then $R$ is a subbranch
of \emph{both} $T$ and $S$.  This is illustrated in Figure~\ref{PQdown}.

\begin{figure}

%\graphic{}

\begin{center}
\begin{verbatim}
              T
               \
               /    } P
              /
             S
            /
     Q {   /
           \
            R
\end{verbatim}   
\end{center}

\caption{A subbranch $S$ of $T$ and  a subbranch $R$ of $S$.}

\label{PQdown}

\end{figure}


We can rephrase this trivial observation formally as:
\begin{lemma}\label{PSQRQPR}
If $T \in \brnch$ and $\vec{P}$ is a selector path for $T$ with result
$S$, and $\vec{Q}$ is a selector path for $S$ with result $R$, then
$\vec{Q}\vec{P}$ is a selector path of $T$ with result $R$.
\end{lemma}

Lemma~\ref{PSQRQPR} has two easy but important corollaries:
\begin{corollary}\label{propsubbranch}
If $S$ is a subbranch of $T$, and $R$ is a subbranch of $S$, then $R$
is a subbranch of $T$.  In addition, if $S$ is a proper subbranch of
$T$, then $R$ is a proper subbranch of $T$.

That is,
\begin{align*}
 S \in \subbrn{T} & \QIMP\ \subbrn{S} \subseteq \subbrn{T}\\
 S \in \propbrn{T} & \QIMP\ \subbrn{S} \subseteq \propbrn{T}.
 \end{align*}
\end{corollary}

\begin{corollary}\label{unionLR}
For $T \in \brnch$,
\begin{align*}
\propbrn{T} = \subbrn{\leftsub{T}} \union \subbrn{\rightsub{T}}
\end{align*}
\end{corollary}

\subsection{Binary Trees}

So far we have resisted saying that things in \brnch\ are ``trees,''
because the abstract setup we have allows some weird stuff to happen.
One weirdness is that nothing forbids ``circular'' branching
structures, that is, branching structures that are proper subbranches
of themselves.  For example, it is entirely possible that there is a
structure $T \in \brnch$ such that \textcolor{red}{
\[
\leftsub{T} = T = \rightsub{T}.
\]}
This weird binary branching structure doesn't even have any leaves.

Even if we forbid structures that are proper subbranches of
themselves, it is still possible to have infinite structures.  For
example, suppose we choose \brnch\ to be the nonnegative integers and
define $\leftsub{n} \eqdef 2n+1$ and $\rightsub{n} \eqdef 2n+2$.  Now we
get the infinite binary branching structure indicated in
Figure~\ref{inftree123}, which is another example with no leaves.

\begin{figure}

%\graphic{}

\begin{verbatim}
                  0
           1            2
        3     4      5      6
       7 8   9 10  11 12  13 14
       .          .             . 
      .           .              .
     .            .               . 

\end{verbatim}

\caption{An infinite tree with no leaves.}

\label{inftree123}

\end{figure}

A structure with an infinite path won't be a good search tree because
you might keep searching down the path forever.  We can stamp out all
the weirdness by forbidding infinite paths.

\begin{definition}
A infinite path in $T \in \brnch$ is infinite sequence
\[
\dots,f_n,\dots,f_1,f_0
\]
of selector functions such that every finite suffix
\[
\vec{P} = (f_n,\dots, f_1,f_0)
\]
is a selector path for $T$.

The \term{finite path trees} \fintr\ are the $T \in \brnch$ that do
not have an infinite path.
\end{definition}

It's useful, and very cool, to realize that the finite path trees have
a simple recursive definition.

\begin{definition}
The set \rectr\ of \term{recursive trees} is defined recursively.  For $T \in brnch$,

\inductioncase{Base case}: ($T$ is a leaf).  $T \in \rectr$.

\inductioncase{Constructor case}: ($T$ is not a leaf).
\[
\leftsub{T}, \rightsub{T} \in \rectr\ \QIMP T \in \rectr.
\]
\end{definition}

\begin{theorem}
The finite path trees are the same as the recursive trees:
\[
\fintr = \rectr.
\]
\end{theorem}

We first prove
\begin{equation}\label{rectrsubfintr}
\rectr \subseteq \fintr,
\end{equation}
that is, no recursive tree $T$ has an infinite path.

\begin{proof}
The proof follows immediately by structural induction on the
definition of \rectr:

\inductioncase{Base case}: ($T$ is a leaf).  $T$ has no infinite path
because it has no selector path of positive length at all.

\inductioncase{Constructor case}: ($T$ is not a leaf).  In this case,
$\leftsub{T}$ and $\rightsub{T}$ are defined and in \rectr.  By
induction hypothesis, neither $\leftsub{T}$ nor $\rightsub{T}$ has an
infinite path.

Now suppose to the contrary that $T$ had an infinite path
\[
\dots,f_n,\dots f_2,f_1,f_0.
\]
Now if $f_0 = \leftsub$, then
\begin{equation}\label{dfnf2f1}
\dots,f_n,\dots,f_2,f_1
\end{equation}
would be an infinite path in $\leftsub{T}$, a contradiction.  Likewise
if $f_0 = \rightsub$, then~\eqref{dfnf2f1} would be an infinite path
in $\rightsub{T}$, a contradiction.  So we get a contradiction in any
case, which implies that $T$ cannot have an infinite path.
\end{proof}

Second, we prove
\begin{equation}\label{fintrsubrectr}
\fintr \subseteq \rectr,
\end{equation}
that is, if $T \in \brnch$ is not a recursive tree, then it has an
infinite path.

\begin{proof}
If $T \notin \rectr$, then $T$ cannot be a leaf, and $\leftsub{T}$ and
$\rightsub{T}$ cannot both be in \rectr.  So let $f_0$ be a selector
such that $f_0(T) \notin \rectr$.  By the same reasoning, 
$\leftsub{f_0(T)}$ and $\rightsub{f_0(T)}$ cannot both be in
\rectr.  So let $f_1$ be a selector such that $f_1(f_0(T)) \notin
\rectr$.  Similarly, let $f_2$ be a selector such that $f_2(f_1(f_0(T))) \notin
\rectr$.  Continuing in this way, we get an infinite path
\[
\dots,f_n,\dots f_2,f_1,f_0
\]
in $T$.
\end{proof}

With the recursive definition of trees, we can now recursively define
some important, basic functions on trees.  For example,

\begin{definition}
The \term{depth} of $T \in \rectr$ is defined by:

\inductioncase{Base case}: ($T$ is a leaf).
\[
\trdpth{T} \eqdef 0.
\]

\inductioncase{Constructor case}: ($T$ is not a leaf).
\[
\trdpth{T} \eqdef \max\set{\trdpth{\leftsub{T}}, \trdpth{\rightsub{T}}} + 1.
\]
\end{definition}

\begin{problem}
Prove that if $T$ is a recursive tree \rectr, then $\trdpth{T}$ is the
length of the longest selector path of $T$.
\begin{solution}
 \TBA{TBA}
\end{solution}
\end{problem}

\begin{definition}
The \emph{size} of $T \in \rectr$ is defined by:

\inductioncase{Base case}: ($T$ is a leaf).
\[
\sizetr{T} \eqdef 1.
\]

\inductioncase{Constructor case}: ($T$ is not a leaf).
\[
\sizetr{T} \eqdef 1 + \sizetr{\leftsub{T}} + \sizetr{\rightsub{T}}.
\]
\end{definition}

We can now prove a basic relation between size and depth of recursive
trees:
\begin{theorem}
For all $T \in \rectr$,
\[
\sizetr{T} \leq 2^{\trdpth{T}+1} - 1.
\]
\end{theorem}

\begin{proof}
The proof is by structural induction on the definition of \rectr:

\inductioncase{Base case}: ($T$ is a leaf).
\[
\sizetr{T} \eqdef 1 = 2^1 - 1 = 2^{\trdpth{T}+1} - 1.
\]

\inductioncase{Constructor case}: ($T$ is not a leaf).
\begin{align*}
\sizetr{T}
& = 1 + \sizetr{\leftsub{T}} + \sizetr{\rightsub{T}}
              & \text{(def. $\sizetr{T}$)}\\
& \leq 1 + (2^{\trdpth{\leftsub{T}}+1} -1)  + (2^{\trdpth{\rightsub{T}}+1} -1)
              & \text{(ind. hypothesis)}\\
& \leq  2\cdot 2^{\max\set{\trdpth{\leftsub{T}}+1,\trdpth{\rightsub{T}}}+1} - 1\\
& = 2\cdot 2^{1+\max\set{\trdpth{\leftsub{T}},\trdpth{\rightsub{T}}}} - 1\\
& = 2\cdot 2^{\trdpth{T}} - 1
              & \text{(def of $\trdpth{T}$)}\\
& =  2^{\trdpth{T}+1} - 1.
\end{align*}
\end{proof}

The number of subtrees of a recursive tree is bounded by the size of the
tree.  That is, for $T \in \rectr$, 
\[
\card{\subbrn{T}} \leq \sizetr{T}.
\]
You might expect that equality should hold, but it may not.  We kept
the circular and infinite weirdness out of \rectr, but there still may
be some unexpected \emph{sharing} of subtrees.  For example, it is is
possible to have a \rectr\ whose left and right subtrees are the same,
or one whose top to bottom paths all end at the \emph{same} leaf.

Sharing of subtrees can be very useful in getting a compact
representation of a tree, but it seriously complicates adding or
removing values from search trees.  So we will finally restrict
ourselves to the free binary trees \freetr, in which sharing is
forbidden.

\begin{definition}
\freetr\ is the set of binary branching structures defined recursively
as follows:

\inductioncase{Base case}:  Any leaf is in \freetr.

\inductioncase{Constructor case}: If $T \in \brnch$ is not a leaf,
$\leftsub{T}, \rightsub{T} \in \freetr$, and these two free trees
have no subtree in common, that is,
\[
\leftsub{T} \intersect \rightsub{T} = \emptyset,
\]
then $T$ is in \freetr.
\end{definition}

Now it is easy to prove that in a free tree, the number of subtrees 
will be equal to the size of the tree.  That is, for $T \in \freetr$,
\begin{equation}\label{subbr=size}
\card{\subbrn{T}} = \sizetr{T}.
\end{equation}

\begin{problem}
%TP finger exercise

Prove~\eqref{subbr=size}
\begin{solution}
By Corollary~\ref{unionLR},
\[
\subbrn{T} = \set{T} \union \subbrn{leftsub{T}} \union \subbrn{rightsub{T}}.
\]
for $T \in \brnch$.  Now if $T \in \freetr$, these three sets are disjoint, so
\[
\card{\subbrn{T}} = 1 + \card{\subbrn{leftsub{T}}} + \card{\subbrn{rightsub{T}}}.
\]
Now~\eqref{subbr=size} follows immediately from the definition of $\sizetr$.
\end{solution}
\end{problem}

\begin{problem}
Prove that \freetr\ is the set of $T \in \brnch$ such that any two
different selector sequences for $T$ have different results.

\begin{solution}
\begin{proof}
\TBA{TBA}

\end{proof}
\end{solution}
\end{problem}

\subsection{The Structure of Binary Trees}

The leaves of a \brnch\ are its subbranches that are leaves.  That is
\[
\text{leaves}(T) \eqdef \subbrn{T} \intersect \leafset.
\]
The nonleaf subtrees are called ``internal.''  That is,
\[
\text{internal}(T) \eqdef \subbrn{T} - \text{leaves}(T).
\]

In free trees, there is always one more leaf than there are internal
subtrees:

\begin{lemma}\label{}
If $T \in \freetr$, then
\[
\card{\text{leaves}(T)} = 1 + \card{\text{internal}(T)}.
\]
\end{lemma}

\begin{proof}
The proof is by structural induction on the definition of \freetr.

\TBA{rest of proof}
\end{proof} 

A $T \in \rectr$ is \term{balanced} when all selector paths whose
results are leaves have length at least $\trdpth{T} -1$.

\begin{lemma}\label{}
If $B \in \freetr$ is balanced, then
\[
\card{\subbrn{B}} > 2^{\text{depth}(B)}.
\]
\end{lemma}

A free tree $T$ is an \term{AVL tree} when
\[
\abs{\trdpth{\leftsub{S}} - \trdpth{\rightsub{S}}} \leq 1
\]
for all subtrees $S \in \subbrn{T}$.

\begin{lemma}\label{}
If $T$ is AVL, then
\[
\card{\subbrn{T}} \geq 2^{\text{depth}(T/3)}???
\]
\end{lemma}

The search tree examples above have a label on each subbranch.
Abstractly, we model structures with labels by assuming there is a
total function
\[
\text{label}:\brnch \to \text{Labels}.
\]

\begin{definition}
The \term{search trees}, \srchtr, are the labelled trees $S$ with real
number as labels such that
\[
\text{label}(\leftsub{S}) < \text{label}(S) < \text{label}(\rightsub{S})
\]
for all $S \in \text{internal}{T}$.
\end{definition}

\begin{lemma}\label{}
Every \srchtr\ is a labelled \freetr.
\end{lemma}

Now we can define a recursive procedure that searches for a label that
matches any given number.  If it finds the number, it returns the
selector sequence that leads to it.
\begin{definition}
The function
\[
\text{search}: \srchtr \times \reals \to \set{\leftsub{},\rightsub{}}^* \union \set{\textbf{fail}}
\]
is defined recursively on the definition of \freetr.

%% look up notation for star

\inductioncase{Base case}: ($T$ is a leaf).
\[
\srchtr(T,r) = \begin{cases} 
\emptystring & \text{if}\ r = \text{label}(T),\\
\textbf{fail} & \text{otherwise}.
\end{cases}
\]

\inductioncase{Constructor case}:
\[
\srchtr(T,r) = \begin{cases} 
\emptystring & \text{if $r = \text{label}(T)$},\\
\textbf{fail} & \text{if $r \neq \text{label}(T)$ and $\leafp{T}$},\\
\leftsub{}\cdot \srchtr(\leftsub{T},r) & \text{if $r < \text{label}(T)$},\\
\rightsub{}\cdot \srchtr(\rightsub{T},r) & \text{if $r > \text{label}(T)$}.
\end{cases}
\]
\end{definition}

\begin{theorem}
If $r \notin \set{\text{label}(S) \suchthat S \in \subbrn{T}}$, then
$\srchtr(T,r) = \textbf{fail}$.  

If $r \in \set{\text{label}(S) \suchthat S \in \subbrn{T}}$, then
$\srchtr(T,r)$ is a selector sequence for $T$ and
\[
\text{label}(\text{result}(\srchtr(T,r)) = r.
\]
\end{theorem}

\begin{proof}

\TBA{By induction on the definition of \srchtr.}

\end{proof}

\subsection{Insertions \& Deletions}


  \endinput
