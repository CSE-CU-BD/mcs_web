\chapter{Deviation from the Mean}\label{deviation_chap}

\section{Why the Mean?}

In the previous chapter we took it for granted that expectation is
important, and we developed a bunch of techniques for calculating
\idx{expected value}s.  But why should we care about this value?
After all, a random variable may never take a value anywhere near its
expected value.

The most important reason to care about the mean value comes from its
connection to estimation by sampling.  For example, suppose we want to
estimate the average age, income, family size, or other measure of a
population.  To do this, we determine a random process for selecting
people ---say throwing darts at census lists.  This process makes the
selected person's age, income, and so on into a random variable whose
\emph{mean} equals the \emph{actual \idx{average}} age or income of the
population.  So we can select a random sample of people and calculate the
average of people in the sample to estimate the true average in the whole
population.  Many fundamental results of probability theory explain
exactly how the reliability of such estimates improves as the sample size
increases, and in this chapter we'll examine a few such results.

In particular, when we make an estimate by repeated sampling, we need to
know how much confidence we should have that our estimate is OK.
Technically, this reduces to finding the probability that an estimate
\emph{deviates} a lot from its expected value.  This topic of
\term{deviation from the mean} is the focus of this final chapter.

\begin{editingnotes}
A random variable may never take a value anywhere near its expected value,
so why is its expected value important?  The reason is suggested by a
property of gambling games that most people recognize intuitively.
Suppose your gamble hinges on the roll of two dice, where you win if the
sum of the dice is seven.  If the dice are fair, the probabilty you win is
$1/6$, which is also your expected number of wins in one roll.  Of course
there's no such thing as $1/6$ of a win in one roll, since either you win
or you don't.  But if you play \emph{many times}, you would expect that
the \emph{fraction} of times you win would be close to $1/6$.  In fact, if
you played a lot of times and found that your fraction of wins wasn't
pretty close to $1/6$, you would become pretty sure that the dice weren't
fair.
\end{editingnotes}


%\hyperdef{expect}{mean}{\section{Variance}}

\section{Markov's Theorem}

\begin{editingnotes}

The first result is Markov's Theorem, which gives a simple, but typically
coarse, upper bound on the probability that the value of a random variable
is more than a certain multiple of its mean.  Markov's result holds if we
know nothing about a random variable except what its mean is and that its
values are nonnegative.  Accordingly, Markov's Theorem is very general,
but also is much weaker than results which take into account more
information about the distribution of the variable.

In many situations, we not only know the mean, but also another numerical
quantity called the \emph{variance} of the random variable.  The second
basic result is Chebyshev's Theorem, which combines Markov's Theorem and
information about the variance to give more refined bounds.

\end{editingnotes}

Markov's theorem is an easy result that gives a generally rough estimate
of the probability that a random variable takes a value \emph{much larger}
than its mean.

The idea behind \idx{Markov's Theorem} can be explained with a simple
example of \emph{intelligence quotient}, \idx{\IQ}.  This quantity was
devised so that the average \IQ\ measurement would be 100.  Now from this
fact alone we can conclude that at most 1/3 the population can have an
\IQ\ of 300 or more, because if more than a third had an \IQ\ of 300, then
the average would have to be \emph{more} than $(1/3)300 = 100$,
contradicting the fact that the average is 100.  So the probability that a
randomly chosen person has an \IQ\ of 300 or more is at most 1/3.  Of
course this is not a very strong conclusion; in fact no \IQ\ of over 300
has ever been recorded.  But by the same logic, we can also conclude that
at most 2/3 of the population can have an \IQ\ of 150 or more.  \IQ's of
over 150 have certainly been recorded, though again, a much smaller
fraction than 2/3 of the population actually has an \IQ\ that high.

But although these conclusions about \IQ\ are weak, they are actually the
strongest general conclusions that can be reached about a random
variable using \emph{only} the fact that it is nonnegative and its mean is
100.  For example, if we choose a random variable equal to 300 with
probability 1/3, and 0 with probability 2/3, then its mean is 100, and the
probability of a value of 300 or more really is 1/3.  So we can't hope to
get a better upper bound based solely on this limited amount of
information.

\begin{editingnotes}

Note that very different distributions can still have the same mean.

\begin{example}
  Suppose that we roll a fair die.  This gives a random variable
  uniformly distributed on $1, 2, \dots 6$.  The mean, or expected
  value, is 3.5.  Of course, this random variable never takes on
  exactly the expected value; in fact, the outcome deviates from the
  mean by at least 0.5 with probability 1.  Furthermore, there is a
  $\frac{2}{3}$ probability that the outcome deviates from the mean by
  at least 1.5 (roll 1, 2, 5, or 6), a $\frac{1}{3}$ probability that
  the outcome deviates by at least 2.5 (roll 1 or 6), and zero
  probability that the outcome deviates by more than 2.5.
\end{example}

\begin{example}
  A random variable with the binomial distribution is much less likely
  to deviate far from the mean.  For example, suppose we flip 100
  fair, mutually independent coins and count the number of heads.  The
  expected number of heads is 50.  There is an 8\% chance that the
  outcome is exactly the mean, and the probability of flipping more
  than 75 heads or fewer than 25 is less than 1 in a billion.
\end{example}

The probability distribution functions for the two preceding examples
are graphed in Figure~\ref{fig:uniform} and Figure~\ref{fig:binom2}.
There is a big difference!  For the uniform distribution, the graph is
flat; that is, outcomes far from the mean are as likely as outcomes
close to the mean.  However, the binomial distribution has a peak
centered on the expected value and the tails fall off rapidly.  This
shape implies that outcomes close to the expected value are vastly
more likely than outcomes far from the expected value.  In other
words, a random variable with the binomial distribution rarely
deviates far from the mean.
\begin{figure}
  \centerline{\includegraphics[height=2in]{uniform}}
  \caption{This is a graph of the uniform distribution arising from
    rolling a fair die.  Outcomes within the range of the distribution
    are equally likely, regardless of distance from the mean.}
  \label{fig:uniform}
\end{figure}
\begin{figure}
  \centerline{\includegraphics[height=2in]{binom2}}
  \caption{This is a rough graph of the binomial distribution given by
    the number of heads that come up when we flip 100 fair, mutually
    independent coins.  Outcomes close to the mean are much more
    likely than outcomes far from the mean.}
  \label{fig:binom2}
\end{figure}

On the other hand, we can define a random variable that always
deviates substantially from its expected value.  Suppose that we glue
100 coins together, so that with probability 1/2 all are heads and
with probability 1/2 all are tails.  The graph of the probability
distribution function for the number of heads is shown in
Figure~\ref{fig:nasty}.  While the expected value of this random
variable is 50, the actual value is always 0 or~100.
\begin{figure}
  \centerline{\includegraphics[height=2in]{nasty}}
  \caption{This is the nasty distribution corresponding to the number
    of heads that come up when we flip 100 coins that are all glued
    together. The outcome always differs from the mean by at
    least~50.}
  \label{fig:nasty}
\end{figure}

Even in this last example, however, the random variable is twice the
mean with probability only $1/2$.  In fact, we will see that this is a
worst-case distribution with respect to deviation from the mean.

\subsection*{Theorem Statement and Some Applications}

\end{editingnotes}

\begin{theorem}[\idx{Markov's Theorem}]\label{markovthm}
  If R is a nonnegative random variable, then for all $x > 0$
  \begin{displaymath}
    \pr{R \geq x} \leq \frac{\expect{R}}{x}.
  \end{displaymath}
\end{theorem}

\begin{editingnotes}

Before we prove Markov's Theorem, let's apply it to the three examples in
the preceding subsection.  First, let the random variable~$R$ be the
number that comes up when we roll a fair die.  By Markov's Theorem, the
probability of rolling a 6 is at most:
\[
\pr{R \geq 6} \leq \frac{\expect{R}}{6} = \frac{3.5}{6} = 0.583\dots
\]
This conclusion is true, but weak.  The actual probability of rolling
a 6 is $1/6 = 0.166\dots$.

This is typical of Markov's Theorem.  The theorem is easy to apply
because it requires so little information about a random variable,
only the expected value and nonnegativity.  But as a consequence,
Markov's Theorem often leads to weak conclusions like the one above.

Suppose that we flip 100 mutually independent, fair coins.  Markov's
Theorem says that the probability of throwing 75 or more heads is at
most:
\[
\pr{\text{heads} \geq 75} \leq \frac{\expect{\text{heads}}}{75} =
\frac{50}{75} = \frac{2}{3}.
\]
Markov's Theorem says that the probability of 75 or more heads is at
most $2/3$, but the actual probability is less than 1 in a
billion!

These two examples show that Markov's Theorem gives weak results for
well-behaved random variables; however, the theorem is actually tight
for some nasty examples.  Suppose we flip 100 fair coins and use
Markov's Theorem to compute the probability of getting all heads:
\[
\pr{\text{heads} \geq 100} \leq \frac{\expect{\text{heads}}}{100} =
\frac{50}{100} = \frac{1}{2}.
\]
If the coins are mutually independent, then the actual probability of
getting all heads is a miniscule 1 in $2^{100}$.  In this case, Markov's
Theorem looks very weak.  However, in applying Markov's Theorem, we made
no independence assumptions.  In fact, if all the coins are glued
together, then probability of throwing all heads is exactly $1/2$.
In this nasty case, Markov's Theorem is actually tight!

\subsection{Proof of Markov's Theorem}

Let $R$ be the weight of a person selected randomly and uniformly.
Suppose that an average person weighs 100 pounds; that is, $\expect{R} =
100$.  What is the probability that a random person weighs at least
200 pounds?

There is insufficient information for an exact answer.  However, we
can safely say that the probability that $R \geq 200$ is most
$1/2$.  If more than half of the people weigh 200 pounds or
more, then the average weight would exceed 100 pounds, even if
everyone else weighed zero!  Markov's Theorem gives the same result:
\begin{displaymath}
  \pr{R \geq 200} \leq \frac{\expect{R}}{200} = \frac{100}{200} = \frac{1}{2}.
\end{displaymath}

Reasoning similar to that above underlies the proof of Markov's
Theorem.  Since expectation is a weighted average of all the outcomes
of the random variable, that is, a sum over all the variables the random
variable can assume, we can give a lower bound on the expectation by
removing some of the terms from the sum defining the expectation; this
new sum can then be modified into an expression involving the
probability of an event in the tail $[R \geq x]$.

\end{editingnotes}

\begin{proof}%[Proof of Markov's Theorem]
For any $x > 0$
\begin{align}
  \expect{R}
  & \eqdef \sum_{y \in \range{R}} y\pr{R=y}\notag\\
  & \geq \sum_{\substack{y \geq x,\\ y \in \range{R}}} y\pr{R=y} & \text{(because $R \geq 0$)}\notag\\
  & \geq \sum_{\substack{y \geq x,\\ y \in \range{R}}} x\pr{R=y}\notag\\
  & = x \sum_{\substack{y \geq x,\\ y \in \range{R}}} \pr{R=y}\notag\\
  & = x \pr{R \geq x}.\label{markovproof}
\end{align}
Dividing the first and last expression~\eqref{markovproof} by $x$ gives
the desired result.
\end{proof}

\iffalse
We will show that $\expect{R} \geq x \pr{R \geq x}$.  Dividing
both sides by $x$ gives the desired result.

So let $I_x$ be the indicator variable for the event $[R \geq x]$, and
consider the random variable $x I_x$.  Note that
\[
R \geq x I_x,
\]
because at any sample point, $\omega$,
\begin{itemize}
\item if $R(\omega) \geq x$ then $R(\omega) \geq x = x\cdot 1 = x I_x(\omega)$, and
\item if $R(\omega) < x$ then $R(\omega) \geq 0 = x \cdot 0 = xI_x(\omega)$.
\end{itemize}
Therefore,
\begin{align*}
\expect{R} & \geq \expect{x I_x} & (\text{since } R \geq xI_x)\\
   & = x \expect{I_x} & \text{(linearity of $\expect{\cdot}$)}\\
   & = x \pr{I_x=1}  &  \text{($I_x$ is an indicator)}\\
   & = x \pr{R \geq x}.  &  (\text{def\ of $I_x$})
\end{align*}

\fi

Our focus is deviation from the mean, so it's useful to rephrase
Markov's Theorem this way:
\begin{corollary}
If $R$ is a nonnegative random variable, then for all $c \geq 1$
\begin{equation}
\pr{R \geq c \cdot \expect{R}\ }  \leq  \frac{1}{c}.\label{markovaboveemean}
\end{equation}
\end{corollary}
This Corollary follows immediately from Markov's Theorem\eqref{markovthm}
by letting $x$ be $c \cdot \expect{R}$.

\iffalse
This gives:
\[
\pr{R \geq c \cdot \expect{R}\ } \leq \frac{\expect{R}}{c \cdot \expect{R}} =
\frac{1}{c}.
\]
\end{proof}
\fi

\subsection{Applying Markov's Theorem}

Let's consider the \idx{Hat-Check problem} again.  Now we ask what the
probability is that $x$ or more men get the right hat, this is, what the
value of $\pr{G \geq x}$ is.

We can compute an upper bound with Markov's Theorem.  Since we know
$\expect{G}=1$, Markov's Theorem implies
\[
\pr{G \geq x} \leq \frac{\expect{G}}{x} = \frac{1}{x}.
\]
For example, there is no better than a 20\% chance that 5 men get the
right hat, regardless of the number of people at the dinner party.

The \idx{Chinese Appetizer problem} is similar to the Hat-Check problem.
In this case, $n$ people are eating appetizers arranged on a circular,
rotating Chinese banquet tray.  Someone then spins the tray so that each
person receives a random appetizer.  What is the probability that everyone
gets the same appetizer as before?

There are $n$ equally likely orientations for the tray after it stops
spinning.  Everyone gets the right appetizer in just one of these $n$
orientations.  Therefore, the correct answer is $1/n$.

But what probability do we get from Markov's Theorem?  Let the random
variable, $R$, be the number of people that get the right appetizer.  
%We showed in previous notes that $\expect{R} = 1$.  
Then of course $\expect{R} = 1$ (right?), so
applying Markov's Theorem, we find:
\begin{displaymath}
  \pr{R \geq n} \leq \frac{\expect{R}}{n} = \frac{1}{n}\,.
\end{displaymath}
So for the Chinese appetizer problem, Markov's Theorem is tight!

On the other hand, Markov's Theorem gives the same $1/n$ bound for the
probability everyone gets their hat in the Hat-Check problem in the case
that all permutations are equally likely.  But the probability of this
event is $1/(n!)$.  So for this case, Markov's Theorem gives a probability
bound that is way off.

\subsection{Markov's Theorem for Bounded Variables}

Suppose we learn that the average \IQ\ among MIT students is 150 (which is
not true, by the way).  What can we say about the probability that an MIT
student has an \IQ\ of more than 200?  Markov's theorem immediately tells
us that no more than $150/200$ or $3/4$ of the students can have such a
high \IQ.  Here we simply applied Markov's Theorem to the random variable,
$R$, equal to the \IQ\ of a random MIT student to conclude:
\[
\pr{R > 200} \leq \frac{\expect{R}}{200}= \frac{150}{200} = \frac{3}{4}.
\]

But let's observe an additional fact (which may be true): no MIT student
has an \IQ\ less than 100.  This means that if we let $T \eqdef R-100$,
then $T$ is nonnegative and $\expect{T} = 50$, so we can apply Markov's
Theorem to $T$ and conclude:
\[
\pr{R > 200} = \pr{T > 100} \leq \frac{\expect{T}}{100}= \frac{50}{100} =
\frac{1}{2}.
\]
So only half, not 3/4, of the students can be as amazing as they think
they are.  A bit of a relief!

In fact, we can get better bounds applying Markov's Theorem to $R-b$
instead of $R$ for any lower bound $b>0$ on $R$ (see
Problem~\ref{PS_Markov_with_bounded_RVs}).  Similarly, if we have any
upper bound, $u$, on a random variable, $S$, then $u-S$ will be a
nonnegative random variable, and applying Markov's Theorem to $u-S$
will allow us to bound the probability that $S$ is much \emph{less}
than its expectation.

\iffalse
Suppose we know that $R \geq \ell$, then can we do better?
Let $T=R-\ell$.  Note that $T \geq 0$.  So, we can use Markov's
Theorem on $T$, to say that 
\begin{eqnarray*}
\pr{R  \geq x }   & = &   \pr{T \geq x -\ell} 
  \leq 
  \frac{\expect{T}}{x -\ell} 
  =   \frac{\expect{R - \ell}}{x - \ell}
  =   \frac{\expect{R} - \ell}{x - \ell} \\
  %& < &  \frac{\expect{R}}{x}
\end{eqnarray*}
%$\pr{R - \ell \geq x} = \pr{T \geq x} 
%\leq 
%\frac{\expect{T}}{x} 
%= \frac{\expect{R - \ell}}{x} =
%= \frac{\expect{R}{x} - \ell/x$
This gives a somewhat better bound on the probability that
$R$ goes crazy!  
\fi

\begin{editingnotes}

\subsection{Why \emph{R} Must be Nonnegative}

Remember that Markov's Theorem applies only to nonnegative random
variables!  The following example shows that the theorem is false if this
restriction is removed.  Let $R$ be -10 with probability $1/2$ and 10 with
probability $1/2$.  Then we have:
\[
\expect{R} = -10 \cdot \frac{1}{2} + 10 \cdot \frac{1}{2} = 0
\]
Suppose that we now tried to compute $\pr{R \geq 5}$ using Markov's
Theorem:
\begin{displaymath}
  \pr{R \geq 5} \leq \frac{\expect{R}}{5} = \frac{0}{5} = 0.
\end{displaymath}
This is the wrong answer!  Obviously, $R$ is at least 5 with
probability $1/2$.  

On the other hand, we can still apply Markov's Theorem indirectly to
derive a bound on the probability that an arbitrary variable like $R$ is 5
more.  Namely, given any random variable, $R$ with expectation 0 and
values $\geq -10$, we can conclude that $\pr{R \geq 5} \le 2/3$.
\begin{proof}
Let $T \eqdef R+10$.  Now $T$ is a nonnegative random variable with
expectation $\expect{R + 10} = \expect{R}+10= 10$, so Markov's Theorem
applies and tells us that $\pr{T \geq 15} \le 10/15 = 2/3$.  But $T \geq
15$ iff $R \geq 5$, so $\pr{R \geq 5} \leq 2/3$, as claimed.
\end{proof}

\subsection{Deviation Below the Mean}

Markov's Theorem says that a random variable is unlikely to greatly exceed
the mean.  Correspondingly, there is a theorem that says a random variable
is unlikely to be much smaller than its mean.

\begin{theorem}
\label{th:below}
Let $l$ be a real number and let $R$ be a random variable such that $R
\leq l$.  For all $x < l$, we have:
\[
\pr{R \leq x} \leq \frac{l - \expect{R}}{l - x}.
\]
\end{theorem}

\begin{proof}
The event that $R \leq x$ is the same as the event that $l - R \geq l -
x$.  Therefore:
\begin{align}
\pr{R \leq x} &  = \pr{l - R \geq l - x}\notag\\
 & \leq \frac{\expect{l - R}}{l - x}. & \text{(by Markov' Theorem)}\label{LR}
\end{align}
Applying Markov's Theorem in line~\eqref{LR} is permissible
since $l - R$ is a nonnegative random variable and $l - x > 0$.
\end{proof}

For example, suppose that the class average on the 6.042 midterm was
75/100.  What fraction of the class scored below 50?

There is not enough information here to answer the question exactly,
but Theorem~\ref{th:below} gives an upper bound.  Let $R$ be the score
of a random student.  Since 100 is the highest possible score, we
can set $L = 100$ to meet the condition in the theorem that $R \leq
L$.  Applying Theorem~\ref{th:below}, we find:
\begin{displaymath}
  \pr{R \leq 50} \leq \frac{100 - 75}{100 - 50} = \frac{1}{2}\,.
\end{displaymath}

That is, at most half of the class scored 50 or worse.  This makes
sense; if more than half of the class scored 50 or worse, then the
class average could not be 75, even if everyone else scored 100.
As with Markov's Theorem, Theorem~\ref{th:below} often gives weak
results.  In fact, based on the data given, the entire class could
have scored above 50.

\end{editingnotes}

\begin{editingnotes}
\subsubsection*{Using Markov To Analyze Non-Random Events}

In the previous examples, we used a theorem about a random variable to
conclude facts about non-random data.  For example, we concluded that
if the average score on a test is 75, then at most $1/2$ the
class scored 50 or worse.  There is no randomness in this problem,
so how can we apply Theorem~\ref{th:below} to reach this conclusion?

The explanation is not difficult.  For any set of scores $S = \set{s_1,
s_2, \dots, s_n}$, we introduce a random variable, $R$, such that
\[
\pr{R = s_i} = \frac{\text{(\# of students with score $s_i$)}}{n}
\]
We then use Theorem~\ref{th:below} to conclude that $\pr{R \leq 50}
\leq 1/2$.  To see why this means (with certainty) that at most
$1/2$ of the students scored 50 or less, we observe that
\begin{eqnarray*}
\pr{R \leq 50}  & = & \sum_{s_i \leq 50} \pr{R = s_i} \\
  & = & \sum_{s_i \leq 50} \frac{\text{(\# of students with score $s_i$)}}{n} \\
  & = & \frac{1}{n} \text{(\# of students with score 50 or less)}.
\end{eqnarray*}
So, if $\pr{R \leq 50} \leq 1/2$, then the number of students
with score 50 or less is at most $n/2$.

\end{editingnotes}

\begin{problems}

\classproblems
\pinput{CP_cold_cows_markov}

\homeworkproblems
\pinput{PS_Markov_with_bounded_RVs}
\end{problems}

\section{Chebyshev's Theorem}

We got more mileage out of Markov's Theorem by applying it to $R-b$
rather than $R$.  More generally, a really good trick for getting
stronger bounds on a random variable $R$ out of Markov's Theorem is to
apply some cleverly chosen function of $R$.

Choosing functions that are powers of $\abs{R}$ turns out to be
specially useful.  In particular, since $\abs{R}^\alpha$ is
nonnegative, Markov's inequality also applies to the event
$[\abs{R}^\alpha \geq x^\alpha]$.  But this event is equivalent to the
event $[\abs{R} \geq x]$, so we have:

\iffalse
It is a bit messy to apply Markov's Theorem directly to this problem,
because it's generally not easy to compute $\expect{\ \abs{R -
\expect{R}}\ }$.  However, since $\abs{R}$ and hence $\abs{R}^k$ are
nonnegative variables for any $R$, Markov's inequality also applies to the
event $[\abs{R}^k \geq x^k]$.  But this event is equivalent to the event
$[\abs{R} \geq x]$, so we have:
\fi

\begin{lemma}\label{lem:Markov2}
For any random variable~$R$, $\alpha \in \reals^+$, and $x > 0$,
\[
\pr{\abs{R} \geq x} \leq \frac{\expect{\abs{R}^\alpha}}{x^\alpha}.
\]
\end{lemma}
Rephrasing~\eqref{lem:Markov2} in terms of the random variable, $\abs{R -
  \expect{R}}$, that measures $R$'s deviation from its mean, we get

\begin{equation}\label{chebE2}
  \pr{\abs{R - \expect{R}\ } \geq x} \leq \frac{\expect{(R - \expect{R})^\alpha}}{x^\alpha}.
\end{equation}
The case when $\alpha =2$ is turns out to be so important that numerator
of the right hand side of~\eqref{chebE2} has been given a name:

\begin{definition}\label{defvar}
The \term{variance}, $\variance{R}$, of a random variable, $R$, is:
\[
\variance{R} \eqdef \expect{(R - \expect{R})^2}.
\]
\end{definition}

The restatement of~\eqref{chebE2} for $\alpha=2$ is known as \term{Chebyshev's
  Theorem}.
\begin{theorem}[Chebyshev]\label{chebthm}
  Let $R$ be a random variable and $x \in \reals^+$.  Then
\[
\pr{\abs{R - \expect{R}} \geq x} \leq \frac{\variance{R}}{x^2}.
\]
\end{theorem}

The expression $\expect{(R - \expect{R})^2}$ for variance is a bit
cryptic; the best approach is to work through it from the inside out.  The
innermost expression, $R - \expect{R}$, is precisely the deviation of $R$
above its mean.  Squaring this, we obtain, $(R - \expect{R})^2$.  This is
a random variable that is near 0 when $R$ is close to the mean and is a
large positive number when $R$ deviates far above or below the mean.  So
if $R$ is always close to the mean, then the variance will be small.  If
$R$ is often far from the mean, then the variance will be large.

\subsection{Variance in Two Gambling Games}

The relevance of variance is apparent when we compare the following
two gambling games.

\textbf{Game A:} We win \$2 with probability $2/3$ and lose \$1 with probability
$1/3$.

\textbf{Game B:} We win \$1002 with probability $2/3$ and lose \$2001 with
probability $1/3$.

Which game is better financially?  We have the same probability, 2/3,
of winning each game, but that does not tell the whole story.  What about
the expected return for each game?  Let random variables $A$ and $B$ be
the payoffs for the two games.  For example, $A$ is 2 with probability
2/3 and -1 with probability 1/3.  We can compute the
expected payoff for each game as follows:
\begin{eqnarray*}
\expect{A} = 2 \cdot \frac{2}{3} + (-1) \cdot \frac{1}{3} = 1, \\
\expect{B} = 1002 \cdot \frac{2}{3} + (-2001) \cdot \frac{1}{3} = 1.
\end{eqnarray*}

The expected payoff is the same for both games, but they are obviously
very different!  This difference is not apparent in their expected value,
but is captured by variance.  We can compute the $\variance{A}$ by working
``from the inside out'' as follows:
\begin{eqnarray*}
A - \expect{A}
        & = &   \left\{
                \begin{array}{cl}
                        1 & \text{ with probability } \frac{2}{3} \\
                        -2 & \text{ with probability } \frac{1}{3}
                \end{array}
                \right. \\
(A - \expect{A})^2
        & = &   \left\{
                \begin{array}{cl}
                        1 & \text{ with probability } \frac{2}{3} \\
                        4 & \text{ with probability } \frac{1}{3}
                \end{array}
                \right. \\
\expect{(A - \expect{A})^2}
        & = &   1 \cdot \frac{2}{3} + 4 \cdot \frac{1}{3} \\
\variance{A} & = & 2.
\end{eqnarray*}

Similarly, we have for $\variance{B}$:
\begin{eqnarray*}
B - \expect{B}
        & = &   \left\{
                \begin{array}{cl}
                        1001 & \text{ with probability } \frac{2}{3} \\
                        -2002 & \text{ with probability } \frac{1}{3}
                \end{array}
                \right. \\
(B - \expect{B})^2
        & = &   \left\{
                \begin{array}{cl}
                        1,002,001 & \text{ with probability } \frac{2}{3} \\
                        4,008,004 & \text{ with probability } \frac{1}{3}
                \end{array}
                \right. \\
\expect{(B - \expect{B})^2}
        & = &   1,002,001 \cdot \frac{2}{3} + 4,008,004 \cdot \frac{1}{3} \\
\variance{B} & = & 2,004,002.
\end{eqnarray*}

The variance of Game A is 2 and the variance of Game B is more than
two million!  Intuitively, this means that the payoff in Game A is
usually close to the expected value of \$1, but the payoff in Game B
can deviate very far from this expected value.

High variance is often associated with high risk.  For example, in ten
rounds of Game A, we expect to make \$10, but could conceivably lose
\$10  instead.  On the other hand, in ten rounds of game B, we also
expect to make \$10, but could actually lose more than \$20,000!

\subsection{Standard Deviation}

Because of its definition in terms of the square of a random variable, the
variance of a random variable may be very far from a typical deviation
from the mean.  For example, in Game B above, the deviation from the mean
is 1001 in one outcome and -2002 in the other. But the variance is a
whopping 2,004,002.  From a dimensional analysis viewpoint, the ``units''
of variance are wrong: if the random variable is in dollars, then the
expectation is also in dollars, but the variance is in square dollars.
For this reason, people often describe random variables using standard
deviation instead of variance.

\begin{definition}
The \term{standard deviation}, $\sigma_R$, of a random variable, $R$, is
the square root of the variance:
\[
\sigma_R \eqdef \sqrt{\variance{R}} = \sqrt{\expect{(R - \expect{R})^2}}.
\]      
\end{definition}

So the standard deviation is the square root of the mean of the square of
the deviation, or the \term{root mean square} for short.  It has the same
units ---dollars in our example ---as the original random variable and as
the mean.  Intuitively, it measures the average deviation from the mean,
since we can think of the square root on the outside as canceling the
square on the inside.

\begin{example}
The standard deviation of the payoff in Game B is:
\[
    \sigma_B  = \sqrt{\variance{B}} = \sqrt{2,004,002} \approx 1416.
\]

The random variable~$B$ actually deviates from the mean by either
positive 1001  or negative 2002; therefore, the standard
deviation of 1416 describes this situation reasonably well.
\end{example}

Intuitively, the standard deviation measures the ``width'' of the ``main
part'' of the distribution graph, as illustrated in
Figure~\ref{fig:stdev}.
\begin{figure}
  \centerline{\includegraphics[height=2in]{stdev}}
  \caption{The standard deviation of a distribution indicates how wide the
    ``main part'' of it is.}
  \label{fig:stdev}
\end{figure}

It's useful to rephrase Chebyshev's Theorem in terms of standard
deviation.
\begin{corollary}
\label{cor:cheby}
Let $R$ be a random variable, and let $c$ be a positive real number.
\[
\pr{\abs{R - \expect{R}} \geq c \sigma_R} \leq \frac{1}{c^2}.
\]
\end{corollary}
Here we see explicitly how the ``likely'' values of $R$ are clustered in
an $O(\sigma_R)$-sized region around $\expect{R}$, confirming that the
standard deviation measures how spread out the distribution of $R$ is
around its mean.

\begin{proof}
  Substituting $x = c \sigma_R$ in Chebyshev's Theorem gives:
  \begin{displaymath}
    \pr{\card{R - \expect{R}} \geq c \sigma_R}
    \leq
    \frac{\variance{R}}{(c \sigma_R)^2}
    =  \frac{\sigma_R^2}{(c \sigma_R)^2}
    = \frac{1}{c^2}.
  \end{displaymath}
\iffalse
  The last equality holds because variance is the square of standard
  deviation: $\variance{R} = \sigma_R^2$.
\fi

\end{proof}

\subsubsection{The IQ\ Example}\label{IQsec}

Suppose that, in addition to the national average \idx{\IQ}\ being 100, we
also know the \idx{standard deviation} of \IQ's is 10.  How rare is an
\IQ\ of 300 or more?

Let the random variable, $R$, be the \IQ\ of a random person.  So we are
supposing that $\expect{R} = 100$, $\sigma_R = 10$, and $R$ is
nonnegative.  We want to compute $\pr{R \geq 300}$.

We have already seen that Markov's Theorem~\ref{markovthm} gives a coarse
bound, namely,
\[
  \pr{R \geq 300} \leq \frac{1}{3}.
\]
Now we apply Chebyshev's Theorem to the same problem:
\[
\pr{R \geq 300} = \pr{\abs{R - 100} \geq 200} \leq
\frac{\variance{R}}{200^2} = \frac{10^2}{200^2} = \frac{1}{400}.
\]
\iffalse
The purpose of the first step is to express the desired probability in the
form required by Chebyshev's Theorem; the equality holds because $R$ is
nonnegative.  Chebyshev's Theorem then yields the inequality.\fi

So Chebyshev's Theorem implies that at most one person in four hundred has
an \IQ\ of 300 or more.  We have gotten a much tighter bound using the
additional information, namely the variance of $R$, than we could get
knowing only the expectation.

\section{Properties of Variance}

The definition of variance of $R$ as $\expect{(R - \expect{R})^2}$ may
seem rather arbitrary.
%
\begin{editingnotes}

The variance is the average \emph{of the square} of the deviation from the
mean.  For this reason, variance is sometimes called the ``mean squared
deviation.''  But why bother squaring?  Why not simply compute the average
deviation from the mean?  That is, why not define variance to be
$\expect{R - \expect{R}\ }$?

The problem with this definition is that the positive and negative
deviations from the mean exactly cancel.  By linearity of expectation,
we have:
\[
  \expect{R - \expect{R}} = \expect{R} - \expect{\expect{R}}.
\]
Since $\expect{R}$ is a constant, its expected value is itself. Therefore
\[
\expect{R - \expect{R}} = \expect{R} - \expect{R} = 0.
\]
By this definition, every random variable has zero variance.  That is not
useful!  Because of the square in the conventional definition, both
positive and negative deviations from the mean increase the variance;
positive and negative deviations do not cancel.

Of course, we could also prevent positive and negative deviations from
canceling by taking an absolute value.
\end{editingnotes}
%
A direct measure of average deviation would be $\expect{\ \abs{R -
    \expect{R}}\ }$.  But the direct measure doesn't have the many useful
properties that variance has, which is what this section is about.

\iffalse
For example, for independent random variables, the variance of a sum
is the sum of the variances; that is, $\variance{R_1 + R_2} =
\variance{R_1} + \variance{R_2}$.  We will prove this fact below.
\fi

\subsection{A Formula for Variance}

Applying linearity of expectation to the formula for variance yields a convenient
alternative formula.
\begin{lemma}\label{alt:var}
\[
\variance{R} = \expect{R^2} - \expectsq{R},
\]
for any random variable, $R$.
\end{lemma}
Here we use the notation \idx{$\expectsq{R}$} as shorthand for
$(\expect{R})^2$.

\begin{editingnotes}
Remember that $\expect{R^2}$ is generally not equal to $\expectsq{R}$.  We
know the expected value of a product is the product of the expected values
for independent variables, but not in general.  And $R$ is not independent
of itself unless it is constant.

\end{editingnotes}

\begin{proof}
Let $\mu = \expect{R}$.  Then
\begin{align*}
\variance{R} & =   \expect{(R - \expect{R})^2}
               & \text{(Def~\ref{defvar} of variance)}\\
        & = \expect{(R - \mu)^2} & \text{(def of $\mu$)}\\
        & = \expect{R^2 - 2  \mu R + \mu^2} \\
        & = \expect{R^2} - 2 \mu \expect{R} + \mu^2 
                & \text{(linearity of expectation)}\\
        & = \expect{R^2} - 2 \mu^2 + \mu^2
              &  \text{(def of $\mu$)}\\
        & = \expect{R^2} - \mu^2\\
        & = \expect{R^2} - \expectsq{R}.
                  &  \text{(def of $\mu$)}
\end{align*}
\end{proof}

For example, if $B$ is a \idx{Bernoulli variable} where $p\eqdef
\pr{B=1}$, then
\begin{lemma}\label{bernoulli-variance}
\begin{equation}%\label{bv}
\variance{B} = p-p^2 = p(1-p).
\end{equation}
\begin{proof}
  By Lemma~\ref{expindic}, $\expect{B}= p$.  But since $B$ only takes
  values 0 and 1, $B^2 = B$.  So Lemma~\ref{bernoulli-variance} follows
  immediately from Lemma~\ref{alt:var}.
\end{proof}

\end{lemma}

\subsection{Variance of Time to Failure}
According to section~\ref{mean_time_to_failure_subsec}, the mean time to
failure is $1/p$ for a process that fails during any given hour with
probability $p$.  What about the variance?  That is, let $C$ be the hour
of the first failure, so $\pr{C=i} = (1-p)^{i-1}p$.  We'd like to find a
formula for $\variance{C}$.

By Lemma~\ref{alt:var},
\begin{equation}\label{varCEC2}
\variance{C} = \expect{C^2} - (1/p)^2
\end{equation}
so all we need is a formula for $\expect{C^2}$.

\iffalse

\begin{align}
\expect{C^2}
   & \eqdef \sum_{i\geq 1} i^2(1-p)^{i-1}p \notag\\ %\label{var_sum_time_to_fail}
   & = p\sum_{i\geq 1} i^2x^{i-1} & \text{(where $x=1-p$)}\label{time_to_fail_gen_func}.
\end{align}
But~\eqref{squares_gen_func} gives the generating function
$x(1+x)/(1-x)^3$ for the nonnegative integer squares, and this implies that
the generating function for the sum in~\eqref{time_to_fail_gen_func} is
$(1+x)/(1-x)^3$.  So,
\begin{align}
\expect{C^2} & = p\, \frac{(1+x)}{(1-x)^3} & \text{(where $x=1-p$)}\notag\\
             & = p\, \frac{2+p}{p^3}\notag\\
             & = \frac{1-p}{p^2} + \frac{1}{p^2}\label{plus1p2},
\end{align}
Combining~\eqref{varCEC2} and~\eqref{plus1p2} gives a simple answer:
\begin{equation}\label{var_time_to_fail}
\variance{C} = \frac{1-p}{p^2} \,.
\end{equation}

It's great to be able apply generating function expertise to knock off
equation~\eqref{var_time_to_fail} mechanically just from the definition of
variance, but there's a more elementary, and memorable, alternative.\fi
In
section~\ref{mean_time_to_failure_subsec} we used conditional expectation
to find the mean time to failure, and a similar approach works for the
variance.  Namely, the expected value of $C^2$ is the probability, $p$, of
failure in the first hour times $1^2$, plus $(1-p)$ times the expected
value of $(C+1)^2$.  So
\begin{align*}
\expect{C^2} & = p\cdot 1^2 + (1-p)\expect{(C+1)^2}\\
             & = p + (1-p) \paren{\expect{C^2} + \frac{2}{p} +1}\\
             & = p+ (1-p)\expect{C^2} + (1-p)\paren{\frac{2}{p} + 1}
                \text{  so}\\[5pt]
p \expect{C^2} & = p+ (1-p)\paren{\frac{2}{p} + 1}\\
               & = \frac{p^2+(1-p)(2+p)}{p}\text{  and}\\[5pt]
\expect{C^2} & = \frac{2 -p}{p^2} % =  \frac{1-p}{p^2} + \frac{1}{p^2}
\end{align*}
%which directly simplifies to~\eqref{plus1p2}.

\begin{editingnotes}

Lemma~\ref{alt:var} gives a convenient way to compute the variance of a
random variable: find the expected value of the square and subtract the
square of the expected value.  For example, we can compute the variance of
the outcome of a fair die as follows:
\begin{gather*}
  \expect{R^2} = \frac{1}{6} (1^2 + 2^2 + 3^2 + 4^2 + 5^2 + 6^2) = \frac{91}{6}, \\
  \expectsq{R} = \left(3 \frac{1}{2}\right)^2 = \frac{49}{4}, \\
  \variance{R}  = \expect{R^2} - \expectsq{R}
  = \frac{91}{6} - \frac{49}{4} = \frac{35}{12}.
\end{gather*}

This result is particularly useful when we want to estimate the variance
of a random variable from a sequence $x_1,x_2,\dots,x_n$, of sample values
of the variable.

\begin{definition*}
For any sequence of real numbers $x_1,x_2,\dots,x_n$, define the
\emph{sample mean}, $\mu_n$, and the \emph{sample variance}, $v_n$, of the
sequence to be:
\begin{eqnarray*}
\mu_n  & \eqdef & \frac{\sum_{i=1}^n x_i}{n},\\
v_n  & \eqdef & \frac{\sum_{i=1}^n (x_i - \mu_n)^2}{n}.
\end{eqnarray*}
\end{definition*}
Notice that if we define a random variable, $R$, which is equally likely
to take each of the values in the sequence, that is $\pr{R = x_i} = 1/n$
for $i = 1,\dots,n$, then $\mu_n = \expect{R}$ and $v_n = \variance{R}$.
So Lemma~\ref{alt:var} applies to $R$ and lets us conclude that
\begin{equation}\label{vn:alt}
v_n = \frac{\sum_{i=1}^n x_i^2}{n} - \left(\frac{\sum_{i=1}^n x_i}{n}\right)^2.
\end{equation}
This leads to a simple procedure for computing the sample mean and
variance while reading the sequence $x_1,\dots,x_n$ from left to right.
Namely, maintain a sum of all numbers seen and also maintain a sum of the
squares of all numbers seen.  That is, we store two values, starting with
the values $x_1$ and $x_1^2$.  Then, as we get to the next number, $x_i$,
we add it to the first sum and add its square, $x_{i}^2$, to the second
sum.  After a single pass through the sequence $x_1,\dots,x_n$, we wind up
with the values of the two sums $\sum_{i=1}^n x_i$ and $\sum_{i=1}^n
x_i^2$.  Then we just plug these two values into~\eqref{vn:alt} to find
the sample variance.

\end{editingnotes}

\begin{editingnotes}

\subsection{Expectation Squared}

The alternate definition of variance given in Lemma~\ref{alt:var} has
a cute implication:
\begin{corollary}
If $R$ is a random variable, then $\expect{R^2} \geq \expectsq{R}$.
\end{corollary}
\begin{proof}
We first defined $\variance{R}$ as an average of a squared expression, so
$\variance{R}$ is nonnegative.  Then we proved that $\variance{R} =
\expect{R^2} - \expectsq{R}$.  This implies that $\expect{R^2} -
\expectsq{R}$ is nonnegative.  Therefore, $\expect{R^2} \geq
\expectsq{R}$.
\end{proof}

In words, the expectation of a square is at least the square of the
expectation. The two are equal exactly when the variance is zero:
\begin{displaymath}
\expect{R^2} = \expectsq{R} \text{  iff  } \expect{R^2} - \expectsq{R} = 0
\text{  iff  } \variance{R} = 0.
\end{displaymath}

\end{editingnotes}

\begin{editingnotes}

\subsubsection*{Zero Variance}

When does a random variable, $R$, have zero variance?\dots when the random
variable \emph{never} deviates from the mean!
\begin{lemma*}%\label{zvar}
The variance of a random variable, $R$, is zero if and only if $\pr{R =
\expect{R}} = 1$.
\end{lemma*}

So saying that $\variance{R}=0$ is almost the same as saying that $R$ is
constant.  Namely, it takes the constant value equal to its expectation on
all sample points with nonzero probability.  (It can take on any finite
values on sample points with zero probability without affecting the
variance.)

\begin{proof}
By the definition of variance,
\[
\variance{R} = 0\qiff \expect{\paren{R - \expect{R}}^2} = 0.
\]
The inner expression on the right, $(R - \expect{R})^2$, is always
nonnegative because of the square.  As a result, $\expect{(R -
\expect{R})^2} = 0$ if and only if $\pr{(R - \expect{R})^2 \neq 0}$ is
zero, which is the same as saying that $\pr{(R - \expect{R})^2 = 0}$ is
one.  That is,
\[
\variance{R} = 0 \QIFF \pr{(R - \expect{R})^2 = 0} = 1.
\]
But the $(R - \expect{R})^2 = 0$ and $R = \expect{R}$ are different
descriptions of the same event.  Therefore,
\[
\variance{R} = 0 \qiff \pr{R = \expect{R}} =1.
\]
\end{proof}

\end{editingnotes}

\subsection{Dealing with Constants}

It helps to know how to calculate the variance of $aR+b$:

\begin{theorem}\label{var.const}
Let $R$ be a random variable, and $a$ a constant. Then
\begin{equation}\label{a2R}
\variance{a R} = a^2 \variance{R}.
\end{equation}
\end{theorem}

\begin{proof}
Beginning with the definition of variance and repeatedly applying
linearity of expectation, we have:
\begin{align*}
\variance{aR}
    & \eqdef \expect{(aR-\expect{aR})^2}\\
    & = \expect{(aR)^2 -2aR\expect{aR} + \expectsq{aR}}\\
    & = \expect{(aR)^2} -\expect{2aR\expect{aR}} + \expectsq{aR}\\
    & = a^2\expect{R^2} -2\expect{aR}\expect{aR} + \expectsq{aR}\\
    & = a^2\expect{R^2} -a^2\expectsq{R}\\
    & = a^2\paren{\expect{R^2} - \expectsq{R}}\\
    & = a^2\variance{R} & \text{(by Lemma~\ref{alt:var})}
\end{align*}
\end{proof}

It's even simpler to prove that adding a constant does not change the
variance, as the reader can verify:
\begin{theorem}\label{var+const}
Let $R$ be a random variable, and $b$ a constant. Then
\begin{equation}\label{R+b}
\variance{R+b} = \variance{R}.
\end{equation}
\end{theorem}

\begin{solution}
\begin{proof}
\begin{align*}
\variance{R+b} & \eqdef \expect{((R+b) - \expect{R+b})^2}\\
               & =  \expect{((R+b) - (\expect{R}+ b))^2}\\
               & =  \expect{(R - \expect{R})^2}\\
               & = \variance{R}.
\end{align*}
\end{proof}

\end{solution}

Recalling that the \idx{standard deviation} is the square root of
variance, this implies that the standard deviation of $a R + b$ is simply
$\abs{a}$ times the standard deviation of $R$:
\begin{corollary}
\[
\sigma_{aR+b} = \abs{a}\sigma_{R}.
\]
\end{corollary}


\subsection{Variance of a Sum}

In general, the variance of a sum is not equal to the sum of the
variances, but variances do add for \emph{\idx{independent}} variables.
In fact, \index{mutual independence} \emph{mutual} independence is not
necessary: \index{pairwise independence} \emph{pairwise} independence will
do.  This is useful to know because there are some important situations
involving variables that are pairwise independent but not mutually
independent.

\begin{theorem}\label{indvar}
If $R_1$ and $R_2$ are independent random variables, then
\begin{equation}\label{vR+R}
\variance{R_1 + R_2} = \variance{R_1} + \variance{R_2}.
\end{equation}
\end{theorem}

\begin{proof}
We may assume that $\expect{R_i} = 0$ for $i=1,2$, since we could always
replace $R_i$ by $R_i-\expect{R_i}$ in equation~\eqref{vR+R}.  This
substitution preserves the independence of the variables, and by
Theorem~\ref{var+const}, does not change the variances.

Now by Lemma~\ref{alt:var}, $\variance{R_i} = \expect{R_i^2}$ and
$\variance{R_1+R_2} = \expect{(R_1+R_2)^2}$, so we need only prove
\begin{equation}\label{E2R+R}
\expect{(R_1+R_2)^2} = \expect{R_1^2} + \expect{R_2^2}.
\end{equation}
But~\eqref{E2R+R} follows from linearity of expectation and the fact that
\begin{equation}\label{rrind}
\expect{R_1R_2} = \expect{R_1}\expect{R_2}
\end{equation}
since $R_1$ and $R_2$ are independent:
\begin{align*}
\expect{(R_1+R_2)^2}
   & = \expect{R_1^2+2R_1R_2 +R_2^2}\\
   & = \expect{R_1^2}+2\expect{R_1R_2} +\expect{R_2^2}\\
   & = \expect{R_1^2}+2\expect{R_1}\expect{R_2} +\expect{R_2^2}
             & \text{(by~\eqref{rrind})}\\
   & = \expect{R_1^2}+2\cdot 0 \cdot 0 +\expect{R_2^2}\\
   & =  \expect{R_1^2} + \expect{R_2^2}
\end{align*}

\iffalse
We will transform the left side into the right side.  We begin by
applying the alternate definition of variance.
\[
\variance{R_1 + R_2} = \expect{(R_1 + R_2)^2} - \expectsq{R_1 + R_2}.
\]

We will work on the first term and then the second term separately.
For the first term, note\begin{eqnarray*}
\expect{(R_1+R_2)^2}
& = &   \expect{R_1^2 + 2 R_1 R_2 + R_2^2} \\
& = &   \expect{R_1^2} + \expect{2 R_1 R_2} + \expect{R_2^2} \\
& = &   \expect{R_1^2} + 2 \expect{R_1} \expect{R_2} + \expect{R_2^2}.
\end{eqnarray*}
First, we multiply out the squared expression.  The second step uses
linearity of expectation.  In the last step, we break the
expectation of the product $R_1 R_2$ into a product of expectations;
this is where we use the fact that $R_1$ and $R_2$ are independent.
Now we work on the second term.
\begin{eqnarray*}
\expectsq{R_1+R_2} & = & (\expect{R_1} + \expect{R_2})^2 \\
& = & \expectsq{R_1} + 2 \expect{R_1} \expect{R_2} + \expectsq{R_2}.
n\end{eqnarray*}
The first step uses linearity of expectation, and in the second step
we multiply out the squared expression.  Now we subtract the
(expanded) second term from the first. Cancelling and rearranging
terms, we find that
\begin{eqnarray*}
\variance{R_1 + R_2} & = &   (\expect{R_1^2} - \expectsq{R_1}) +
(\expect{R_2^2}) - \expectsq{R_2}) \\
& = &   \variance{R_1} + \variance{R_2}.
\end{eqnarray*}
\fi
\end{proof}

An independence condition is necessary.  If we ignored independence, then
we would conclude that $\variance{R + R} = \variance{R} + \variance{R}$.
However, by Theorem~\ref{var.const}, the left side is equal to $4
\variance{R}$, whereas the right side is $2 \variance{R}$.  This implies
that $\variance{R}=0$, which, by the Lemma above,
\iffalse Lemma~\ref{zvar}\fi
essentially only holds if $R$ is constant.

The proof of Theorem~\ref{indvar} carries over straightforwardly to
the sum of any finite number of variables.  So we have:

\begin{theorem}\label{th:varsum}[\idx{Pairwise Independent Additivity} of
  Variance]
  If $R_1, R_2, \dots, R_n$ are \index{pairwise independent}
  \emph{pairwise} independent random variables, then
\begin{equation}\label{vsum}
\variance{R_1 + R_2 + \cdots + R_n} = \variance{R_1} + \variance{R_2} +
  \cdots + \variance{R_n}.
\end{equation}
\end{theorem}
\begin{editingnotes}

\begin{proof}
  We may assume that $\expect{R_i} = 0$ for $i=1,\dots,n$, since we could
  always replace $R_i$ by $\paren{R_i-\expect{R_i}}$ in
  equation~\eqref{vsum}.  This substitution preserves the independence of
  the variables, and by Theorem~\ref{var+const}, does not change the
  variances.

  Now by Lemma~\ref{alt:var}, $\variance{R_i} = \expect{R_i^2}$ and
\[
\variance{R_1+R_2+\cdots+R_n} = \expect{(R_1+R_2+\cdots+R_n)^2},
\]
so we need only prove
\begin{equation}\label{E2R+R}
\expect{(R_1+R_2+\cdots+R_n)^2} = \expect{R_1^2} + \expect{R_2^2} + \cdots
+ \expect{R_n^2}
\end{equation}
But~\eqref{E2R+R} follows from linearity of expectation and the fact that
\begin{equation}\label{rrind}
\expect{R_iR_j} = \expect{R_i}\expect{R_j} = 0 \cdot 0 = 0
\end{equation}
for $i \neq j$, since $R_i$ and $R_j$ are independent.  Namely,
\begin{align*}
\expect{(R_1+R_2+\cdots+R_n)^2}
   & = \expect{\sum_{1\leq i,j \leq n} R_iR_j}\\
   & = \sum_{1\leq i,j \leq n} \expect{R_iR_j} & \text{linearity of $\expect{}$}\\
   & = \sum_{1 \leq i \leq n} \expect{R_i^2}
             + \sum_{1 \leq i \neq j \leq n} \expect{R_iR_j} &
             \text{(rearranging the sum)}\\
   & = \sum_{1 \leq i \leq n} \expect{R_i^2}
            + \sum_{1 \leq i \neq j \leq n} 0
             & \text{(by~\eqref{rrind})}\\
   & =  \expect{R_1^2} + \expect{R_2^2} + \cdots + \expect{R_n^2}.
\end{align*}


\iffalse
By linearity of expectation, we have
\begin{align}
\expect{\biggl(\sum_{i=1}^n R_i\biggr)^2} &
    = \expect{\sum_{i=1}^n \sum_{j=1}^n R_i R_j} \notag\\
   &  = \sum_{i=1}^n \sum_{j=1}^n \expect{R_i R_j} & \text{(linearity)}\notag\\
   & = \sum_{1\le i \neq j \le n} \expect{R_i}\expect{R_j} + \sum_{i=1}^n
     \expect{R_i^2}.
       & \text{(pairwise independence)} \label{ER2}
\end{align}
In~\eqref{ER2}, we use the fact that the expectation
of the product of two independent variables is the product of their
expectations.

Also,
\begin{align}
\expectsq{\sum_{i=1}^n R_i} & = \biggl(\expect{\sum_{i=1}^n R_i}\biggr)^2 \notag\\
  &  = \biggl(\sum_{i=1}^n \expect{R_i}\biggr)^2 &\text{(linearity)} \notag\\
  &  = \sum_{i=1}^n \sum_{j=1}^n \expect{R_i} \expect{R_j}\notag\\
  & = \sum_{1\le i \neq j \le n} \expect{R_i}\expect{R_j} + \sum_{i=1}^n
     \expectsq{R_i}.\label{E2R}
\end{align}
So,
\begin{align*}
\variance{\biggl(\sum_{i=1}^n R_i\biggr)}
   & =  \expect{\biggl(\sum_{i=1}^n R_i\biggr)^2} -
\expectsq{\sum_{i=1}^n R_i}  & \text{(Lemma~\ref{alt:var})}\\
   &  = \sum_{1\le i \neq j \le n} \expect{R_i} \expect{R_j}
        + \sum_{i=1}^n \expect{R_i^2} - \\
   & \quad \paren{\sum_{1\le i \neq j \le n} \expect{R_i}\expect{R_j}
        + \sum_{i=1}^n \expectsq{R_i}}
      & \text{(by~\eqref{ER2} and~\eqref{E2R})}\\
   & = \sum_{i=1}^n \expect{R_i^2} - \sum_{i=1}^n \expectsq{R_i}\\
   & = \sum_{i=1}^n (\expect{R_i^2} - \expectsq{R_i})
             & \text{(reordering the sums)}\\
   & = \sum_{i=1}^n \variance{R_i}. & \text{(Theorem~\ref{th:alt})}
\end{align*}
\fi
\end{proof}
\end{editingnotes}


Now we have a simple way of computing the variance of a variable, $J$,
that has an $(n,p)$-\idx{binomial distribution}.  We know that $J =
\sum_{k=1}^n I_k$ where the $I_k$ are mutually independent indicator
variables with $\pr{I_k=1}=p$.  The variance of each $I_k$ is $p(1-p)$
by Lemma~\ref{bernoulli-variance}, so by linearity of variance, we have
\begin{lemma*}[Variance of the Binomial Distribution]
If $J$ has the $(n,p)$-binomial distribution, then
\begin{equation}\label{p1p}
\variance{J} = n \variance{I_k} = np(1-p).
\end{equation}
\end{lemma*}

\begin{problems}
\practiceproblems
\pinput{TP_markov_chebyshev_for_card_games}

\classproblems
\pinput{CP_chebyshev_hat_check}
\pinput{CP_chebyshev_tight}
\pinput{CP_infinite_variance}

\homeworkproblems
\pinput{PS_Chebyshev_one_sided}
\pinput{PS_n_keys}
\end{problems}

%\subsection{Applying Chebyshev's Theorem}

\section{Estimation by Random Sampling}


\subsubsection{Polling again}
\begin{editingnotes}

\textcolor{blue}{This paragraph reflects an alternative exposition
  where polling estimation and confidence were based only on binomial
  distribution properties, even before expectation was introduced.}

In Chapter~<none>, we used bounds on the binomial distribution to determine
confidence levels for a poll of voter preferences of Franken vs.\ Coleman.
Now that we know the variance of the binomial distribution, we can use
Chebyshev's Theorem as an alternative approach to calculate poll size.

The setup is the same as in Chapter~<none>
\end{editingnotes}

Suppose we had wanted an advance estimate of the fraction of the
Massachusetts voters who favored Scott Brown over everyone else in the
recent Democratic primary election to fill Senator Edward Kennedy's seat.

\iffalse
\footnote{We can only keep our fingers crossed for this race to happen --
when they ran against each other for the U.S. Senate in 2000, they
generated some of the best entertainment in TV history.}  \fi

Let $p$ be this unknown fraction, and let's suppose we have some random
process ---say throwing darts at voter registration lists ---which will
select each voter with equal probability.  We can define a Bernoulli
variable, $K$, by the rule that $K=1$ if the random voter most prefers
Brown, and $K=0$ otherwise.

Now to estimate $p$, we take a large number, $n$, of random choices of
voters\footnote{We're choosing a random voter $n$ times \emph{with
    replacement}.  That is, we don't remove a chosen voter from the set of
  voters eligible to be chosen later; so we might choose the same voter
  more than once in $n$ tries!  We would get a slightly better estimate if
  we required $n$ \emph{different} people to be chosen, but doing so
  complicates both the selection process and its analysis, with little gain
  in accuracy.}  and count the fraction who favor Brown.  That is, we
define variables $K_1, K_2, \dots$, where $K_i$ is interpreted to be the
indicator variable for the event that the $i$th chosen voter prefers
Brown.  Since our choices are made independently, the $K_i$'s are
independent.  So formally, we model our estimation process by simply
assuming we have mutually independent Bernoulli variables $K_1, K_2,
\dots,$ each with the same probability, $p$, of being equal to 1.  Now let
$S_n$ be their sum, that is,
\begin{equation}\label{LN12:Sn}
S_n \eqdef \sum_{i=1}^n K_i.
\end{equation}
So $S_n$ has the binomial distribution with parameter $n$, which we can
choose, and unknown parameter $p$.

The variable $S_n/n$ describes the fraction of voters we will sample
who favor Scott Brown.  Most people intuitively expect this sample
fraction to give a useful approximation to the unknown fraction, $p$
---and they would be right.
\iffalse
Note that
\[
\expect{\frac{S_n}{n}} = \sum_{i=1}^n \expect{K_i} = pn.
\]
\fi
So we will use the sample value, $S_n/n$, as our \emph{statistical
  estimate} of $p$ and use the Pairwise Independent Sampling
Theorem~\ref{th:pairwise-sampling} to work out how good an estinate
this is.

\subsection{Sampling}
Suppose we want our estimate to be within $0.04$ of the Brown favoring
fraction, $p$, at least 95\% of the time.  This means we want
\begin{equation}\label{pollsizeinequality}
\pr{\abs{\frac{S_n}{n} - p} \leq 0.04} \geq 0.95\ .
\end{equation}
So we better determine the number, $n$, of times we must poll voters so
that inequality~\eqref{pollsizeinequality} will hold.

\begin{editingnotes}
the value, $S_n/n$, of our estimate will, with probability at least
$1 -\delta$, be within $\epsilon$ of the actual fraction in the nation
favoring Brown.

We let $\epsilon$ be the margin of error we can tolerate, and let $\delta$
be the probability that our result lies outside this margin, so in this
case we'd have $\epsilon = 0.04$ and $\delta \le 0.05$.

We want to determine the number, $n$, of times we must poll voters so that
the value, $S_n/n$, of our estimate will, with probability at least
$1 -\delta$, be within $\epsilon$ of the actual fraction in the nation
favoring Brown.
\end{editingnotes}

Now $S_n$ is binomially distributed, so from~\eqref{p1p} we have
\[
\variance{S_n}  = n(p(1-p)) \leq n \cdot \frac{1}{4} = \frac{n}{4}\label{n4}
\]
The bound of 1/4 follows from the fact that $p(1-p)$ is maximized when $p
= 1-p$, that is, when $p=1/2$ (check this yourself!).

Next, we bound the variance of $S_n/n$:
\begin{align}
\variance{\frac{S_n}{n}}
       & = \paren{\frac{1}{n}}^2 \variance{S_n}
                     & \text{(by~\eqref{a2R})}\notag\\
       & \leq \paren{\frac{1}{n}}^2 \frac{n}{4} & \text{(by~\eqref{n4})}\notag\\
       & = \frac{1}{4n}\label{1/4n}
\end{align}
Now from Chebyshev and~\eqref{1/4n} we have:
\begin{equation}\label{CK}
\pr{\abs{\frac{S_n}{n} - p} \geq 0.04}
    \leq \frac{\variance{S_n/n}}{(0.04)^2}
       = \frac{1}{4n(0.04)^2} = \frac{156.25}{n}
\end{equation}

To make our our estimate with  95\% confidence, we want the righthand
side of~\eqref{CK} to be at most 1/20.  So we choose $n$ so that
\[
\frac{156.25}{n} \leq \frac{1}{20},
\]
that is,
\[
n \geq 3,125.
\]

A more exact calculation of the tail of this binomial distribution
shows that the above sample size is about four times larger than
necessary, but it is still a feasible size to sample.  The fact that
the sample size derived using \idx{Chebyshev's Theorem} was unduly
pessimistic should not be surprising.  After all, in applying the
Chebyshev Theorem, we only used the variance of $S_n$.  It makes sense
that more detailed information about the distribution leads to better
bounds.  But working through this example using only the
\idx{variance} has the virtue of illustrating an approach to
estimation that is applicable to arbitrary random variables, not just
binomial variables.

\subsection{Matching Birthdays}\label{bday_deviation_subsec}

There are important cases where the relevant distributions are not
binomial because the mutual independence properties of the voter
preference example do not hold.  In these cases, estimation methods
based on the Chebyshev bound may be the best approach.  Birthday
Matching is an example.  We already saw in
Section~\ref{birthday_principle_sec} that in a class of 85 students it
is virtually certain that two or more students will have the same
birthday.  This suggests that quite a few pairs of students are likely
to have the same birthday.  How many?

So as before, suppose there are $n$ students and $d$ days in the year, and
let $D$ be the number of pairs of students with the same birthday.  Now it
will be easy to calculate the expected number of pairs of students with
matching birthdays.  Then we can take the same approach as we did in
estimating voter preferences to get an estimate of the probability of
getting a number of pairs close to the expected number.

Unlike the situation with voter preferences, having \idx{matching
  birthdays} for different pairs of students are not \idx{mutually
  independent} events, but the matchings are \emph{\idx{pairwise
    independent}}, as explained in
Section~\ref{birthday_principle_sec}.
%
\iffalse For example, knowing that Alice and Bob have matching
birthdays, and also that Ted and Alice have matching birthdays
obviously implies that Bob and Ted have matching birthdays.  On the
other hand, knowing that Alice and Bob have matching birthdays tells
us nothing about whether Alice and Carol have matching birthdays,
namely, these two events really are independent.  So even though the
events that various pairs of students have matching birthdays are not
mutually independent, indeed not even three-way independent, they are
\index{pairwise independent} \emph{pairwise} independent.  \fi
% This will allow us to apply the same reasoning to Birthday Matching
as we did for voter preference.  Namely, let $B_1,B_2,\dots,B_n$ be
the birthdays of $n$ independently chosen people, and let $E_{i,j}$ be
the indicator variable for the event that the $i$th and $j$th people
chosen have the same birthdays, that is, the event $[B_i = B_j]$.  So
our probability model, the $B_i$'s are mutually independent variables,
the $E_{i,j}$'s are pairwise independent.  Also, the expectations of
$E_{i,j}$ for $i \neq j$ equals the probability that $B_i = B_j$,
namely, $1/d$.

Now, $D$, the number of matching pairs of birthdays among the $n$
choices is simply the sum of the $E_{i,j}$'s:
\begin{equation}\label{Vn}
D \eqdef \sum_{1\le i < j \le n} E_{i,j}.
\end{equation}
So by linearity of expectation
\[
\expect{D} = \expect{\sum_{1\le i < j \le n} E_{i,j}} = 
               \sum_{1\le i < j \le n} \expect{E_{i,j}} =
               \binom{n}{2}\cdot \frac{1}{d}.
\]
Similarly,
\begin{align*}
\variance{D}
   & = \variance{\sum_{1\le i < j \le n} E_{i,j}}\\
   & = \sum_{1\le i < j \le n} \variance{E_{i,j}}
           & \text{(by Theorem~\ref{th:varsum})}\\
   & = \binom{n}{2} \cdot \frac{1}{d}\paren{1-\frac{1}{d}}.
           & \text{(by Lemma~\ref{bernoulli-variance})}
\end{align*} 

In particular, for a class of $n= 85$ students with $d=365$ possible
birthdays, we have $\expect{D} \approx 9.7$ and $\variance{D} < 9.7 (1-
1/365) < 9.7$.  So by Chebyshev's Theorem
\[
\pr{\abs{D - 9.7} \geq x} < \frac{9.7}{x^2}.
\]

Letting $x=5$, we conclude that there is a better than 50\% chance that in
a class of 85 students, the number of pairs of students with the same
birthday will be between 5 and 14.

%In fact, there turned out to be
%\emph{exactly} the 16 matches expected in the class this term!


\hyperdef{pairwise}{independent}{\subsection{Pairwise Independent Sampling}}

The reasoning we used above to analyze voter polling and matching
birthdays is very similar.  We summarize it in slightly more general form
with a basic result we call the \idx{Pairwise Independent Sampling}
Theorem.  In particular, we do not need to restrict ourselves to sums of
zero-one valued variables, or to variables with the same distribution.
For simplicity, we state the Theorem for pairwise independent variables
with possibly different distributions but with the same mean and variance.

\begin{theorem}[Pairwise Independent Sampling]\label{th:pairwise-sampling}
Let $G_1, \dots, G_n$ be pairwise independent variables with the same
mean, $\mu$, and deviation, $\sigma$.  Define
\begin{equation}\label{ln14.Sn}
S_n \eqdef \sum_{i=1}^n G_i.
\end{equation}
Then
\[
\pr{\abs{\frac{S_n}{n} - \mu} \geq x}
    \leq \frac{1}{n} \paren{\frac{\sigma}{x}}^2.
\]
\end{theorem}

\begin{proof}
We observe first that the expectation of $S_n/n$ is $\mu$:
\begin{align*}
\expect{\frac{S_n}{n}} & = \expect{\frac{\sum_{i=1}^n G_i}{n}}
         & \text{(def of $S_n$)}\\
 & = \frac{\sum_{i=1}^n \expect{G_i}}{n} 
     & \text{(linearity of expectation)}\\
 & = \frac{\sum_{i=1}^n \mu}{n}\\
 & = \frac{n\mu}{n} = \mu.
\end{align*}

The second important property of $S_n/n$ is that its variance is the
variance of $G_i$ divided by $n$:
\begin{align}
\variance{\frac{S_n}{n}} & =  \paren{\frac{1}{n}}^2 \variance{S_n}
          & \mbox{(by~\eqref{a2R})}\notag\\
 & =  \frac{1}{n^2} \variance{\sum_{i=1}^n G_i} 
          & \text{(def of $S_n$)}\notag\\
 & =  \frac{1}{n^2} \sum_{i=1}^n \variance{G_i}
        & \text{(pairwise independent additivity)}\notag\\
 & =  \frac{1}{n^2}\cdot n\sigma^2 =  \frac{\sigma^2}{n}.\label{Snu}
\end{align}

This is enough to apply \idx{Chebyshev's Theorem} and conclude:
\begin{align*}
\pr{\abs{\frac{S_n}{n} - \mu} \geq x} & \leq \frac{\variance{S_n/n}}{x^2}.
       & \text{(Chebyshev's bound)}\\
    & = \frac{\sigma^2/n}{x^2} & \text{(by~\eqref{Snu})}\\
    & = \frac{1}{n} \paren{\frac{\sigma}{x}}^2.
\end{align*}

\end{proof}

The Pairwise Independent Sampling Theorem provides a precise general
statement about how the average of independent samples of a random
variable approaches the mean.  In particular, it proves what is known as
the \idx{Law of Large Numbers}\footnote{This is the \index{Weak Law of
    Large Numbers} \emph{Weak} Law of Large Numbers.  As you might
  suppose, there is also a Strong Law, but it's outside the scope of
  6.042.} : by choosing a large enough sample size, we can get arbitrarily
accurate estimates of the mean with confidence arbitrarily close to 100\%.

\begin{corollary}\label{weaklaw}[Weak Law of Large Numbers]
  Let $G_1, \dots, G_n$ be pairwise independent variables with the same
  mean, $\mu$, and the same finite deviation, and let
\[
S_n \eqdef \frac{\sum_{i=1}^n G_i}{n}.
\]
Then for every $\epsilon > 0$,
\[
\lim_{n \rightarrow \infty}
        \pr{\abs{S_n - \mu}  \leq \epsilon} = 1.
\]
\end{corollary}

\section{Confidence versus Probability}

%\hyperdef{sam}{pling}{\section{Polling}}

So Chebyshev's Bound implies that sampling 3,125 voters will yield a
fraction that, 95\% of the time, is within 0.04 of the actual fraction
of the voting population who prefer Brown.  \begin{editingnotes}
  Estimates of the binomial distribution show that a sample size
  around 664 would do.
\end{editingnotes}

Notice that the actual size of the voting population was never
considered because \emph{it did not matter}.  People who have not
studied probability theory often insist that the \idx{population size}
should matter.  But our analysis shows that polling a little over 3000
people people is always sufficient, whether there are ten thousand, or
million, or billion \dots voters.  You should think about an intuitive
explanation that might persuade someone who thinks population size
matters.

Now suppose a pollster actually takes a sample of 3,125 random voters to
estimate the fraction of voters who prefer Brown, and the pollster finds
that 1250 of them prefer Brown.  It's tempting, \textbf{but sloppy}, to
say that this means:
\begin{falseclm*}
  With probability 0.95, the fraction, $p$, of voters who prefer Brown
  is $1250/3125 \pm 0.04$.  Since $1250/3125 -0.04 > 1/3$, there is a 95\%
  chance that more than a third of the voters prefer Brown to all other
  candidates.
\end{falseclm*}
What's objectionable about this statement is that it talks about the
probability or ``chance'' that a real world fact is true, namely that the
actual fraction, $p$, of voters favoring Brown is more than 1/3.  But $p$
is what it is, and it simply makes no sense to talk about the probability
that it is something else.  For example, suppose $p$ is actually 0.3;
then it's nonsense to ask about the probability that it is within 0.04 of
1250/3125 ---it simply isn't.

This example of voter preference is typical: we want to estimate a fixed,
unknown real-world quantity.  But \emph{being unknown does not make this
  quantity a random variable}, so it makes no sense to talk about the
probability that it has some property.

A more careful summary of what we have accomplished goes this way:
\begin{quote}
We have described a probabilistic procedure for estimating the value of
the actual fraction, $p$.  The probability that \emph{our estimation
procedure} will yield a value within 0.04 of $p$ is 0.95.
\end{quote}
This is a bit of a mouthful, so special phrasing closer to the sloppy
language is commonly used.  The pollster would describe his conclusion by
saying that
\begin{quote}
At the 95\% \term{confidence level}, the fraction of voters
who prefer Brown is $1250/3125 \pm 0.04$.
\end{quote}

So confidence levels refer to the results of estimation procedures for
real-world quantities.  The phrase ``confidence level'' should be heard as
a reminder that some statistical procedure was used to obtain an estimate,
and in judging the credibility of the estimate, it may be important to
learn just what this procedure was.

\begin{editingnotes}
Maybe include example from CP\_drug\_confidence here.
\end{editingnotes}

\begin{problems}
\practiceproblems
\pinput{FP_random_sampling}

\classproblems
\pinput{CP_gallup_poll}
\pinput{CP_birthday-deviation}
\pinput{CP_size_of_sample_vs_population}
\pinput{CP_pairwise_independent_theorem}
\pinput{CP_drug_confidence}

\examproblems
\pinput{FP_sampling_concepts}
\end{problems}


%% The Chernoff Bound %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{editingnotes}
\textcolor{blue}{to be inserted}

\section*{Extreme Deviations}

\subsection*{The Chernoff Bound}

\end{editingnotes}

%% Conclusion %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\TBA{Conclusion...}

\endinput
