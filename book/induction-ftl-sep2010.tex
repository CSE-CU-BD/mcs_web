\chapter{Induction}\label{induction_chap}

\section{Structural Induction}

Up to now, we have focussed on induction over the natural numbers.
But the idea of induction is far more general---it can be applied to a
much richer class of sets.  In particular, it is especially useful in
connection with sets or data types that are defined recursively.

\subsection{Recursive Data Types}

\term{Recursive data types} play a central role in programming.  They
are specified by \term{recursive definitions}
that say how to build something from its parts.  \emph{Recursive}
definitions have two parts:
\begin{itemize}
\item \textbf{Base case(s)} that don't depend on anything else.
\item \textbf{Constructor case(s)} that depend on previous cases.
\end{itemize}
Let's see how this works in a couple of examples: Strings of brackets
and expression evaluation.

\subsubsection{Example 1: Strings of Brackets}

Let $\brkts$ be the set of all sequences (or strings) of square
brackets.  For example,
the following two strings are in $\brkts$:
\begin{equation}\label{2strings}
\lefbrk\rhtbrk\rhtbrk\lefbrk\lefbrk\lefbrk\lefbrk\lefbrk\rhtbrk\rhtbrk\quad \text{and}\quad \lefbrk\lefbrk\lefbrk\rhtbrk\rhtbrk\lefbrk\rhtbrk\rhtbrk\lefbrk\rhtbrk
\end{equation}

\begin{definition}\label{prns-def}
The set $\brkts$ of strings of brackets can be defined recursively as
follows:
\begin{itemize}

\item \textbf{Base case:} The \term{empty string}, $\emptystring$, is in
  $\brkts$.

\item \textbf{Constructor case:} If $s \in \brkts$, then
$s\rhtbrk$ and $s\lefbrk$ are in $\brkts$.

\end{itemize}

\end{definition}

Here, we're writing $s\rhtbrk$ to indicate the string that is the sequence
of brackets (if any) in the string $s$, followed by a right bracket;
similarly for $s\lefbrk$.

A string $s \in \brkts$ is called a \term{matched string} if its
brackets can be ``matched up'' in the usual way.  For example, the left hand
string in~\ref{2strings} is not matched because its second right
bracket does not 
have a matching left bracket.  The string on the right is matched.
The set of matched strings can be defined recursively as follows.

\begin{definition}\label{RM-def}
Recursively define the set, $\RM$, of strings as follows:
\begin{itemize}

\item \textbf{Base case:} $\emptystring \in\RM$.

\item \textbf{Constructor case:} If $s,t \in\RM$, then
\[
\lefbrk s\, \rhtbrk t \in\RM.
\]

\end{itemize}

\end{definition}

Here we're writing $\lefbrk s\, \rhtbrk t$ to indicate the string that
starts with a left bracket, followed by the sequence of brackets
(if any) in the string $s$, followed by a right bracket, and ending
with the sequence of brackets in the string $t$.

Using this definition, we can see that $\emptystring \in\RM$ by the Base
case, so
\[
\lefbrk\emptystring\rhtbrk\emptystring=\lefbrk\rhtbrk\in\RM
\]
by the Constructor case.  So now,
\begin{align*}
\lefbrk\emptystring\rhtbrk\lefbrk\rhtbrk &= \lefbrk\rhtbrk\lefbrk\rhtbrk \in \RM
    & \text{(letting $s = \emptystring, t = \lefbrk\rhtbrk$)}\\
\lefbrk\lefbrk\rhtbrk\rhtbrk\emptystring & = \lefbrk\lefbrk\rhtbrk\rhtbrk \in \RM
    & \text{(letting $s = \lefbrk\rhtbrk, t = \emptystring$)}\\
&\ \ \lefbrk\lefbrk\rhtbrk\rhtbrk\lefbrk\rhtbrk \in \RM
    & \text{(letting $s = \lefbrk\rhtbrk, t = \lefbrk\rhtbrk$)}
\end{align*}
are also strings in $\RM$ by repeated applications of the Constructor
case.

In general, $\RM$ will contain precisely the strings with matching
brackets. This is because the constructor case is, in effect,
identifying the bracket that matches the leftmost bracket in any
string.  Since that matching bracket is unique, this method of
constructing $\RM$ gives a unique way of constructing any string with
matched brackets.  This will turn out to be important later when we
talk about ambiguity.

Strings with matched brackets arise in the area of expression
parsing.  A brief history of the advances in this field is provided in
the box on the next page.

\floatingtextbox{
\textboxheader{Expression Parsing}

%Flesh this out and get references

During the early development of computer science in the 1950's and 60's,
creation of effective programming language compilers was a central
concern.  A key aspect in processing a program for compilation was
expression parsing.  The problem was to take in an expression like
\[
x + y * z^2 \div y + 7
\]
and \emph{put in} the brackets that determined how it
should be evaluated---should it be
\begin{align*}
[[x + y] * z^2 \div y] + 7,\text{ or}, \\
x + [y * z^2 \div [y + 7]], \text{ or},\\
[x + [y * z^2 ]] \div [y + 7],
\end{align*}
or \dots?

The Turing award (the ``Nobel Prize'' of computer science) was ultimately
bestowed on Robert Floyd, for, among other things, being discoverer of a
simple program that would insert the brackets properly.

In the 70's and 80's, this parsing technology was packaged into high-level
compiler-compilers that automatically generated parsers from expression
grammars.  This automation of parsing was so effective that the subject
stopped demanding attention and largely disappeared from the
computer science curriculum by the 1990's.}

\subsubsection{Example 2: Arithmetic Expressions}

Expression evaluation is a key feature of programming languages, and
recognition of expressions as a recursive data type is a key to
understanding how they can be processed.

To illustrate this approach we'll work with a toy example: arithmetic
expressions like $3x^2 + 2x + 1$ involving only one variable, ``$x$.''
We'll refer to the data type of such expressions as $\aexp$.  Here is its
definition:

\begin{definition}
The set $\aexp$ is defined recursively as follows:
\begin{itemize}
\item \textbf{Base cases:}

\begin{enumerate}

\item The variable, $x$, is in $\aexp$.

\item The arabic numeral, $\mtt{k}$, for any nonnegative integer, $k$, is
  in $\aexp$.

\end{enumerate}

\item \textbf{Constructor cases:} If $e,f \in \aexp$, then
\begin{enumerate}
\setcounter{enumi}{2}

\item $(e \sumsym f) \in \aexp$.  The expression $(e \sumsym f)$ is called a
  \emph{sum}.  The \aexp's $e$ and $f$ are called the \emph{components} of
  the sum; they're also called the \emph{summands}.

\item $(e \prodsym f) \in \aexp$.  The expression $(e \prodsym f)$ is called a
  \emph{product}.  The \aexp's $e$ and $f$ are called the
  \emph{components} of the product; they're also called the
  \emph{multiplier} and \emph{multiplicand}.

\item $\minussym(e) \in \aexp$.  The expression $\minussym(e)$ is called a
  \emph{negative}.
\end{enumerate}
\end{itemize}
\end{definition}

Notice that \aexp's are fully parenthesized, and exponents aren't allowed.
So the $\aexp$ version of the polynomial expression $3x^2 + 2x + 1$ would
officially be written as
\begin{equation}\label{fullparens}
((\mtt{3} \prodsym (x \prodsym x)) \sumsym ((\mtt{2} \prodsym x) \sumsym \mtt{1})).
\end{equation}
These parentheses and $\ast$'s clutter up examples, so we'll often use
simpler expressions like ``$3x^2 + 2x + 1$'' instead
of~\eqref{fullparens}.  But it's important to recognize that $3x^2 +
2x + 1$ is not an \aexp; it's an \emph{abbreviation} for an $\aexp$.

%% Structural Induction on Recursive Data Types %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Structural Induction on Recursive Data Types}

Structural induction is a method for proving that
some property, $P$, holds for all the elements of a recursively-defined data
type.  The proof consists of two steps:
\begin{itemize}
\item Prove $P$ for the base cases of the definition. 
\item Prove $P$ for the constructor cases of the definition, assuming that it
  is true for the component data items.  
\end{itemize}

A very simple application of structural induction proves that
(recursively-defined) matched strings always have an equal number of left
and right brackets.  To do this, define a predicate, $P$, on strings $s
\in \brkts$:
\[
P(s) \eqdef\ \ s \text{ has an equal number of left and right brackets}.
\]
\begin{theorem}\label{thm:RM-is-balanced}
$P(s)$ holds for all $s \in\RM$.
\end{theorem}

\begin{proof}

By structural induction on the definition that $s \in\RM$, using
$P(s)$ as the induction hypothesis.

\textbf{Base case:} $P(\emptystring)$ holds because the empty string has zero
left and zero right brackets.

\textbf{Constructor case:} For $r = \lefbrk s\,\rhtbrk t$, we must show
that $P(r)$ holds, given that $P(s)$ and $P(t)$ holds.  So let $n_s$,
$n_t$ be, respectively, the number of left brackets in $s$ and $t$.  So
the number of left brackets in $r$ is $1+n_s+n_t$.

Now from the respective hypotheses $P(s)$ and $P(t)$, we know that the
number of right brackets in $s$ is $n_s$, and likewise, the number of
right brackets in $t$ is $n_t$.  So the number of right brackets in
$r$ is $1+n_s+n_t$, which is the same as the number of left brackets.
This proves $P(r)$.  We conclude by structural induction that $P(s)$ holds
for all $s \in\RM$.
\end{proof}

\subsection{Functions on Recursively-defined Data Types}

\subsubsection{A Quick Review of Functions}\label{funcsubsec}

A \term{function} assigns an element of one set, called the
\term{domain}, to elements of another set, called the \term{codomain}.
The notation
\[
f: A \to B
\]
indicates that $f$ is a function with domain, $A$, and codomain, $B$.  The
familiar notation ``$f(a) = b$'' indicates that $f$ assigns the element $b
\in B$ to $a$.  Here $b$ would be called the \term*{value} of $f$ at
\term*{argument} $a$.

Functions are often defined by formulas as in:
\[
f_1(x) \eqdef \frac{1}{x^2}
\]
where $x$ is a real-valued variable, or
\[
f_2(y,z) \eqdef y\mathtt{10}yz
\]
where $y$ and $z$ range over binary strings, or
\[
f_3(x, n) \eqdef \text{ the pair } (n, x)
\]
where $n$ ranges over the nonnegative integers.

A function with a finite domain could be specified by a table that shows
the value of the function at each element of the domain.  For example, a function
$f_4(P,Q)$ where $P$ and $Q$ are propositional variables is specified by:
\[\begin{array}{|cc|c|}
\hline
P & Q & f_4(P,Q)\\
\hline \true & \true & \true\\
\hline \true & \false & \false\\
\hline \false & \true & \true\\
\hline \false & \false & \true\\
\hline
\end{array}\]
Notice that $f_4$ could also have been described by a formula:
\[
f_4(P,Q)  \eqdef [P \QIMPLIES Q].
\]

A function might also be defined by a procedure for computing its value at
any element of its domain, or by some other kind of specification.  For
example, define $f_5(y)$ to be the length of a left to right search of the
bits in the binary string $y$ until a \texttt{1} appears, so
\begin{align*}
f_5(0010) & = 3,\\
f_5(100)  & = 1,\\
f_5(0000) & \text{ is undefined}.
\end{align*}

Notice that $f_5$ does not assign a value to a string of just \texttt{0}'s.
This illustrates an important fact about functions: they need not assign a
value to every element in the domain.  In fact this came up in our first
example $f_1(x)=1/x^2$, which does not assign a value to $0$.  So in
general, functions may be \term{partial functions}, meaning that there may be domain
elements for which the function is not defined.  If a function is defined
on every element of its domain, it is called a \term{total function}.

It's often useful to find the set of values a function takes when applied
to the elements in \emph{a set} of arguments.  So if $f:A \to B$, and $S$
is a subset of $A$, we define $f(S)$ to be the set of all the values that
$f$ takes when it is applied to elements of $S$.  That is,
\[
f(S) \eqdef \set{b \in B \suchthat f(s) = b \text{ for some } s
  \in S}.
\]
For example, if we let $[r,s]$ denote the interval from $r$ to $s$ on the
real line, then $f_1([1,2]) = [1/4,1]$.

For another example, let's take the ``search for a \texttt{1}''
function, $f_5$.  If we let $X$ be the set of binary words which
start with an even number of \texttt{0}'s followed by a
\texttt{1}, then $f_5(X)$ would be the odd nonnegative integers.

Applying $f$ to a set, $S$, of arguments is referred to as
``applying $f$ \idx{pointwise} to $S$'', and the
set $f(S)$ is referred to as the \term{image} of $S$ under
$f$.\footnote{There is a picky distinction between the function $f$ which
  applies to elements of $A$ and the function which applies $f$ pointwise
  to subsets of $A$, because the domain of $f$ is $A$, while the domain of
  pointwise-$f$ is $\power(A)$.  It is usually clear from context whether
  $f$ or pointwise-$f$ is meant, so there is no harm in overloading the
  symbol $f$ in this way.}  The set of values that arise from applying $f$
to all possible arguments is called the \term{range} of $f$.  That is,
\[
\range{f} \eqdef f(\domain{f}).
\]

\subsubsection{Recursively-Defined Functions}

Functions on recursively-defined data types can be defined recursively
using the same cases as the data type definition.  Namely, to define a
function, $f$, on a recursive data type, define the value of $f$ for the
base cases of the data type definition, and then define the value of $f$
in each constructor case in terms of the values of $f$ on the component
data items.

For example, consider the function
\begin{equation*}
    \meval: \aexp \cross \integers \to \integers,
\end{equation*}
which evaluates any expression in~$\aexp$ using the value~$n$
for~$x$.  It is useful to express this function with a recursive
definition as follows:
\begin{definition}\label{meval-def}
  The \term{evaluation function}, $\meval: \aexp \times \integers \to
  \integers$, is defined recursively on expressions, $e \in \aexp$, as
  follows.  Let $n$ be any integer.

\begin{itemize}
\item \textbf{Base cases:}

\begin{enumerate}

\item\label{eval-var} Case[$e$ is $x$]
\[
\meval(x, n) \eqdef n.
\]
(The value of the variable, $x$, is given to be $n$.)

\item\label{eval-const} Case[$e$ is $k$]
\[
\meval(\mtt{k}, n) \eqdef k.
\]
(The value of the numeral $\mtt{k}$ is the integer $k$, no matter what
value $x$ has.)

\end{enumerate}

\item \textbf{Constructor cases:}

\begin{enumerate}
\setcounter{enumi}{2}

\item\label{eval-sum} Case[$e$ is $(e_1 \sumsym e_2)$]
\[
\meval((e_1 \sumsym e_2),n) \eqdef
  \meval(e_1,n)+\meval(e_2,n).
\]

\item\label{eval-prod} Case[$e$ is $(e_1 \prodsym e_2)$]
\[
\meval((e_1 \prodsym e_2),n) \eqdef \meval(e_1,n) \cdot \meval(e_2,n).
\]

\item\label{eval-minus} Case[$e$ is $\minussym(e_1)$]
\[
\meval(\minussym(e_1),n) \eqdef - \meval(e_1,n).
\]
\end{enumerate}

\end{itemize}

\end{definition}

For example, here's how the recursive definition of $\meval$ would arrive at
the value of $3+x^2$ when $x$ is 2:
\begin{align*}
\meval((\mtt{3} \sumsym (x \prodsym x)),2)
 & = \meval(\mtt{3},2) + \meval((x \prodsym x),2)
                  & \text{(by~Def~\ref{meval-def}.\ref{eval-sum})}\\
 & = 3 + \meval((x \prodsym x),2) & \text{(by~Def~\ref{meval-def}.\ref{eval-const})}\\
 & = 3 + (\meval(x,2) \cdot \meval(x,2)) & \text{(by~Def~\ref{meval-def}.\ref{eval-prod})}\\
 & = 3 + (2 \cdot 2) & \text{(by~Def~\ref{meval-def}.\ref{eval-var})}\\
 & = 3 + 4 = 7.
\end{align*}

\subsubsection{A Second Example}

We next consider the function on matched strings that specifies the
depth of the matched brackets in any string.  This function can be
specified recursively as follows:
\begin{definition}
The \emph{depth} $d(s)$ of a string $s \in\RM$ is defined
recursively by the rules:
\begin{itemize}
\item $d(\emptystring) \eqdef 0.$
\item $d(\lefbrk s\,\rhtbrk t)
    \eqdef \max \set{d(s) + 1, d(t)}$
\end{itemize}
\end{definition}

\subsubsection{Ambiguity}

When a recursive definition of a data type
allows the same element to be constructed in more than one way, the
definition is said to be \emph{ambiguous}.  A function defined recursively
from an ambiguous definition of a data type will not be well-defined
unless the values specified for the different ways of constructing the
element agree.

We were careful to choose an \emph{un}ambiguous definition of $\RM$ to
ensure that functions defined recursively on the definition would always
be well-defined.  As an example of the trouble an ambiguous definition can
cause, let's consider another definition of the matched strings.

\begin{definition}\label{M}
  Define the set, $M \subseteq \brkts$ recursively as follows:
\begin{itemize}

\item \textbf{Base case:} $\emptystring \in M$,

\item \textbf{Constructor cases:} if $s,t \in M$, then
  the strings $\lefbrk s\, \rhtbrk$ and $st$ are also in $M$.
\end{itemize}
\end{definition}

By using structural induction, it is possible to prove that $M =
\RM$.  Indeed, the definition of~$M$ might even seem like a more
natural way to define the set of matched strings than the definition
of~$\RM$.  But
the definition of $M$ is ambiguous, while the
(perhaps less natural) definition of $\RM$ is unambiguous.  Does this
ambiguity matter?
Yes, it can.  For example, suppose we defined
\begin{align*}
  f(\emptystring)        & \eqdef 1,\\
  f(\ \lefbrk s\,\rhtbrk\ ) & \eqdef 1+ f(s),\\
  f(st)                  & \eqdef (f(s)+1) \cdot (f(t)+1)
                            & \text{for } st\neq \emptystring.
\end{align*}

Let $a$ be the string $\lefbrk\lefbrk\rhtbrk\rhtbrk \in M$ built by two successive
applications of the first $M$ constructor starting with $\emptystring$.  Next
let 
\begin{align*}
    b & ::= aa \\
      &   = \lefbrk\lefbrk\rhtbrk\rhtbrk\lefbrk\lefbrk\rhtbrk\rhtbrk
\end{align*}
and
\begin{align*}
    c & ::= bb \\
      &   = \lefbrk\lefbrk\rhtbrk\rhtbrk\lefbrk\lefbrk\rhtbrk\rhtbrk\lefbrk\lefbrk\rhtbrk\rhtbrk\lefbrk\lefbrk\rhtbrk\rhtbrk
\end{align*}
each be built by successive applications
of the second $M$ constructor starting with $a$.

Alternatively, we can build $ba$ from the second constructor with $s=b$
and $t=a$, and then get to $c$ using the second constructor with $s=ba$
and $t=a$.

By applying these rules to the first way of constructing~$c$,
$f(a) = 2$, \ $f(b) = (2+1)(2+1)=9$, and $f(c) = f(bb)=
(9+1)(9+1)=100$.  Using the second way of constructing~$c$, we find
that $f(ba) = (9+1)(2+1) = 27$ and $f(c) = f(ba\,a) = (27 +1)
(2+1) = 84$.
The outcome is that $f(c)$ is defined to be both 100 and 84, which shows
that the rules defining $f$ are inconsistent.

Note that structural induction remains a sound proof method even
for ambiguous recursive definitions, which is why it is easy to prove
that $M=\RM$.

\subsection{Recursive Functions on~$\naturals$---Structural Induction
  versus Ordinary Induction}

The nonnegative integers can be understood as a recursive data type.
\begin{definition}\label{0succ}
The set, $\naturals$, is a data type defined recursivly as:
\begin{itemize}

\item
\textbf{Base Case:} $0 \in \naturals$.

\item
\textbf{Constructor Case:} If $n \in \naturals$, then the
\emph{successor}, $n+1$, of $n$ is in $\naturals$.

\end{itemize}

\end{definition}

This means that ordinary induction is a special case of structural
induction on the recursive Definition~\ref{0succ}. Conversely, most
proofs based on structural induction that you will encounter in
computer science can also be reformatted into proofs that use only
ordinary induction.  The decision as to which technique to use is up
to you, but it will often be the case that structural induction
provides the easiest approach when you are dealing with recursive data
structures or functions.

Definition~\ref{0succ} also justifies 
the familiar recursive definitions of functions on the nonnegative
integers.  Here are some examples.

\subsubsection{The Factorial Function}

The factorial function is often written ``$n!$.''  You will be seeing
it a lot in Parts~III and~IV of this text. For now, we'll use the
notation $\fac(n)$ and define it recursively as follows:
\begin{itemize}

\item\textbf{Base Case:} $\text{fac}(0) \eqdef 1$.

\item\textbf{Constructor Case:}
$\text{fac}(n+1) \eqdef (n+1)\cdot \text{fac}(n)$ for $n \ge 0$.

\end{itemize}

\subsubsection{The Fibonacci numbers.}

Fibonacci numbers arose out of an effort 800 years ago to model
population growth.  We will study them at some length in Part~III\@.
The $n$th Fibonacci number, $\fib(n)$, can be defined recursively
by:
\begin{itemize}

\item\textbf{Base Cases:}
$\fib(0) ::= 0$ and $\fib(1) ::= 1$

\item\textbf{Constructor Case:}
$\fib(n) \eqdef \fib(n-1) + \fib(n-2)$ for $n \geq 2$.
\end{itemize}
Here the recursive step starts at $n=2$ with base cases for $n = 0$
and $n = 1$.  This
is needed since the recursion relies on two previous values.

What is $\fib(4)$?  Well, $\fib(2) =
\fib(1)+\fib(0) = 1$, $\fib(3) =
\fib(2)+\fib(1) = 2$, so $\fib(4) = 3$.  The sequence
starts out $0, 1, 1, 2, 3, 5, 8, 13, 21,\dots$.

\subsubsection{Sum-notation}

Let ``$S(n)$'' abbreviate the expression ``$\sum_{i=1}^n f(i)$.''  We
can recursively define $S(n)$ with the rules
\begin{itemize}
\item\textbf{Base Case:} $S(0) \eqdef 0$.
\item\textbf{Constructor Case:} $S(n+1) \eqdef  f(n+1) + S(n)$ for $n\geq 0$.
\end{itemize}

\subsubsection{Ill-formed Function Definitions}

There are some blunders to watch out for when defining functions
recursively.  Below are some function specifications that resemble
good definitions of functions on the nonnegative integers, but they
aren't.
\begin{definition}
\begin{equation}\label{f1}
f_1(n)\eqdef 2+f_1(n-1).
\end{equation}
\end{definition}
This ``definition'' has no base case.  If some function, $f_1$,
satisfied~(\ref{f1}), so would a function obtained by adding a constant to
the value of $f_1$.  So equation~(\ref{f1}) does not uniquely define
an $f_1$.

\begin{definition}
\begin{equation}\label{f2}
f_2(n) \eqdef \begin{cases}
                0,       & \text{if $n=0$},\\
                f_2(n+1) &  \text{otherwise}.
              \end{cases}
\end{equation}
\end{definition}
This ``definition'' has a base case, but still doesn't uniquely determine
$f_2$.  Any function that is 0 at 0 and constant everywhere else would
satisfy the specification, so~\eqref{f2} also does not uniquely define
anything.

In a typical programming language, evaluation of $f_2(1)$ would begin with
a recursive call of $f_2(2)$, which would lead to a recursive call of
$f_2(3)$, \dots with recursive calls continuing without end.  This
``operational'' approach interprets~\eqref{f2} as defining a
\emph{partial} function, $f_2$, that is undefined everywhere but 0.

\begin{definition}
\begin{equation}\label{f3}
f_3(n) \eqdef \begin{cases}
  0, &  \text{if $n$ is divisible by 2,}\\
  1, &  \text{if $n$ is divisible by 3,}\\
  2, & \text{otherwise.}
 \end{cases}
\end{equation}
\end{definition}
This ``definition'' is inconsistent: it requires $f_3(6) = 0$ and $f_3(6)
=1$, so~(\ref{f3}) doesn't define anything.

\subsubsection{A Mysterious Function}
Mathematicians have been wondering about the following function
specification for many years:
\begin{equation}\label{f5}
f_4(n) \eqdef\begin{cases}
 1, & \text{if $n\le 1$},\\
 f_4(n/2) &  \text{if $n>1$ is even},\\
 f_4(3n+1)& \text{if $n>1$ is odd}.
\end{cases}
\end{equation}
For example, $f_4(3)=1$ because
\[
f_4(3)\eqdef f_4(10)\eqdef f_4(5)\eqdef f_4(16)\eqdef f_4(8)\eqdef
f_4(4)\eqdef f_4(2)\eqdef f_4(1)\eqdef 1.
\]
The constant function equal to 1 will satisfy~\eqref{f5}, but it's not
known if another function does too.  The problem is that the third case
specifies $f_4(n)$ in terms of $f_4$ at arguments larger than $n$, and so
cannot be justified by induction on $\naturals$.  It's known that any
$f_4$ satisfying~\eqref{f5} equals 1 for all $n$ up to over a billion.

\problemsection

\begin{problems}
\practiceproblems
%% Strong Induction Problems %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\pinput{CP_3_and_5_cent_stamps_by_induction}

\classproblems

%% Ordinary Induction Problems %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\pinput{CP_cubic_series}
\pinput{CP_geometric_series_induction}
\pinput{CP_sum_of_inverse_squares_induction}
\pinput{CP_courtyard_tiling_corner}
\pinput{CP_flawed_induction_proof}
\pinput{CP_false_arithmetic_series_proof}
\pinput{CP_box_unstacking} %not strong induction, but depends on stacking game

\homeworkproblems
\pinput{PS_sums_and_products_of_integers}
\pinput{PS_ripple_carry_adder_correctness}
\pinput{PS_periphery_length_game}

%% Strong Induction Problems %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\pinput{CP_3_and_7_cent_stamps_by_induction}
\pinput{PS_team_division}
\pinput{PS_prime_divides_integer_product}

\begin{editingnotes}
%\begin{problem}
Use strong induction to prove the Well Ordering Principle. \hint Prove
that if a set of nonnegative integers contains an integer, $n$, then it
has a smallest element.
%\end{problem}
\end{editingnotes}

\begin{editingnotes}

Simultaneous recursive definitions:

  You can define several things at the same time, in terms of each
  other.  For example, we may define two functions $f$ and $g$ from
  $\naturals$ to $\naturals$, recursively, by:
  \begin{itemize}
  \item
    $f(0) \eqdef 1$,
  \item
    $g(0) \eqdef 1$,
  \item
    $f(n+1) \eqdef f(n) + g(n)$, for $n \geq 0$,
  \item
    $g(n+1) \eqdef f(n) \times g(n)$, for $n \geq 0$.
  \end{itemize}

\end{editingnotes}

\begin{editingnotes}

\subsubsection{Induction on Fibonacci Numbers}

We can use the recursive definition of a function to establish its
properties by structural induction.

As an illustration, we'll prove a cute identity involving Fibonacci
numbers.  Fibonacci numbers provide lots of fun for mathematicians because
they satisfy many such identities.
\begin{proposition}
  $\forall n \geq 0 (\Sigma_{i=0}^n F_i^2 = F_n F_{n+1})$.
\end{proposition}

Example: $n = 4$:
\[
0^2 + 1^2 + 1^2 + 2^2 + 3^2 = 15 = 3 \cdot 5.
\]
Let's try a proof by (standard, not strong) induction.  The theorem
statement suggests trying it with $P(n)$ defined as:
\[
\sum_{i=0}^n F_i^2 = F_n F_{n+1}.
\]

\textbf{Base case} ($n=0$). 
$\Sigma_{i=0}^0 F_i^2 \eqdef (F_0)^2 = 0 = F_0 F_1$ because
$F_0 \eqdef 0$.

\textbf{Inductive step} ($n\geq 0$).  Now we stare at the gap between
$P(n)$ and $P(n+1)$.  $P(n+1)$ is given by a summation that's obtained
from that for $P(n)$ by adding one term; this suggests that, once again,
we subtract.  The difference is just the term $F_{n+1}^2$.  Now, we are
assuming that the original $P(n)$ summation totals $F_n F_{n+1}$ and want
to show that the new $P(n+1)$ summation totals $F_{n+1} F_{n+2}$.  So we
would \emph{like} the difference to be
\[
F_{n+1} F_{n+2} - F_n F_{n+1}.
\]

So, the actual difference is $F_{n+1}^2$ and the difference we want is
$F_{n+1} F_{n+2} - F_n F_{n+1}$.  Are these the same?  We want to check
that:
\[
F_{n+1}^2 = F_{n+1} F_{n+2} - F_n F_{n+1}.
\]
But this is true, because it is really the Fibonacci definition in
disguise: to see this, divide by $F_{n+1}$.

\end{editingnotes}

\end{problems}
\begin{editingnotes}

\section{Tagged data}

Labelling a recursively defined data item with a tag that uniquely
determines the rule used to construct it is a standard approach to
avoiding ambiguous recursive definitions in programming.  This
amounts to working with data items that are already \term{parsed}, that
is, represented as \term{parse trees}.

For example, the parse tree for the arithmetic expression
\begin{equation}\label{ax}
-(a(x\cdot x)+ bx) + 1
\end{equation}
is shown in Figure~\ref{fig:parse}.

\begin{figure}[htbp]

\gnote{Is this in or out?  Does it need to be redrawn?}

\graphic[height=4.5in]{parsetree}
\caption{Parse tree for $-(a(x\cdot x)+ bx) + 1$.}
\label{fig:parse}
\end{figure}

In a computer, such a tree would be represented by pairs or triples
that begin with a
\emph{tag} equal to the label of the top node of the parse tree.  
The general definition of parse trees for $\aexp$'s would be:

\begin{definition}\label{arithparse}
The set, $\paexp$, of \emph{parse trees for arithmetic expressions} 
over a set of
\emph{variables}, $V$, is defined recursively as follows:
\begin{itemize}
\item \textbf{Base cases:}
\begin{enumerate}
\item If $n \in \integers$, then $\ang{\texttt{int}, n} \in \paexp$.
\item If $v \in V$, then $\ang{\texttt{var}, v} \in \paexp$.
\end{enumerate}
\item \textbf{Constructor cases:} if $e,e' \in \paexp$, then
\begin{enumerate}
\item $\ang{\texttt{sum}, e, e'} \in \paexp$,
\item $\ang{\texttt{prod}, e, e'} \in \paexp$, and
\item $\ang{\texttt{minus}, e} \in \paexp$.
\end{enumerate}
\end{itemize}
\end{definition}

So the $\paexp$ corresponding to formula~\ref{ax} would be:
\begin{equation}\label{axtag}
\begin{array}{rll}
\left< \right. \texttt{sum}, 
         & \left< \right. \texttt{minus},\ \ \left< \right. \texttt{sum},
               & \ang{\texttt{prod},\ \ \ang{\texttt{var},\ a},
                                     \ang{\texttt{prod},\ \
                                            \ang{\texttt{var},\ x},\
                                            \ang{\texttt{var},\ x}}},\\
                               && \left. \left. \ang{\texttt{prod},\ \
                                       \ang{\texttt{var},\ b},\
                                       \ang{\texttt{var},\ x}}
                                   \right> \right>,\\
         & \left. \left. \ang{\texttt{int},\ 1} \right> \right>
\end{array}
\end{equation}
Now the expression~\ref{ax} is certainly a lot more humanly
intelligible than~\ref{axtag}, but~\ref{axtag} is in the
representation best suited and commonly used in compiling and
processing computer programs.

\end{editingnotes}


\begin{editingnotes}

Turn this into a problem

One precise way to determine if a string is matched is to start with 0 and
read the string from left to right, adding 1 to the count for each left
bracket and subtracting 1 from the count for each right bracket.
For example, here are the counts for the two strings above
\[\begin{array}{rrrrrrrrrrrrr}
& \lefbrk & \rhtbrk & \rhtbrk & \lefbrk & \lefbrk & \lefbrk & \lefbrk &
\lefbrk & \rhtbrk & \rhtbrk & \rhtbrk & \rhtbrk\\
0 & 1 & 0 & -1 & 0 & 1 & 2 & 3 & 4 & 3 & 2 & 1 & 0\\
\\
\\
& \lefbrk & \lefbrk & \lefbrk & \rhtbrk & \rhtbrk & \lefbrk & \rhtbrk &
\rhtbrk & \lefbrk & \rhtbrk\\
0 & 1 & 2 & 3 & 2 & 1 & 2 & 1 & 0 & 1 & 0
\end{array}\]
A string has a \term{good count} if its running count never goes
negative and ends with 0.  So the second string above has a good count, but
the first one does not because its count went negative at the third step.
\begin{definition}\label{gc-def}
Let
\[
\GC \eqdef \set{ s \in \brkts \suchthat s\ \text{has a good count}}.
\]
\end{definition}
\end{editingnotes}

\endinput
