\chapter{Random Variables and Distributions}\label{ran_var_chap}

Thus far, we have focused on probabilities of \idx{events}.  For
example, we computed the probability that you win the Monty Hall game,
or that you have a rare medical condition given that you tested
positive.  But, in many cases we would like to more more.  For
example, \emph{how many} contestants must play the Monty Hall game
until one of them finally wins?  \emph{How long} will this condition
last?  \emph{How much} will I lose gambling with strange dice all
night?  To answer such questions, we need to work with random
variables.

\section{Definitions and Examples}\label{ran_var_examples_sec}

\begin{definition}
  A \term{random variable}, $R$, on a probability space is a total function
  whose domain is the sample space.
\end{definition}
The codomain of $R$ can be anything, but will usually be a subset of
the real numbers.  Notice that the name ``random variable'' is a
misnomer; random variables are actually functions!

For example, suppose we toss three independent\footnote{Going forward,
  when we talk about flipping independent coins, we will assume that
  they are mutually independent.}, unbiased coins.  Let $C$ be the
number of heads that appear.  Let $M = 1$ if the three coins come up
all heads or all tails, and let $M = 0$ otherwise.  Every outcome of
the three coin flips uniquely determines the values of $C$ and $M$.
For example, if we flip heads, tails, heads, then $C = 2$ and $M = 0$.
If we flip tails, tails, tails, then $C = 0$ and $M = 1$.  In effect,
$C$ counts the number of heads, and $M$ indicates whether all the
coins match.

Since each outcome uniquely determines $C$ and $M$, we can regard them
as functions mapping outcomes to numbers.  For this experiment, the
sample space is
\begin{equation*}
    \sspace = \set{ HHH, HHT, HTH, HTT, THH, THT, TTH, TTT }
\end{equation*}
$C$ is a function that maps each outcome in the sample space to a
number as follows:
\begin{align*}
C(HHH) & = 3 &  C(THH) & =  2 \\
C(HHT) & = 2 &  C(THT) & =  1 \\
C(HTH) & = 2 &  C(TTH) & =  1 \\
C(HTT) & = 1 &  C(TTT) & =  0.
\end{align*}
Similarly, $M$ is a function mapping each outcome another way:
\[
\begin{array}{rclcrcl}
M(HHH) & = & 1 & \quad & M(THH) & = & 0 \\
M(HHT) & = & 0 & \quad & M(THT) & = & 0 \\
M(HTH) & = & 0 & \quad & M(TTH) & = & 0 \\
M(HTT) & = & 0 & \quad & M(TTT) & = & 1.
\end{array}
\]
So $C$ and $M$ are \idx{random variables}.

\subsection{Indicator Random Variables}

An \term{indicator random variable} is a random variable that maps
every outcome to either 0 or~1.  Indicator random variables are also
called \term{Bernoulli variables}.  The random variable~$M$ is an
example.  If all three coins match, then $M=1$; otherwise, $M = 0$.

Indicator random variables are closely related to events.  In
particular, an indicator partitions the sample space into those
outcomes mapped to 1 and those outcomes mapped to 0.  For example, the
indicator $M$ partitions the sample space into two blocks as follows:
\[
\underbrace{HHH \quad TTT}_{\text{$M = 1$}} \quad
\underbrace{HHT \quad HTH \quad HTT \quad
        THH \quad THT \quad TTH}_{\text{$M = 0$}}.
\]

In the same way, an event, $E$, partitions the sample space into those
outcomes in $E$ and those not in $E$.  So $E$ is naturally associated with
an indicator random variable, \index{$I_E$, indicator for event $E$}
$I_E$, where $I_E(w) = 1$ for outcomes $w \in E$ and $I_E(w) = 0$ for
outcomes $w \notin E$.  Thus, $M=I_E$ where $E$ is the event that all
three coins match.

\subsection{Random Variables and Events}

There is a strong relationship between events and more general random
variables as well.  A random variable that takes on several values
partitions the sample space into several blocks.  For example, $C$
partitions the sample space as follows:
\[
\underbrace{TTT}_{\text{$C = 0$}} \quad
\underbrace{TTH \quad THT \quad HTT}_{\text{$C = 1$}} \quad
\underbrace{THH \quad HTH \quad HHT}_{\text{$C = 2$}} \quad
\underbrace{HHH}_{\text{$C = 3$}}.
\]
Each block is a subset of the sample space and is therefore
an event.  Thus, we can regard an equation or inequality involving a
random variable as an event.  For example, the event that $C = 2$
consists of the outcomes $THH$, $HTH$, and $HHT$.  The event $C \leq
1$ consists of the outcomes $TTT$, $TTH$, $THT$, and $HTT$.

Naturally enough, we can talk about the probability of events defined
by properties of random variables.  For example,
\begingroup
\openup2pt
\begin{align*}
\pr{C = 2}
        &=    \pr{THH} + \pr{HTH} + \pr{HHT} \\
        &=    \frac{1}{8} + \frac{1}{8} + \frac{1}{8} \\
        &=  \frac{3}{8}.
\end{align*}
\endgroup

As another example:
\begingroup\openup2pt
\begin{align*}
\pr{M = 1}
        &= \pr{TTT} + \pr{HHH}\\
        &= \frac{1}{8} + \frac{1}{8} \\
        &= \frac{1}{4}.
\end{align*}
\endgroup

\subsection{Functions of Random Variables}

Random variables can be combined to form other random variables.  For
example, suppose that you roll two unbiased, independent 6-sided
dice.  Let $D_i$~be the random variable denoting the outcome of the
$i$th~die for $i = 1$,~$2$.  For example,
\begin{equation*}
    \pr{D_1 = 3} = 1/6.
\end{equation*}
Then let $T = D_1 + D+2$. \ $T$~is also a random variable and it
denotes the sum of the two dice.  For example,
\begin{align*}
    \pr{T = 2} &= 1/36 \\
\noalign{\noindent and}
    \pr{T = 7} &= 1/6.
\end{align*}

Random variables can be combined in complicated ways, as we will see
in Chapter~\ref{deviation_chap}.  For example,
\begin{equation*}
    Y = e^T
\end{equation*}
is also a random variable.  In this case,
\begin{align*}
    \pr{Y = e^2} &= 1/36 \\
\noalign{\noindent and}
    \pr{Y = e^7} &= 1/6.
\end{align*}

\subsection{Conditional Probability}

Mixing conditional probabilities and events involving random variables
creates no new difficulties.  For example, $\prcond{C \geq 2}{M = 0}$
is the probability that at least two coins are heads ($C \geq 2$),
given that not all three coins are the same ($M = 0$).  We can compute
this probability using the definition of conditional probability:
\begingroup\openup2pt
\begin{align*}
\prcond{C \geq 2}{M = 0}
        & =    \frac{\pr{[C \geq 2] \intersect [M = 0]}}{\pr{M = 0}} \\
        & =    \frac{\pr{\set{THH, HTH, HHT}}}
                        {\pr{\set{THH, HTH, HHT, HTT, THT, TTH }}} \\
        & =    \frac{3/8}{6/8} \\
        & = \frac{1}{2}.
\end{align*}
\endgroup
The expression $[C \geq 2] \intersect [M = 0]$ on the first line may look odd;
what is the set operation $\intersect$ doing between an inequality and an
equality?  But recall that, in this context, $[C \geq 2]$ and $[M = 0]$
are \emph{events}, namely, \emph{sets} of outcomes.

\subsection{Independence}

The notion of independence carries over from events to random variables as
well.  Random variables $R_1$ and $R_2$ are \index{independent random
  variables} \emph{independent} iff for all $x_1$ in the codomain of
$R_1$, and $x_2$ in the codomain of $R_2$, we have:
\[
\pr{R_1 = x_1 \QAND R_2 = x_2}  =  \pr{R_1 = x_1} \cdot \pr{R_2 = x_2}.
\]
As with events, we can formulate independence for random
variables in an equivalent and perhaps more intuitive way: random
variables $R_1$ and $R_2$ are independent if for all $x_1$ and $x_2$
\[
\prcond{R_1 = x_1}{R_2 = x_2}  =  \pr{R_1 = x_1}.
\]
whenever the left-hand conditional probability is defined, that is,
whenever
\begin{equation*}
    \pr{R_2 = x_2} > 0.
\end{equation*}

For example, are $C$ and~$M$ independent?  Intuitively, the answer
should be ``no''.  The number of heads, $C$, completely determines
whether all three coins match; that is, whether $M = 1$.  But, to
verify this intuition, we must find some $x_1, x_2 \in \reals$ such
that:
\[
\pr{C = x_1 \QAND M = x_2} \neq \pr{C = x_1} \cdot \pr{M = x_2}.
\]
One appropriate choice of values is $x_1 = 2$ and $x_2 = 1$.
In this case, we have:
\[
    \pr{C = 2 \QAND M = 1} = 0
\]
and
\begin{equation*}
    \pr{M = 1} \cdot \pr{C = 2} = \frac{1}{4} \cdot \frac{3}{8} \ne 0.
\end{equation*}
The first probability is zero because we never have exactly two heads
($C = 2$) when all three coins match ($M = 1$).  The other two
probabilities were computed earlier.

On the other hand, let $H_1$ be the indicator variable for the event
that the first flip is a Head, so
\[
[H_1 = 1] = \set{HHH, HTH, HHT, HTT}.
\]
Then $H_1$ is independent of $M$, since
\begin{align*}
\pr{M=1} & = 1/4 = \prcond{M=1}{H_1=1} = \prcond{M=1}{H_1=0}\\
\pr{M=0} & = 3/4 = \prcond{M=0}{H_1=1} = \prcond{M=0}{H_1=0}
\end{align*}
This example is an instance of a simple lemma:
\begin{lemma}
  Two events are independent iff their \idx{indicator variables} are
  independent.
\end{lemma}
As with events, the notion of independence generalizes to more than two
random variables.
\begin{definition}\label{def:RV_mutually_ind}
Random variables $R_1, R_2, \dots, R_n$ are \term{mutually independent} iff
\begin{align*}
\lefteqn{\pr{R_1 = x_1 \QAND R_2 = x_2 \QAND \cdots \QAND R_n = x_n}}\\
        & =  \pr{R_1 = x_1} \cdot \pr{R_2 = x_2} \cdots \pr{R_n = x_n}.
\end{align*}
for all $x_1, x_2, \dots, x_n$.
\end{definition}

A consequence of Definition~\ref{def:RV_mutually_ind} is that the
probability that any \emph{subset} of the variables takes a particular
set of values is equal to the product of the probabilities that the
individual variables take their values.  Thus, for example, if $R_1,
R_2, \dots, R_{100}$ are mutually independent random variables, then
it follows that:
\begin{align*}
\lefteqn{\pr{R_1 = 7 \QAND R_7 = 9.1 \QAND R_{23} = \pi}}\qquad \\
& = \pr{R_1 = 7} \cdot \pr{R_7 = 9.1} \cdot \pr{R_{23} = \pi}.
\end{align*}
The proof is based on summing over all possible values for all of the
other random variables.

\subsection{Distribution Functions}\label{distributions_sec}

A random variable maps outcomes to values. Often, random variables
that show up for different spaces of outcomes wind up behaving in much
the same way because they have the same probability of having any
given value.  Hence, random variables on different probability spaces
may wind up having the same \term{probability density function}.

\begin{definition}
Let $R$ be a random variable with codomain $V$.
The \term{probability density function (pdf)} of $R$
is a function $\pdf_R : V \to [0,1]$ defined by:
%
\[
\pdf_R(x) \eqdef \begin{cases}
            \pr{R = x} & \text{if } x \in \range{R},\\
             0 & \text{if } x \notin \range{R}.
           \end{cases}
\]
\end{definition}
%
A consequence of this definition is that
%
\[
\sum_{x \in \range{R}} \pdf_R(x) = 1.
\]
This is because $R$~has a value for each outcome, so summing the
probabilities over all outcomes is the same as summing over the
probabilities of each value in the range of~$R$.

As an example, suppose that you roll two unbiased, independent,
6-sided dice.  Let $T$~be the random variable that equals the sum of
the two rolls.  This random variable takes on values in the set $V =
\set{2, 3, \dots, 12}$.  A plot of the probability density function is
shown in Figure~\ref{fig:16F2}:
%
\begin{figure}

\gnote{Figure\_16F2.pdf}

\begin{picture}(305,110)(-75,-40)
%\put(-75,-40){\dashbox(305,110)} % bounding box
\put(0,0){\vector(1,0){230}}
\put(0,0){\vector(0,1){70}}
\put(-1,60){\line(1,0){2}}
\put(-15,60){\makebox(0,0){{\small $6/36$}}}
\put(-1,30){\line(1,0){2}}
\put(-15,30){\makebox(0,0){{\small $3/36$}}}
\put(-50,40){\makebox(0,0){$\pdf(x)$}}
\put(120,-30){\makebox(0,0){$x \in V$}}
\put(0,0){\framebox(20,10){}}
\put(10,-10){\makebox(0,0){2}}
\put(20,0){\framebox(20,20){}}
\put(30,-10){\makebox(0,0){3}}
\put(40,0){\framebox(20,30){}}
\put(50,-10){\makebox(0,0){4}}
\put(60,0){\framebox(20,40){}}
\put(70,-10){\makebox(0,0){5}}
\put(80,0){\framebox(20,50){}}
\put(90,-10){\makebox(0,0){6}}
\put(100,0){\framebox(20,60){}}
\put(110,-10){\makebox(0,0){7}}
\put(120,0){\framebox(20,50){}}
\put(130,-10){\makebox(0,0){8}}
\put(140,0){\framebox(20,40){}}
\put(150,-10){\makebox(0,0){9}}
\put(160,0){\framebox(20,30){}}
\put(170,-10){\makebox(0,0){10}}
\put(180,0){\framebox(20,20){}}
\put(190,-10){\makebox(0,0){11}}
\put(200,0){\framebox(20,10){}}
\put(210,-10){\makebox(0,0){12}}
\end{picture}

\caption{The probability density function for the sum of two 6-sided
  dice.}

\label{fig:16F2}

\end{figure}
%
The lump in the middle indicates that sums close to 7 are the most
likely.  The total area of all the rectangles is 1 since the dice must
take on exactly one of the sums in $V = \set{2, 3, \dots, 12}$.

A closely-related concept to a~$\pdf$ is the \term{cumulative
  distribution function (cdf)} for a random variable whose codomain is
the real numbers.  This is a function $\cdf_R : \reals \to [0,1]$
defined by:
%
\[
    \cdf_R(x) = \pr{R \leq x}.
\]
%
As an example, the cumulative distribution function for the random
variable $T$ is shown in Figure~\ref{fig:16F3}:
%
\begin{figure}

\gnote{Figure\_16F3.pdf}

\begin{picture}(305,155)(-75,-40)
%\put(-75,-40){\dashbox(305,155)} % bounding box
\put(0,0){\vector(1,0){230}}
\put(0,0){\vector(0,1){115}}
\put(-1,108){\line(1,0){2}}
\put(-15,108){\makebox(0,0){{\small $1$}}}
\put(-1,54){\line(1,0){2}}
\put(-15,54){\makebox(0,0){{\small $1/2$}}}
\put(-15,0){\makebox(0,0){{\small $0$}}}
\put(-50,81){\makebox(0,0){$\cdf(x)$}}
\put(120,-30){\makebox(0,0){$x \in V$}}
\put(0,0){\framebox(20,3){}}
\put(10,-10){\makebox(0,0){2}}
\put(20,0){\framebox(20,9){}}
\put(30,-10){\makebox(0,0){3}}
\put(40,0){\framebox(20,18){}}
\put(50,-10){\makebox(0,0){4}}
\put(60,0){\framebox(20,30){}}
\put(70,-10){\makebox(0,0){5}}
\put(80,0){\framebox(20,45){}}
\put(90,-10){\makebox(0,0){6}}
\put(100,0){\framebox(20,63){}}
\put(110,-10){\makebox(0,0){7}}
\put(120,0){\framebox(20,78){}}
\put(130,-10){\makebox(0,0){8}}
\put(140,0){\framebox(20,90){}}
\put(150,-10){\makebox(0,0){9}}
\put(160,0){\framebox(20,99){}}
\put(170,-10){\makebox(0,0){10}}
\put(180,0){\framebox(20,105){}}
\put(190,-10){\makebox(0,0){11}}
\put(200,0){\framebox(20,108){}}
\put(210,-10){\makebox(0,0){12}}
\end{picture}

\caption{The cumulative distribution function for the sum of two
  6-sided dice.}

\label{fig:16F3}

\end{figure}
%
The height of the $i$th bar in the cumulative distribution function
is equal to the \emph{sum} of the heights of the leftmost $i$ bars
in the probability density function.  This follows from the
definitions of pdf and~cdf:
%
\begin{align*}
\cdf_R(x) & = \pr{R \leq x} \\
          & = \sum_{y \leq x} \pr{R = y} \\
          & = \sum_{y \leq x} \pdf_R(y)
\end{align*}

In summary, $\pdf_R(x)$ measures the probability that $R = x$ and
$\cdf_R(x)$ measures the probability that $R \leq x$.  Both $\pdf_R$
and $\cdf_R$ capture the same information about the random variable
$R$---you can derive one from the other---but sometimes one is more
convenient.

One of the really interesting things about density functions and
distribution functions is that many random variables turn out to have
the \emph{same} pdf and~cdf.  In other words, even though $R$ and~$S$
are different random variables on different probability spaces, it is
often the case that
\begin{equation*}
    \pdf_R = \pdf_s.
\end{equation*}
In fact, some pdfs are so common that they are given special names.
For example, the three most important distributions in computer
science are the \term{Bernoulli distribution}, the \term{uniform
  distribution}, and the \term{binomial distribution}.  We look more
closely at these common distributions in the next several sections.

\section{Bernoulli Distributions}\label{sec:bernoulli_dist}

The Bernoulli distribution is the simplest and most common
distribution function.  That's because it is the distribution function
for an indicator random variable.  Specifically, the \term{Bernoulli
  distribution} has a probability density function of the form $f_p:
\set{0, 1} \to [0, 1]$ where
\begin{align*}
    f_p(0) &= p, \quad\text{and} \\
    f_p(1) &= 1 - p,
\end{align*}
for some $p \in [0, 1]$.  The corresponding cumulative distribution
function is:
%
\begin{align*}
    F_p(0) & = p \\
    F_p(1) & = 1.
\end{align*}

\section{Uniform Distributions}\label{sec:uniform_dist}

\subsection{Definition}

A random variable that takes on each possible value with the same
probability is said to be \term{uniform}.  The \term{uniform
  distribution} has a pdf of the form
\begin{equation*}
    f_p : \set{1, 2, \dots, n} \to [0,1]
\end{equation*}
where
\begin{equation*}
    f_n(k) = \frac{1}{n}
\end{equation*}
for some $n \in \naturals^+$.  The cumulative distribution function
is:
\begin{equation*}
    F_n(k) = \frac{k}{n}
\end{equation*}
Uniform distributions arise frequently in practice.  For example, the
number rolled on a fair die is uniform on the set $\set{1, 2, \dots,
  6}$.

\subsection{The Numbers Game}\label{bigger_number_subsec}

Enough definitions---let's play a game!  I have two envelopes.  Each
contains an integer in the range $0, 1, \dots, 100$, and the numbers
are distinct.  To win the game, you must determine which envelope
contains the larger number.  To give you a fighting chance, we'll let
you peek at the number in one envelope selected at random.  Can you
devise a strategy that gives you a better than 50\% chance of winning?

For example, you could just pick an envelope at random and guess that
it contains the larger number.  But this strategy wins only 50\% of
the time.  Your challenge is to do better.

So you might try to be more clever.  Suppose you peek in one envelope
and see the number~12.  Since 12~is a small number, you might guess
that that the number in the other envelope is larger.  But perhaps
we've been tricky and put small numbers in \emph{both} envelopes.
Then your guess might not be so good!

An important point here is that the numbers in the envelopes may
\emph{not} be random.  We're picking the numbers and we're choosing
them in a way that we think will defeat your guessing strategy.  We'll
only use randomization to choose the numbers if that serves our
purpose, which is to make you lose!

\subsubsection{Intuition Behind the Winning Strategy}

Amazingly, there is a strategy that wins more than 50\% of the time,
regardless of what numbers we put in the envelopes!

Suppose that you somehow knew a number~$x$ that was in between the
numbers in the envelopes.  Now you peek in one envelope and see a
number.  If it is bigger than~$x$, then you know you're peeking at the
higher number.  If it is smaller than $x$, then you're peeking at the
lower number.  In other words, if you know a number $x$ between the
numbers in the envelopes, then you are certain to win the game.

The only flaw with this brilliant strategy is that you do \emph{not}
know such an~$x$.  Oh well.

But what if you try to \emph{guess}~$x$?  There is some probability
that you guess correctly.  In this case, you win 100\% of the time.
On the other hand, if you guess incorrectly, then you're no worse off
than before; your chance of winning is still 50\%.  Combining these
two cases, your overall chance of winning is better than 50\%!

Informal arguments about probability, like this one, often sound
plausible, but do not hold up under close scrutiny.  In contrast, this
argument sounds completely implausible---but is actually correct!

\subsubsection{Analysis of the Winning Strategy}

For generality, suppose that we can choose numbers from the set
$\set{0, 1, \dots, n}$.  Call the lower number~$L$ and the higher
number~$H$.

Your goal is to guess a number $x$ between $L$ and $H$.  To avoid
confusing equality cases, you select $x$ at random from among the
half-integers:
%
\[
\set{\frac{1}{2},\ 1\frac{1}{2},\ 2\frac{1}{2},\ \dots,\ n - \frac{1}{2}}
\]
%
But what probability distribution should you use?

The uniform distribution turns out to be your best bet.  An informal
justification is that if I figured out that you were unlikely to pick
some number---say $50\frac{1}{2}$---then I'd always put 50 and 51 in
the envelopes.  Then you'd be unlikely to pick an $x$ between $L$ and
$H$ and would have less chance of winning.

After you've selected the number $x$, you peek into an envelope and
see some number $p$.  If $p > x$, then you guess that you're looking
at the larger number.  If $p < x$, then you guess that the other
number is larger.

All that remains is to determine the probability that this strategy
succeeds.  We can do this with the usual four step method and a tree
diagram.

\paragraph{Step 1: Find the sample space.}

You either choose~$x$ too low ($< L$), too high ($> H$), or just right
($L < x < H$).  Then you either peek at the lower number ($p = L$) or
the higher number ($p = H$).  This gives a total of six possible
outcomes, as show in Figure~\ref{fig:16F4}.

\begin{figure}[h]

\graphic[height=2.5in]{numbers-game}

\caption{The tree diagram for the numbers game.}

\label{fig:16F4}

\end{figure}

\paragraph{Step 2: Define events of interest.}

The four outcomes in the event that you win are marked in the tree
diagram.

\paragraph{Step 3: Assign outcome probabilities.}

First, we assign edge probabilities.  Your guess $x$ is too low with
probability $L/n$, too high with probability $(n-H)/n$, and just right
with probability $(H-L)/n$.  Next, you peek at either the lower or
higher number with equal probability.  Multiplying along root-to-leaf
paths gives the outcome probabilities.

\paragraph{Step 4: Compute event probabilities.}

The probability of the event that you win is the sum of the
probabilities of the four outcomes in that event:
%
\begin{align*}
\pr{\text{win}}
    & = \frac{L}{2n} + \frac{H-L}{2n} + \frac{H-L}{2n}  + \frac{n-H}{2n} \\
    & = \frac{1}{2} + \frac{H-L}{2n} \\
    & \geq \frac{1}{2} + \frac{1}{2n}
\end{align*}
%
The final inequality relies on the fact that the higher number $H$ is
at least 1 greater than the lower number $L$ since they are required
to be distinct.

Sure enough, you win with this strategy more than half the time,
regardless of the numbers in the envelopes!  For example, if I choose
numbers in the range $0, 1, \dots, 100$, then you win with probability at
least $\frac{1}{2} + \frac{1}{200} = 50.5\%$.  Even better, if I'm allowed
only numbers in the range $0, \dots, 10$, then your probability of
winning rises to 55\%!  By Las Vegas standards, those are great odds!

\subsection{Randomized Algorithms}

The best strategy to win the numbers game is an example of a
\term{randomized algorithm}---it uses random numbers to influence
decisions.  Protocols and algorithms that make use of random numbers
are very important in computer science.  There are many problems for
which the best known solutions are based on a random number generator.

For example, the most commonly-used protocol for deciding when to send
a broadcast on a shared bus or Ethernet is a randomized algorithm
known as \term{exponential backoff}.  One of the most commonly-used
sorting algorithms used in practice (called \term{quicksort}) uses
random numbers.  You'll see many more examples if you take an
algorithms course.  In each case, randomness is used to improve the
probability that the algorithm runs quickly or otherwise performs
well.

\section{Binomial Distribution}\label{binomial_distribution_section}

\subsection{Definitions}

The third commonly-used distribution in computer science is the
\term{binomial distribution}.  The standard example of a random
variable with a binomial distribution is the number of heads that come
up in $n$ independent flips of a coin.  If the coin is fair, then the
number of heads has an \term{unbiased binomial distribution},
specified by the pdf
\begin{equation*}
    f_n: \set{1, 2, \dots, k} \to [0, 1]
\end{equation*}
where
\begin{equation*}
    f_n(k) = \binom{n}{k} 2^{-n}
\end{equation*}
for some $n \in \naturals^+$.  This is because there are
$\binom{n}{k}$ sequences of $n$ coin tosses with exactly $k$ heads,
and each such sequence has probability $2^{-n}$.

A plot of~$f_{20}(k)$ is shown in Figure~\ref{fig:16F5}.  The most
likely outcome is $k = 10$ heads, and the probability falls off
rapidly for larger and smaller values of $k$.  The falloff regions to
the left and right of the main hump are usually called the \term{tails
  of the distribution}.  We'll talk a lot more about these tails in
Section~\ref{expectation_sec}.

\begin{figure}

\gnote{Figure\_16F5.pdf}

% GNUPLOT: LaTeX picture
\setlength{\unitlength}{0.240900pt}
\ifx\plotpoint\undefined\newsavebox{\plotpoint}\fi
\sbox{\plotpoint}{\rule[-0.200pt]{0.400pt}{0.400pt}}%
\begin{picture}(1500,900)(0,0)
\font\gnuplot=cmr10 at 10pt
\gnuplot
\sbox{\plotpoint}{\rule[-0.200pt]{0.400pt}{0.400pt}}%
\put(140.0,82.0){\rule[-0.200pt]{4.818pt}{0.400pt}}
\put(120,82){\makebox(0,0)[r]{0}}
\put(1419.0,82.0){\rule[-0.200pt]{4.818pt}{0.400pt}}
\put(140.0,168.0){\rule[-0.200pt]{4.818pt}{0.400pt}}
\put(120,168){\makebox(0,0)[r]{0.02}}
\put(1419.0,168.0){\rule[-0.200pt]{4.818pt}{0.400pt}}
\put(140.0,255.0){\rule[-0.200pt]{4.818pt}{0.400pt}}
\put(120,255){\makebox(0,0)[r]{0.04}}
\put(1419.0,255.0){\rule[-0.200pt]{4.818pt}{0.400pt}}
\put(140.0,341.0){\rule[-0.200pt]{4.818pt}{0.400pt}}
\put(120,341){\makebox(0,0)[r]{0.06}}
\put(1419.0,341.0){\rule[-0.200pt]{4.818pt}{0.400pt}}
\put(140.0,428.0){\rule[-0.200pt]{4.818pt}{0.400pt}}
\put(120,428){\makebox(0,0)[r]{0.08}}
\put(1419.0,428.0){\rule[-0.200pt]{4.818pt}{0.400pt}}
\put(140.0,514.0){\rule[-0.200pt]{4.818pt}{0.400pt}}
\put(120,514){\makebox(0,0)[r]{0.1}}
\put(1419.0,514.0){\rule[-0.200pt]{4.818pt}{0.400pt}}
\put(140.0,601.0){\rule[-0.200pt]{4.818pt}{0.400pt}}
\put(120,601){\makebox(0,0)[r]{0.12}}
\put(1419.0,601.0){\rule[-0.200pt]{4.818pt}{0.400pt}}
\put(140.0,687.0){\rule[-0.200pt]{4.818pt}{0.400pt}}
\put(120,687){\makebox(0,0)[r]{0.14}}
\put(1419.0,687.0){\rule[-0.200pt]{4.818pt}{0.400pt}}
\put(140.0,774.0){\rule[-0.200pt]{4.818pt}{0.400pt}}
\put(120,774){\makebox(0,0)[r]{0.16}}
\put(1419.0,774.0){\rule[-0.200pt]{4.818pt}{0.400pt}}
\put(140.0,860.0){\rule[-0.200pt]{4.818pt}{0.400pt}}
\put(120,860){\makebox(0,0)[r]{0.18}}
\put(1419.0,860.0){\rule[-0.200pt]{4.818pt}{0.400pt}}
\put(140.0,82.0){\rule[-0.200pt]{0.400pt}{4.818pt}}
\put(140,41){\makebox(0,0){0}}
\put(140.0,840.0){\rule[-0.200pt]{0.400pt}{4.818pt}}
\put(465.0,82.0){\rule[-0.200pt]{0.400pt}{4.818pt}}
\put(465,41){\makebox(0,0){5}}
\put(465.0,840.0){\rule[-0.200pt]{0.400pt}{4.818pt}}
\put(790.0,82.0){\rule[-0.200pt]{0.400pt}{4.818pt}}
\put(790,41){\makebox(0,0){10}}
\put(790.0,840.0){\rule[-0.200pt]{0.400pt}{4.818pt}}
\put(1114.0,82.0){\rule[-0.200pt]{0.400pt}{4.818pt}}
\put(1114,41){\makebox(0,0){15}}
\put(1114.0,840.0){\rule[-0.200pt]{0.400pt}{4.818pt}}
\put(1439.0,82.0){\rule[-0.200pt]{0.400pt}{4.818pt}}
\put(1439,41){\makebox(0,0){20}}
\put(1439.0,840.0){\rule[-0.200pt]{0.400pt}{4.818pt}}
\put(140.0,82.0){\rule[-0.200pt]{312.929pt}{0.400pt}}
\put(1439.0,82.0){\rule[-0.200pt]{0.400pt}{187.420pt}}
\put(140.0,860.0){\rule[-0.200pt]{312.929pt}{0.400pt}}
\put(140.0,82.0){\rule[-0.200pt]{0.400pt}{187.420pt}}
\put(140,82){\usebox{\plotpoint}}
\put(140.0,82.0){\rule[-0.200pt]{25.294pt}{0.400pt}}
\put(245.0,82.0){\usebox{\plotpoint}}
\put(245.0,83.0){\rule[-0.200pt]{15.899pt}{0.400pt}}
\put(311.0,83.0){\rule[-0.200pt]{0.400pt}{0.964pt}}
\put(311.0,87.0){\rule[-0.200pt]{15.658pt}{0.400pt}}
\put(376.0,87.0){\rule[-0.200pt]{0.400pt}{3.613pt}}
\put(376.0,102.0){\rule[-0.200pt]{15.899pt}{0.400pt}}
\put(442.0,102.0){\rule[-0.200pt]{0.400pt}{10.600pt}}
\put(442.0,146.0){\rule[-0.200pt]{15.658pt}{0.400pt}}
\put(507.0,146.0){\rule[-0.200pt]{0.400pt}{23.126pt}}
\put(507.0,242.0){\rule[-0.200pt]{15.899pt}{0.400pt}}
\put(573.0,242.0){\rule[-0.200pt]{0.400pt}{38.544pt}}
\put(573.0,402.0){\rule[-0.200pt]{15.899pt}{0.400pt}}
\put(639.0,402.0){\rule[-0.200pt]{0.400pt}{47.939pt}}
\put(639.0,601.0){\rule[-0.200pt]{15.658pt}{0.400pt}}
\put(704.0,601.0){\rule[-0.200pt]{0.400pt}{41.676pt}}
\put(704.0,774.0){\rule[-0.200pt]{15.899pt}{0.400pt}}
\put(770.0,774.0){\rule[-0.200pt]{0.400pt}{16.863pt}}
\put(770.0,844.0){\rule[-0.200pt]{12.527pt}{0.400pt}}
\put(822.0,774.0){\rule[-0.200pt]{0.400pt}{16.863pt}}
\put(822.0,774.0){\rule[-0.200pt]{15.899pt}{0.400pt}}
\put(888.0,601.0){\rule[-0.200pt]{0.400pt}{41.676pt}}
\put(888.0,601.0){\rule[-0.200pt]{15.899pt}{0.400pt}}
\put(954.0,402.0){\rule[-0.200pt]{0.400pt}{47.939pt}}
\put(954.0,402.0){\rule[-0.200pt]{15.658pt}{0.400pt}}
\put(1019.0,242.0){\rule[-0.200pt]{0.400pt}{38.544pt}}
\put(1019.0,242.0){\rule[-0.200pt]{15.899pt}{0.400pt}}
\put(1085.0,146.0){\rule[-0.200pt]{0.400pt}{23.126pt}}
\put(1085.0,146.0){\rule[-0.200pt]{15.658pt}{0.400pt}}
\put(1150.0,102.0){\rule[-0.200pt]{0.400pt}{10.600pt}}
\put(1150.0,102.0){\rule[-0.200pt]{15.899pt}{0.400pt}}
\put(1216.0,87.0){\rule[-0.200pt]{0.400pt}{3.613pt}}
\put(1216.0,87.0){\rule[-0.200pt]{15.899pt}{0.400pt}}
\put(1282.0,83.0){\rule[-0.200pt]{0.400pt}{0.964pt}}
\put(1282.0,83.0){\rule[-0.200pt]{15.658pt}{0.400pt}}
\put(1347.0,82.0){\usebox{\plotpoint}}
\put(1347.0,82.0){\rule[-0.200pt]{22.163pt}{0.400pt}}
\end{picture}

\caption{The pdf for the unbiased binomial distribution for $n =
  20$,~$f_{20}(k)$.}

\label{fig:16F5}

\end{figure}

The cumulative distribution function for the unbiased binomial
distribution is
\begin{equation*}
    F_n(k) = \sum_{i = 0}^k \binom{n}{i} 2^{-n}
\end{equation*}
for $k \in \set{1, 2, \dots, n}$.

\subsubsection{The General Binomial Distribution}

\dmj{First sentence garbled.  I'm confused.}  If the coins are biased
so that each coin is heads with probability~$p$.  Then the number of
heads has a \term{general binomial density function} specified by the
pdf
\begin{equation*}
    f_{n,p} : \set{1, 2, \dots, n} \to [0, 1]
\end{equation*}
where
\[
    f_{n, p}(k) = \binom{n}{k} p^k (1-p)^{n-k}.
\]
for some $n \in \naturals^+$ and $p \in [0, 1]$.  This is because
there are $\binom{n}{k}$ sequences with $k$ heads and $n - k$ tails,
but now the probability of each such sequence is $p^k (1-p)^{n-k}$.

For example, the plot in Figure~\ref{fig:16F7} shows the probability
density function $f_{n, p}(k)$ corresponding to flipping $n=20$
independent coins that are heads with probability $p = 0.75$.  The
graph shows that we are most likely to get around $k = 15$ heads, as
you might expect.  Once again, the probability falls off quickly for
larger and smaller values of $k$.
%
\begin{figure}
% GNUPLOT: LaTeX picture
\setlength{\unitlength}{0.240900pt}
\ifx\plotpoint\undefined\newsavebox{\plotpoint}\fi
\sbox{\plotpoint}{\rule[-0.200pt]{0.400pt}{0.400pt}}%
\begin{picture}(1500,900)(0,0)
\font\gnuplot=cmr10 at 10pt
\gnuplot
\sbox{\plotpoint}{\rule[-0.200pt]{0.400pt}{0.400pt}}%
\put(140.0,82.0){\rule[-0.200pt]{4.818pt}{0.400pt}}
\put(120,82){\makebox(0,0)[r]{0}}
\put(1419.0,82.0){\rule[-0.200pt]{4.818pt}{0.400pt}}
\put(140.0,238.0){\rule[-0.200pt]{4.818pt}{0.400pt}}
\put(120,238){\makebox(0,0)[r]{0.05}}
\put(1419.0,238.0){\rule[-0.200pt]{4.818pt}{0.400pt}}
\put(140.0,393.0){\rule[-0.200pt]{4.818pt}{0.400pt}}
\put(120,393){\makebox(0,0)[r]{0.1}}
\put(1419.0,393.0){\rule[-0.200pt]{4.818pt}{0.400pt}}
\put(140.0,549.0){\rule[-0.200pt]{4.818pt}{0.400pt}}
\put(120,549){\makebox(0,0)[r]{0.15}}
\put(1419.0,549.0){\rule[-0.200pt]{4.818pt}{0.400pt}}
\put(140.0,704.0){\rule[-0.200pt]{4.818pt}{0.400pt}}
\put(120,704){\makebox(0,0)[r]{0.2}}
\put(1419.0,704.0){\rule[-0.200pt]{4.818pt}{0.400pt}}
\put(140.0,860.0){\rule[-0.200pt]{4.818pt}{0.400pt}}
\put(120,860){\makebox(0,0)[r]{0.25}}
\put(1419.0,860.0){\rule[-0.200pt]{4.818pt}{0.400pt}}
\put(140.0,82.0){\rule[-0.200pt]{0.400pt}{4.818pt}}
\put(140,41){\makebox(0,0){0}}
\put(140.0,840.0){\rule[-0.200pt]{0.400pt}{4.818pt}}
\put(449.0,82.0){\rule[-0.200pt]{0.400pt}{4.818pt}}
\put(449,41){\makebox(0,0){5}}
\put(449.0,840.0){\rule[-0.200pt]{0.400pt}{4.818pt}}
\put(759.0,82.0){\rule[-0.200pt]{0.400pt}{4.818pt}}
\put(759,41){\makebox(0,0){10}}
\put(759.0,840.0){\rule[-0.200pt]{0.400pt}{4.818pt}}
\put(1068.0,82.0){\rule[-0.200pt]{0.400pt}{4.818pt}}
\put(1068,41){\makebox(0,0){15}}
\put(1068.0,840.0){\rule[-0.200pt]{0.400pt}{4.818pt}}
\put(1377.0,82.0){\rule[-0.200pt]{0.400pt}{4.818pt}}
\put(1377,41){\makebox(0,0){20}}
\put(1377.0,840.0){\rule[-0.200pt]{0.400pt}{4.818pt}}
\put(140.0,82.0){\rule[-0.200pt]{312.929pt}{0.400pt}}
\put(1439.0,82.0){\rule[-0.200pt]{0.400pt}{187.420pt}}
\put(140.0,860.0){\rule[-0.200pt]{312.929pt}{0.400pt}}
\put(140.0,82.0){\rule[-0.200pt]{0.400pt}{187.420pt}}
\put(140,82){\usebox{\plotpoint}}
\put(140.0,82.0){\rule[-0.200pt]{113.705pt}{0.400pt}}
\put(612.0,82.0){\rule[-0.200pt]{0.400pt}{0.482pt}}
\put(612.0,84.0){\rule[-0.200pt]{15.899pt}{0.400pt}}
\put(678.0,84.0){\rule[-0.200pt]{0.400pt}{1.686pt}}
\put(678.0,91.0){\rule[-0.200pt]{12.527pt}{0.400pt}}
\put(730.0,91.0){\rule[-0.200pt]{0.400pt}{5.300pt}}
\put(730.0,113.0){\rule[-0.200pt]{15.899pt}{0.400pt}}
\put(796.0,113.0){\rule[-0.200pt]{0.400pt}{12.768pt}}
\put(796.0,166.0){\rule[-0.200pt]{15.899pt}{0.400pt}}
\put(862.0,166.0){\rule[-0.200pt]{0.400pt}{25.294pt}}
\put(862.0,271.0){\rule[-0.200pt]{12.527pt}{0.400pt}}
\put(914.0,271.0){\rule[-0.200pt]{0.400pt}{38.785pt}}
\put(914.0,432.0){\rule[-0.200pt]{15.899pt}{0.400pt}}
\put(980.0,432.0){\rule[-0.200pt]{0.400pt}{42.157pt}}
\put(980.0,607.0){\rule[-0.200pt]{15.658pt}{0.400pt}}
\put(1045.0,607.0){\rule[-0.200pt]{0.400pt}{25.294pt}}
\put(1045.0,712.0){\rule[-0.200pt]{15.899pt}{0.400pt}}
\put(1111.0,672.0){\rule[-0.200pt]{0.400pt}{9.636pt}}
\put(1111.0,672.0){\rule[-0.200pt]{12.527pt}{0.400pt}}
\put(1163.0,499.0){\rule[-0.200pt]{0.400pt}{41.676pt}}
\put(1163.0,499.0){\rule[-0.200pt]{15.899pt}{0.400pt}}
\put(1229.0,290.0){\rule[-0.200pt]{0.400pt}{50.348pt}}
\put(1229.0,290.0){\rule[-0.200pt]{15.899pt}{0.400pt}}
\put(1295.0,148.0){\rule[-0.200pt]{0.400pt}{34.208pt}}
\put(1295.0,148.0){\rule[-0.200pt]{12.527pt}{0.400pt}}
\put(1347.0,92.0){\rule[-0.200pt]{0.400pt}{13.490pt}}
\put(1347.0,92.0){\rule[-0.200pt]{15.899pt}{0.400pt}}
\put(1413.0,83.0){\rule[-0.200pt]{0.400pt}{2.168pt}}
\put(1413.0,83.0){\rule[-0.200pt]{6.263pt}{0.400pt}}
\end{picture}

\caption{The pdf for the general binomial distribution~$f_{n, p}(k)$
  for $n = 20$ and $p = .75$.}

\label{fig:16F7}

\end{figure}

The cumulative distribution function for the general binomial
distribution is
\begin{equation}\label{eqn:16F5}
    F_{n, p} = \sum_{i = 0}^k \binom{n}{i} p^i (1 - p)^{n - i}
\end{equation}
for $k \in \set{1, 2, \dots, n}$.

\subsubsection{Approximating the Binomial Density Function}

Computing the general binomial density function is daunting if not
impossible when $k$ and~$n$ are large.  Fortunately, there is an
approximate closed-form formula for this function based on an
approximation for the binomial coefficient.  In the formula, $k$~is
replaced by~$\alpha n$ where $\alpha$~is a number between 0 and~1.
%
\begin{lemma}\label{LN12:bincoeff-bound}
\begin{align*}
\binom{n}{\alpha n}
        & = \frac{2^{n H(\alpha)}}{\sqrt{2 \pi \alpha (1 - \alpha) n}}
            e^{\epsilon(n) - \epsilon((1 - \alpha)n) - \epsilon(\alpha n)}\\
        & \sim \frac{2^{n H(\alpha)}}{\sqrt{2 \pi \alpha (1 - \alpha) n}}
\end{align*}
where $H(\alpha)$ is the famous \term{entropy function}
\begin{equation*}
H(\alpha) \eqdef \alpha \log_2 \frac{1}{\alpha} +
                (1 - \alpha) \log_2 \frac{1}{1 - \alpha}
\end{equation*}
and
\begin{equation*}
    \frac{1}{12 m + 1} \le \epsilon(m) \le \frac{1}{12m}
\end{equation*}
for all $m \in \naturals^+$.
\end{lemma}

%
The graph of $H$ is shown in Figure~\ref{LN12:entropy}.

                                % GNUPLOT: LaTeX picture
\begin{figure}
\setlength{\unitlength}{0.240900pt}
\ifx\plotpoint\undefined\newsavebox{\plotpoint}\fi
\sbox{\plotpoint}{\rule[-0.200pt]{0.400pt}{0.400pt}}%
\begin{picture}(1500,900)(0,0)
  \font\gnuplot=cmr10 at 10pt
  \gnuplot
  \sbox{\plotpoint}{\rule[-0.200pt]{0.400pt}{0.400pt}}%
  \put(220.0,113.0){\rule[-0.200pt]{292.934pt}{0.400pt}}
  \put(220.0,113.0){\rule[-0.200pt]{0.400pt}{184.048pt}}
  \put(220.0,113.0){\rule[-0.200pt]{4.818pt}{0.400pt}}
  \put(198,113){\makebox(0,0)[r]{0}}
  \put(1416.0,113.0){\rule[-0.200pt]{4.818pt}{0.400pt}}
  \put(220.0,266.0){\rule[-0.200pt]{4.818pt}{0.400pt}}
  \put(198,266){\makebox(0,0)[r]{0.2}}
  \put(1416.0,266.0){\rule[-0.200pt]{4.818pt}{0.400pt}}
  \put(220.0,419.0){\rule[-0.200pt]{4.818pt}{0.400pt}}
  \put(198,419){\makebox(0,0)[r]{0.4}}
  \put(1416.0,419.0){\rule[-0.200pt]{4.818pt}{0.400pt}}
  \put(220.0,571.0){\rule[-0.200pt]{4.818pt}{0.400pt}}
  \put(198,571){\makebox(0,0)[r]{0.6}}
  \put(1416.0,571.0){\rule[-0.200pt]{4.818pt}{0.400pt}}
  \put(220.0,724.0){\rule[-0.200pt]{4.818pt}{0.400pt}}
  \put(198,724){\makebox(0,0)[r]{0.8}}
  \put(1416.0,724.0){\rule[-0.200pt]{4.818pt}{0.400pt}}
  \put(220.0,877.0){\rule[-0.200pt]{4.818pt}{0.400pt}}
  \put(198,877){\makebox(0,0)[r]{1}}
  \put(1416.0,877.0){\rule[-0.200pt]{4.818pt}{0.400pt}}
  \put(220.0,113.0){\rule[-0.200pt]{0.400pt}{4.818pt}}
  \put(220,68){\makebox(0,0){0}}
  \put(220.0,857.0){\rule[-0.200pt]{0.400pt}{4.818pt}}
  \put(463.0,113.0){\rule[-0.200pt]{0.400pt}{4.818pt}}
  \put(463,68){\makebox(0,0){0.2}}
  \put(463.0,857.0){\rule[-0.200pt]{0.400pt}{4.818pt}}
  \put(706.0,113.0){\rule[-0.200pt]{0.400pt}{4.818pt}}
  \put(706,68){\makebox(0,0){0.4}}
  \put(706.0,857.0){\rule[-0.200pt]{0.400pt}{4.818pt}}
  \put(950.0,113.0){\rule[-0.200pt]{0.400pt}{4.818pt}}
  \put(950,68){\makebox(0,0){0.6}}
  \put(950.0,857.0){\rule[-0.200pt]{0.400pt}{4.818pt}}
  \put(1193.0,113.0){\rule[-0.200pt]{0.400pt}{4.818pt}}
  \put(1193,68){\makebox(0,0){0.8}}
  \put(1193.0,857.0){\rule[-0.200pt]{0.400pt}{4.818pt}}
  \put(1436.0,113.0){\rule[-0.200pt]{0.400pt}{4.818pt}}
  \put(1436,68){\makebox(0,0){1}}
  \put(1436.0,857.0){\rule[-0.200pt]{0.400pt}{4.818pt}}
  \put(220.0,113.0){\rule[-0.200pt]{292.934pt}{0.400pt}}
  \put(1436.0,113.0){\rule[-0.200pt]{0.400pt}{184.048pt}}
  \put(220.0,877.0){\rule[-0.200pt]{292.934pt}{0.400pt}}
  \put(45,495){\makebox(0,0){$H(\alpha)$}}
  \put(828,23){\makebox(0,0){$\alpha$}}
  \put(220.0,113.0){\rule[-0.200pt]{0.400pt}{184.048pt}}
  \put(232,175){\usebox{\plotpoint}}
  \multiput(232.58,175.00)(0.493,1.845){23}{\rule{0.119pt}{1.546pt}}
  \multiput(231.17,175.00)(13.000,43.791){2}{\rule{0.400pt}{0.773pt}}
  \multiput(245.58,222.00)(0.492,1.746){21}{\rule{0.119pt}{1.467pt}}
  \multiput(244.17,222.00)(12.000,37.956){2}{\rule{0.400pt}{0.733pt}}
  \multiput(257.58,263.00)(0.492,1.573){21}{\rule{0.119pt}{1.333pt}}
  \multiput(256.17,263.00)(12.000,34.233){2}{\rule{0.400pt}{0.667pt}}
  \multiput(269.58,300.00)(0.492,1.401){21}{\rule{0.119pt}{1.200pt}}
  \multiput(268.17,300.00)(12.000,30.509){2}{\rule{0.400pt}{0.600pt}}
  \multiput(281.58,333.00)(0.493,1.250){23}{\rule{0.119pt}{1.085pt}}
  \multiput(280.17,333.00)(13.000,29.749){2}{\rule{0.400pt}{0.542pt}}
  \multiput(294.58,365.00)(0.492,1.272){21}{\rule{0.119pt}{1.100pt}}
  \multiput(293.17,365.00)(12.000,27.717){2}{\rule{0.400pt}{0.550pt}}
  \multiput(306.58,395.00)(0.492,1.142){21}{\rule{0.119pt}{1.000pt}}
  \multiput(305.17,395.00)(12.000,24.924){2}{\rule{0.400pt}{0.500pt}}
  \multiput(318.58,422.00)(0.493,1.052){23}{\rule{0.119pt}{0.931pt}}
  \multiput(317.17,422.00)(13.000,25.068){2}{\rule{0.400pt}{0.465pt}}
  \multiput(331.58,449.00)(0.492,1.056){21}{\rule{0.119pt}{0.933pt}}
  \multiput(330.17,449.00)(12.000,23.063){2}{\rule{0.400pt}{0.467pt}}
  \multiput(343.58,474.00)(0.492,0.970){21}{\rule{0.119pt}{0.867pt}}
  \multiput(342.17,474.00)(12.000,21.201){2}{\rule{0.400pt}{0.433pt}}
  \multiput(355.58,497.00)(0.492,0.970){21}{\rule{0.119pt}{0.867pt}}
  \multiput(354.17,497.00)(12.000,21.201){2}{\rule{0.400pt}{0.433pt}}
  \multiput(367.58,520.00)(0.493,0.853){23}{\rule{0.119pt}{0.777pt}}
  \multiput(366.17,520.00)(13.000,20.387){2}{\rule{0.400pt}{0.388pt}}
  \multiput(380.58,542.00)(0.492,0.841){21}{\rule{0.119pt}{0.767pt}}
  \multiput(379.17,542.00)(12.000,18.409){2}{\rule{0.400pt}{0.383pt}}
  \multiput(392.58,562.00)(0.492,0.841){21}{\rule{0.119pt}{0.767pt}}
  \multiput(391.17,562.00)(12.000,18.409){2}{\rule{0.400pt}{0.383pt}}
  \multiput(404.58,582.00)(0.493,0.734){23}{\rule{0.119pt}{0.685pt}}
  \multiput(403.17,582.00)(13.000,17.579){2}{\rule{0.400pt}{0.342pt}}
  \multiput(417.58,601.00)(0.492,0.712){21}{\rule{0.119pt}{0.667pt}}
  \multiput(416.17,601.00)(12.000,15.616){2}{\rule{0.400pt}{0.333pt}}
  \multiput(429.58,618.00)(0.492,0.755){21}{\rule{0.119pt}{0.700pt}}
  \multiput(428.17,618.00)(12.000,16.547){2}{\rule{0.400pt}{0.350pt}}
  \multiput(441.58,636.00)(0.492,0.669){21}{\rule{0.119pt}{0.633pt}}
  \multiput(440.17,636.00)(12.000,14.685){2}{\rule{0.400pt}{0.317pt}}
  \multiput(453.58,652.00)(0.493,0.616){23}{\rule{0.119pt}{0.592pt}}
  \multiput(452.17,652.00)(13.000,14.771){2}{\rule{0.400pt}{0.296pt}}
  \multiput(466.58,668.00)(0.492,0.625){21}{\rule{0.119pt}{0.600pt}}
  \multiput(465.17,668.00)(12.000,13.755){2}{\rule{0.400pt}{0.300pt}}
  \multiput(478.58,683.00)(0.492,0.582){21}{\rule{0.119pt}{0.567pt}}
  \multiput(477.17,683.00)(12.000,12.824){2}{\rule{0.400pt}{0.283pt}}
  \multiput(490.00,697.58)(0.497,0.493){23}{\rule{0.500pt}{0.119pt}}
  \multiput(490.00,696.17)(11.962,13.000){2}{\rule{0.250pt}{0.400pt}}
  \multiput(503.58,710.00)(0.492,0.539){21}{\rule{0.119pt}{0.533pt}}
  \multiput(502.17,710.00)(12.000,11.893){2}{\rule{0.400pt}{0.267pt}}
  \multiput(515.58,723.00)(0.492,0.539){21}{\rule{0.119pt}{0.533pt}}
  \multiput(514.17,723.00)(12.000,11.893){2}{\rule{0.400pt}{0.267pt}}
  \multiput(527.00,736.58)(0.496,0.492){21}{\rule{0.500pt}{0.119pt}}
  \multiput(527.00,735.17)(10.962,12.000){2}{\rule{0.250pt}{0.400pt}}
  \multiput(539.00,748.58)(0.590,0.492){19}{\rule{0.573pt}{0.118pt}}
  \multiput(539.00,747.17)(11.811,11.000){2}{\rule{0.286pt}{0.400pt}}
  \multiput(552.00,759.58)(0.600,0.491){17}{\rule{0.580pt}{0.118pt}}
  \multiput(552.00,758.17)(10.796,10.000){2}{\rule{0.290pt}{0.400pt}}
  \multiput(564.00,769.58)(0.543,0.492){19}{\rule{0.536pt}{0.118pt}}
  \multiput(564.00,768.17)(10.887,11.000){2}{\rule{0.268pt}{0.400pt}}
  \multiput(576.00,780.59)(0.669,0.489){15}{\rule{0.633pt}{0.118pt}}
  \multiput(576.00,779.17)(10.685,9.000){2}{\rule{0.317pt}{0.400pt}}
  \multiput(588.00,789.59)(0.728,0.489){15}{\rule{0.678pt}{0.118pt}}
  \multiput(588.00,788.17)(11.593,9.000){2}{\rule{0.339pt}{0.400pt}}
  \multiput(601.00,798.59)(0.669,0.489){15}{\rule{0.633pt}{0.118pt}}
  \multiput(601.00,797.17)(10.685,9.000){2}{\rule{0.317pt}{0.400pt}}
  \multiput(613.00,807.59)(0.758,0.488){13}{\rule{0.700pt}{0.117pt}}
  \multiput(613.00,806.17)(10.547,8.000){2}{\rule{0.350pt}{0.400pt}}
  \multiput(625.00,815.59)(0.950,0.485){11}{\rule{0.843pt}{0.117pt}}
  \multiput(625.00,814.17)(11.251,7.000){2}{\rule{0.421pt}{0.400pt}}
  \multiput(638.00,822.59)(0.874,0.485){11}{\rule{0.786pt}{0.117pt}}
  \multiput(638.00,821.17)(10.369,7.000){2}{\rule{0.393pt}{0.400pt}}
  \multiput(650.00,829.59)(1.033,0.482){9}{\rule{0.900pt}{0.116pt}}
  \multiput(650.00,828.17)(10.132,6.000){2}{\rule{0.450pt}{0.400pt}}
  \multiput(662.00,835.59)(1.033,0.482){9}{\rule{0.900pt}{0.116pt}}
  \multiput(662.00,834.17)(10.132,6.000){2}{\rule{0.450pt}{0.400pt}}
  \multiput(674.00,841.59)(1.123,0.482){9}{\rule{0.967pt}{0.116pt}}
  \multiput(674.00,840.17)(10.994,6.000){2}{\rule{0.483pt}{0.400pt}}
  \multiput(687.00,847.59)(1.267,0.477){7}{\rule{1.060pt}{0.115pt}}
  \multiput(687.00,846.17)(9.800,5.000){2}{\rule{0.530pt}{0.400pt}}
  \multiput(699.00,852.59)(1.267,0.477){7}{\rule{1.060pt}{0.115pt}}
  \multiput(699.00,851.17)(9.800,5.000){2}{\rule{0.530pt}{0.400pt}}
  \multiput(711.00,857.60)(1.797,0.468){5}{\rule{1.400pt}{0.113pt}}
  \multiput(711.00,856.17)(10.094,4.000){2}{\rule{0.700pt}{0.400pt}}
  \multiput(724.00,861.61)(2.472,0.447){3}{\rule{1.700pt}{0.108pt}}
  \multiput(724.00,860.17)(8.472,3.000){2}{\rule{0.850pt}{0.400pt}}
  \multiput(736.00,864.61)(2.472,0.447){3}{\rule{1.700pt}{0.108pt}}
  \multiput(736.00,863.17)(8.472,3.000){2}{\rule{0.850pt}{0.400pt}}
  \multiput(748.00,867.61)(2.472,0.447){3}{\rule{1.700pt}{0.108pt}}
  \multiput(748.00,866.17)(8.472,3.000){2}{\rule{0.850pt}{0.400pt}}
  \put(760,870.17){\rule{2.700pt}{0.400pt}}
  \multiput(760.00,869.17)(7.396,2.000){2}{\rule{1.350pt}{0.400pt}}
  \put(773,872.17){\rule{2.500pt}{0.400pt}}
  \multiput(773.00,871.17)(6.811,2.000){2}{\rule{1.250pt}{0.400pt}}
  \put(785,874.17){\rule{2.500pt}{0.400pt}}
  \multiput(785.00,873.17)(6.811,2.000){2}{\rule{1.250pt}{0.400pt}}
  \put(810,875.67){\rule{2.891pt}{0.400pt}}
  \multiput(810.00,875.17)(6.000,1.000){2}{\rule{1.445pt}{0.400pt}}
  \put(797.0,876.0){\rule[-0.200pt]{3.132pt}{0.400pt}}
  \put(834,875.67){\rule{2.891pt}{0.400pt}}
  \multiput(834.00,876.17)(6.000,-1.000){2}{\rule{1.445pt}{0.400pt}}
  \put(822.0,877.0){\rule[-0.200pt]{2.891pt}{0.400pt}}
  \put(859,874.17){\rule{2.500pt}{0.400pt}}
  \multiput(859.00,875.17)(6.811,-2.000){2}{\rule{1.250pt}{0.400pt}}
  \put(871,872.17){\rule{2.500pt}{0.400pt}}
  \multiput(871.00,873.17)(6.811,-2.000){2}{\rule{1.250pt}{0.400pt}}
  \put(883,870.17){\rule{2.700pt}{0.400pt}}
  \multiput(883.00,871.17)(7.396,-2.000){2}{\rule{1.350pt}{0.400pt}}
  \multiput(896.00,868.95)(2.472,-0.447){3}{\rule{1.700pt}{0.108pt}}
  \multiput(896.00,869.17)(8.472,-3.000){2}{\rule{0.850pt}{0.400pt}}
  \multiput(908.00,865.95)(2.472,-0.447){3}{\rule{1.700pt}{0.108pt}}
  \multiput(908.00,866.17)(8.472,-3.000){2}{\rule{0.850pt}{0.400pt}}
  \multiput(920.00,862.95)(2.472,-0.447){3}{\rule{1.700pt}{0.108pt}}
  \multiput(920.00,863.17)(8.472,-3.000){2}{\rule{0.850pt}{0.400pt}}
  \multiput(932.00,859.94)(1.797,-0.468){5}{\rule{1.400pt}{0.113pt}}
  \multiput(932.00,860.17)(10.094,-4.000){2}{\rule{0.700pt}{0.400pt}}
  \multiput(945.00,855.93)(1.267,-0.477){7}{\rule{1.060pt}{0.115pt}}
  \multiput(945.00,856.17)(9.800,-5.000){2}{\rule{0.530pt}{0.400pt}}
  \multiput(957.00,850.93)(1.267,-0.477){7}{\rule{1.060pt}{0.115pt}}
  \multiput(957.00,851.17)(9.800,-5.000){2}{\rule{0.530pt}{0.400pt}}
  \multiput(969.00,845.93)(1.123,-0.482){9}{\rule{0.967pt}{0.116pt}}
  \multiput(969.00,846.17)(10.994,-6.000){2}{\rule{0.483pt}{0.400pt}}
  \multiput(982.00,839.93)(1.033,-0.482){9}{\rule{0.900pt}{0.116pt}}
  \multiput(982.00,840.17)(10.132,-6.000){2}{\rule{0.450pt}{0.400pt}}
  \multiput(994.00,833.93)(1.033,-0.482){9}{\rule{0.900pt}{0.116pt}}
  \multiput(994.00,834.17)(10.132,-6.000){2}{\rule{0.450pt}{0.400pt}}
  \multiput(1006.00,827.93)(0.874,-0.485){11}{\rule{0.786pt}{0.117pt}}
  \multiput(1006.00,828.17)(10.369,-7.000){2}{\rule{0.393pt}{0.400pt}}
  \multiput(1018.00,820.93)(0.950,-0.485){11}{\rule{0.843pt}{0.117pt}}
  \multiput(1018.00,821.17)(11.251,-7.000){2}{\rule{0.421pt}{0.400pt}}
  \multiput(1031.00,813.93)(0.758,-0.488){13}{\rule{0.700pt}{0.117pt}}
  \multiput(1031.00,814.17)(10.547,-8.000){2}{\rule{0.350pt}{0.400pt}}
  \multiput(1043.00,805.93)(0.669,-0.489){15}{\rule{0.633pt}{0.118pt}}
  \multiput(1043.00,806.17)(10.685,-9.000){2}{\rule{0.317pt}{0.400pt}}
  \multiput(1055.00,796.93)(0.728,-0.489){15}{\rule{0.678pt}{0.118pt}}
  \multiput(1055.00,797.17)(11.593,-9.000){2}{\rule{0.339pt}{0.400pt}}
  \multiput(1068.00,787.93)(0.669,-0.489){15}{\rule{0.633pt}{0.118pt}}
  \multiput(1068.00,788.17)(10.685,-9.000){2}{\rule{0.317pt}{0.400pt}}
  \multiput(1080.00,778.92)(0.543,-0.492){19}{\rule{0.536pt}{0.118pt}}
  \multiput(1080.00,779.17)(10.887,-11.000){2}{\rule{0.268pt}{0.400pt}}
  \multiput(1092.00,767.92)(0.600,-0.491){17}{\rule{0.580pt}{0.118pt}}
  \multiput(1092.00,768.17)(10.796,-10.000){2}{\rule{0.290pt}{0.400pt}}
  \multiput(1104.00,757.92)(0.590,-0.492){19}{\rule{0.573pt}{0.118pt}}
  \multiput(1104.00,758.17)(11.811,-11.000){2}{\rule{0.286pt}{0.400pt}}
  \multiput(1117.00,746.92)(0.496,-0.492){21}{\rule{0.500pt}{0.119pt}}
  \multiput(1117.00,747.17)(10.962,-12.000){2}{\rule{0.250pt}{0.400pt}}
  \multiput(1129.58,733.79)(0.492,-0.539){21}{\rule{0.119pt}{0.533pt}}
  \multiput(1128.17,734.89)(12.000,-11.893){2}{\rule{0.400pt}{0.267pt}}
  \multiput(1141.58,720.79)(0.492,-0.539){21}{\rule{0.119pt}{0.533pt}}
  \multiput(1140.17,721.89)(12.000,-11.893){2}{\rule{0.400pt}{0.267pt}}
  \multiput(1153.00,708.92)(0.497,-0.493){23}{\rule{0.500pt}{0.119pt}}
  \multiput(1153.00,709.17)(11.962,-13.000){2}{\rule{0.250pt}{0.400pt}}
  \multiput(1166.58,694.65)(0.492,-0.582){21}{\rule{0.119pt}{0.567pt}}
  \multiput(1165.17,695.82)(12.000,-12.824){2}{\rule{0.400pt}{0.283pt}}
  \multiput(1178.58,680.51)(0.492,-0.625){21}{\rule{0.119pt}{0.600pt}}
  \multiput(1177.17,681.75)(12.000,-13.755){2}{\rule{0.400pt}{0.300pt}}
  \multiput(1190.58,665.54)(0.493,-0.616){23}{\rule{0.119pt}{0.592pt}}
  \multiput(1189.17,666.77)(13.000,-14.771){2}{\rule{0.400pt}{0.296pt}}
  \multiput(1203.58,649.37)(0.492,-0.669){21}{\rule{0.119pt}{0.633pt}}
  \multiput(1202.17,650.69)(12.000,-14.685){2}{\rule{0.400pt}{0.317pt}}
  \multiput(1215.58,633.09)(0.492,-0.755){21}{\rule{0.119pt}{0.700pt}}
  \multiput(1214.17,634.55)(12.000,-16.547){2}{\rule{0.400pt}{0.350pt}}
  \multiput(1227.58,615.23)(0.492,-0.712){21}{\rule{0.119pt}{0.667pt}}
  \multiput(1226.17,616.62)(12.000,-15.616){2}{\rule{0.400pt}{0.333pt}}
  \multiput(1239.58,598.16)(0.493,-0.734){23}{\rule{0.119pt}{0.685pt}}
  \multiput(1238.17,599.58)(13.000,-17.579){2}{\rule{0.400pt}{0.342pt}}
  \multiput(1252.58,578.82)(0.492,-0.841){21}{\rule{0.119pt}{0.767pt}}
  \multiput(1251.17,580.41)(12.000,-18.409){2}{\rule{0.400pt}{0.383pt}}
  \multiput(1264.58,558.82)(0.492,-0.841){21}{\rule{0.119pt}{0.767pt}}
  \multiput(1263.17,560.41)(12.000,-18.409){2}{\rule{0.400pt}{0.383pt}}
  \multiput(1276.58,538.77)(0.493,-0.853){23}{\rule{0.119pt}{0.777pt}}
  \multiput(1275.17,540.39)(13.000,-20.387){2}{\rule{0.400pt}{0.388pt}}
  \multiput(1289.58,516.40)(0.492,-0.970){21}{\rule{0.119pt}{0.867pt}}
  \multiput(1288.17,518.20)(12.000,-21.201){2}{\rule{0.400pt}{0.433pt}}
  \multiput(1301.58,493.40)(0.492,-0.970){21}{\rule{0.119pt}{0.867pt}}
  \multiput(1300.17,495.20)(12.000,-21.201){2}{\rule{0.400pt}{0.433pt}}
  \multiput(1313.58,470.13)(0.492,-1.056){21}{\rule{0.119pt}{0.933pt}}
  \multiput(1312.17,472.06)(12.000,-23.063){2}{\rule{0.400pt}{0.467pt}}
  \multiput(1325.58,445.14)(0.493,-1.052){23}{\rule{0.119pt}{0.931pt}}
  \multiput(1324.17,447.07)(13.000,-25.068){2}{\rule{0.400pt}{0.465pt}}
  \multiput(1338.58,417.85)(0.492,-1.142){21}{\rule{0.119pt}{1.000pt}}
  \multiput(1337.17,419.92)(12.000,-24.924){2}{\rule{0.400pt}{0.500pt}}
  \multiput(1350.58,390.43)(0.492,-1.272){21}{\rule{0.119pt}{1.100pt}}
  \multiput(1349.17,392.72)(12.000,-27.717){2}{\rule{0.400pt}{0.550pt}}
  \multiput(1362.58,360.50)(0.493,-1.250){23}{\rule{0.119pt}{1.085pt}}
  \multiput(1361.17,362.75)(13.000,-29.749){2}{\rule{0.400pt}{0.542pt}}
  \multiput(1375.58,328.02)(0.492,-1.401){21}{\rule{0.119pt}{1.200pt}}
  \multiput(1374.17,330.51)(12.000,-30.509){2}{\rule{0.400pt}{0.600pt}}
  \multiput(1387.58,294.47)(0.492,-1.573){21}{\rule{0.119pt}{1.333pt}}
  \multiput(1386.17,297.23)(12.000,-34.233){2}{\rule{0.400pt}{0.667pt}}
  \multiput(1399.58,256.91)(0.492,-1.746){21}{\rule{0.119pt}{1.467pt}}
  \multiput(1398.17,259.96)(12.000,-37.956){2}{\rule{0.400pt}{0.733pt}}
  \multiput(1411.58,215.58)(0.493,-1.845){23}{\rule{0.119pt}{1.546pt}}
  \multiput(1410.17,218.79)(13.000,-43.791){2}{\rule{0.400pt}{0.773pt}}
  \put(846.0,876.0){\rule[-0.200pt]{3.132pt}{0.400pt}}
\end{picture}

\caption{The Entropy Function}\label{LN12:entropy}

\end{figure}

Lemma~\eqref{LN12:bincoeff-bound} provides an excellent approximation
for binomial coefficients.  We'll skip its derivation, which consists
of plugging in Theorem~\ref{thm:stirling} for the factorials in the
binomial coefficient and then simplifying.

Now let's plug this formula into the general binomial density
function.  The probability of flipping $\alpha n$ heads in $n$ tosses
of a coin that comes up heads with probability $p$ is:
\begin{align}
f_{n, p}(\alpha n)
    &= \frac{ 2^{n H(\alpha)} p^{\alpha n} (1 - p)^{(1 - \alpha)n}
                e^{\epsilon(n) - \epsilon((1 - \alpha) n) -
                  \epsilon(\alpha n)}}
            { \sqrt{2 \pi \alpha (1 - \alpha) n} } \notag\\[4pt]
    &= \frac{ 2^{n \left(\alpha \log\frac{p}{\alpha}
                    + (1 - \alpha) \log \frac{1 - p}{1 - \alpha}\right)} 
                e^{\epsilon(n) - \epsilon((1 - \alpha)n) -
                  \epsilon{\alpha n}}
            }
            { \sqrt{2 \pi \alpha (1 - \alpha) n} } \notag \\[4pt]
    &\sim \frac{ 2^{n \left(\alpha \log\frac{p}{\alpha}
                    + (1 - \alpha) \log \frac{1 - p}{1 - \alpha}\right)} 
            }
            { \sqrt{2 \pi \alpha (1 - \alpha) n} } \label{LN12:binbnd}
\end{align}
%
%
This formula is as ugly as a bowling shoe, but it's useful because
it's easy to evaluate.  For example, suppose we flip a fair coin $n$
times.  What is the probability of getting \emph{exactly} $pn$ heads?
Plugging $\alpha = p$ into~Equation~\ref{LN12:binbnd} gives:
%
\begin{align*}
f_{n, p}(pn)
    &= \frac{ e^{\epsilon(n) - 2 \epsilon(n/2)} }
            { \sqrt{2 \pi p (1 - p) n} } \\[4pt]
    &\sim \frac{1}{\sqrt{2 \pi p (1 - p) n} }
\end{align*}
%
Thus, for example, if we flip a fair coin (where $p = 1/2$) \ $n =
100$ times, the probability of getting exactly 50 heads is
$0.079\dots$ or around 8\%.\footnote{The contribution of the
  $e^{\epsilon(n) - 2 \epsilon(n/2)}$ term is so small for $n = 100$
  that it disappears in the\dots.}

In many fields, including computer science, probability analyses come
down to getting small bounds on the \idx{tails} of the binomial
distribution.  In the context of a problem, this typically means that
there is very small probability that too many \emph{bad} things
happen.  For example, we would like to know that it is very unlikely
that too many bits are corrupted in a message, or that too many
servers or communication links become overloaded, or that a randomized
algorithm runs for too long.

As an example, let's compute the probability of flipping at most 75 or
more tails in 100~tosses of a fair coin.  This is the same as the
probability or flipping at most 25~heads.

The probability of getting \emph{at most} $\alpha n$ heads is given by
the cumulative binomial distribution function
\begin{equation}\label{LN12:Jsum}
F_{n, p}(\alpha n)
    = \sum_{i = 0}^{\alpha n} \binom{n}{i} p^i (1 - p)^{n - i}.
\end{equation}
We can bound this sum by bounding the ratio of successive terms.

In particular, for $i \le \alpha n$,
\begingroup
\openup3pt
\begin{align*}
\frac{ \displaystyle \binom{n}{i - 1} p^{i - 1} (1 - p)^{n - (i - 1)} }
     { \displaystyle \binom{n}{i}     p^i       (1 - p)^{n - i} }
    &=    \frac{\displaystyle
                  \frac{ n! p^{i - 1} (1 - p)^{n - i + 1} }
                       { (i - 1)! (n - i + 1) ! }
              }
              {\displaystyle
                  \frac{ n! p^i (1 - p)^{n - i} }
                       { i! (n - i)! }
              } \\
    &=    \frac{ i (1 - p) }{ (n - i + 1) p } \\
    &\le  \frac{ \alpha n (1 - p) }{ (n - \alpha n + 1) p } \\
    &\le  \frac{ \alpha (1 - p) }{ (1 - \alpha) p }.
\end{align*}
\endgroup
This means that for $\alpha < p$,
\begingroup
\openup3pt
\begin{align}
F_{n, p}(\alpha n)
    &<  f_{n, p}(\alpha n)
        \sum_{i = 0}^\infty \left[ \frac{\alpha(1 - p)}{(1 - \alpha)p}\right]^i
\notag \\
    &= \frac{\displaystyle f_{n, p}(\alpha n)}
            {\displaystyle 1 - \frac{\alpha (1 - p)}{(1 - \alpha)p}}
            \notag\\
    &= \left( \frac{1 - \alpha}{1 - \alpha/p} \right) f_{n, p}(\alpha n).
\end{align}
\endgroup

In other words, the probability of at most $\alpha n$~heads is at most
\begin{equation*}
    \frac{1 - \alpha}{1 - \alpha/p}
\end{equation*}
times the probability of exactly $\alpha n$~heads. For our scenario,
where $p = 1/2$ and $\alpha = 1/4$,
\begin{align*}
\frac{1 - \alpha}{1 - \alpha/p}
    &= \frac{3/4}{1/2} \\
    &= 3/2.
\end{align*}
Plugging $n = 100$, \ $\alpha = 1/4$, and $p = 1/2$ into
Equation~\ref{eqn:16F5}, we find that the probability of at most
25~heads in 100~coin flips is
\begin{align*}
F_{100, 1/2}(25)
    &< \frac{3}{2} \cdot
        \frac{  \displaystyle
                2^{100 \left( \frac{1}{4} \log 2
                     + \frac{3}{4} \log \frac{2}{3} \right) }
                e^{ \epsilon(100) - \epsilon(75) - \epsilon(25) }
             }
             { \sqrt{75 \pi /2} } \\[3pt]
    &\le 2.9 \cdot 10^{-7}.
\end{align*}

%% INSERT I GOES HERE

This says that flipping 25 or fewer heads is extremely unlikely, which
is consistent with our earlier claim that the tails of the binomial
distribution are very small.  In fact, notice that the probability of
flipping \emph{25 or fewer} heads is only 50\% more than the
probability of flipping \emph{exactly 25} heads.  Thus, flipping
exactly 25 heads is twice as likely as flipping any number between 0
and 24!

\noindent \textbf{Caveat}: The upper bound on $F_{n, p}(\alpha n)$ in
Equation~\ref{eqn:16F7} holds only if $\alpha < p$.  If this is not
the case in your problem, then try thinking in complementary terms;
that is, look at the number of tails flipped instead of the number of
heads.

\section{Continuous Distributions}

You may have noticed that all of the distributions we have discussed
thus far are for finite sample spaces.  That's because finite
distributions are the most common in computer science.  They are also
the easiest to work with.

More generally, there are important distributions on infinite sample
spaces.  We will briefly mention some of the most important in the
following subsections.  For the most part in this text, however, our
focus will continue to be on finite probability spaces.

\subsection{Continuous uniform Distributions}

We have already talked about the uniform distribution on a finite
sample space~$\set{1, 2, \dots, n}$.  The uniform distribution can
also be defined on the infinite sample space~$[0, n]$.  In this case,
the pdf is
\begin{equation*}
    f_n: [0, n] \to [0, 1]
\end{equation*}
where
\begin{equation*}
    f_n(x) = \frac{1}{n}
\end{equation*}
and the cdf is
\begin{equation*}
    F_n(x) = \frac{x}{n}.
\end{equation*}

The difference between the continuous and discrete uniform
distributions is that the pdf for the continuous uniform distribution
is nonzero for any real $x \in [0, n]$ whereas the pdf for the
discrete uniform distribution is nonzero only for integer~$x \in [1,
  n]$.

\subsection{Normal Distributions}

The \term{standard normal distribution} is defined by the pdf
\begin{equation*}
    f: \reals \to [0, 1]
\end{equation*}
where
\begin{equation*}
    f(x) = \frac{ e^{-x^2/2} }{ \sqrt{2\pi} }.
\end{equation*}
A graph of~$f(x)$ is shown in Figure~\ref{fig:16L1}.

\begin{figure}

\gnote{Figure L1, p. ??? (L-4), ftl-ch16-???.}

\graphic{Fig_16L1}

\caption{The plot of the pdf for the standard normal distribution.}

\label{fig:16L1}

\end{figure}

The cumulative distribution function for the standard normal is
\begin{equation*}
    F(x) = \int_{-\infty}^x \frac{ e^{-z^2/2} \, dz }{\sqrt{2\pi}}.
\end{equation*}

The general normal distribution is defined based on two parameters:
$\mu$ (the mean) and~$\sigma^2$ (the variance).  It's pdf is the
function
\begin{equation*}
    f_{\mu, \sigma^2} : \reals \to [0, 1]
\end{equation*}
where
\begin{equation}\label{eqn:16N1}
    f_{\mu, \sigma^2} = \frac{ e^{ - (x - \mu)^2 / 2 \sigma^2 } }
                             { \sqrt{2 \pi \sigma^2 } }.
\end{equation}

The normal distribution is similar to the binomial distribution in
some respects.  For example, if we set $\mu = n/2$, \ $\sigma^2 =
n/4$, and~$x = \alpha n$ in Equation~\ref{eqn:16N1}, the resulting pdf
is exponentially small in~$n$, which is similar to the behavior of
Equation~\ref{eqn:16F5} when $p = 1/2$.  We'll talk further about the
relationship in Chapter~\ref{deviation_chap}, where the reasons for
the choices of $\mu$, $\sigma^2$, and~$x$ above will become apparent.

\endinput
