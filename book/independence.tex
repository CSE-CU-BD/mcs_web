\chapter{Independence}

\section{Definition}

Suppose that we flip two fair coins simultaneously on opposite sides
of a room.  Intuitively, the way one coin lands does not affect the
way the other coin lands.  The mathematical concept that captures
this intuition is called \term{independence}:
\begin{definition}\label{def:independence}
Events $A$ and $B$ are independent if $\pr{B} = 0$ or if
\begin{equation}\label{eqn:independence}
    \pr{A \intersect B} = \pr{A} \cdot \pr{B}
\end{equation}
\end{definition}
In other words, $A$ and~$B$ are independent if knowing that $B$
happens does not alter the probability that $A$~happens, as is the
case with flipping two coins on opposite sides of a room.

Equivalently, $A$ and~$B$ are independent if and only if
\begin{equation}\label{eqn:15D3}
    \pr{A \intersect B} = \pr{A} \cdot \pr{B}.
\end{equation}
This follows from the definition of independence and
Definition~\ref{LN12:prcond}.  

Generally, independence is something you \emph{assume} in modeling a
phenomenon---or wish you could realistically assume.  Many useful
probability formulas only hold if certain events are independent, so a
dash of independence can greatly simplify the analysis of a system.

For example, consider the experiment of flipping two fair coins.  Let
$A$ be the event that the first coin comes up heads, and let $B$ be
the event that the second coin is heads.  If we assume that $A$ and
$B$ are independent, then the probability that both coins come up
heads is:
%
\begin{align*}
\pr{A \intersect B} & = \pr{A} \cdot \pr{B} \\[2pt]
              & = \frac{1}{2} \cdot \frac{1}{2} \\[2pt]
              & = \frac{1}{4}
\end{align*}

On the other hand, let $C$ be the event that tomorrow is cloudy and
$R$ be the event that tomorrow is rainy.  Perhaps $\pr{C} = 1/5$ and
$\pr{R} = 1/10$ in Boston.  If these events were independent, then we
could conclude that the probability of a rainy, cloudy day was quite
small:
%
\begin{align*}
\pr{R \intersect C} & = \pr{R} \cdot \pr{C} \\[2pt]
              & = \frac{1}{5} \cdot \frac{1}{10} \\[2pt]
              & = \frac{1}{50}
\end{align*}
%
Unfortunately, these events are definitely not independent; in
particular, every rainy day is cloudy.  Thus, the probability of a
rainy, cloudy day is actually $1/10$.


\paragraph{Potential Pitfall.}

Students sometimes get the idea that disjoint events are independent.
The \emph{opposite} is true: if $A \intersect B = \emptyset$, then
knowing that $A$ happens means you know that $B$ does not happen.  So
disjoint events are \emph{never} independent---unless one of them has
probability zero.


\section{Mutual Independence}

We have defined what it means for two events to be independent.  What
if there are more than two events?  For example, how can we say that
the flips of $n$~coins are all independent of one another?

\dmj{I tried moving the ``distinct'' constraint into the text to keep
  the display from overflowing, but I assume there is a better way to
  phrase this.}  Events $E_1, \ldots, E_n$ are \term{mutually
  independent} if and only if \emph{for every subset} of the events,
the probability of the intersection is the product of the
probabilities.  In other words, all of the following equations must
hold for all distinct $i$, $j$, $k$, and~$l$:\footnote{There is an
  equivalent definition based on conditional probabilities, as in
  Definition:~\ref{def:independence}, but it is more complicated for
  large values of~$n$.}
%
\begin{align*}
\pr{E_i \intersect E_j}
    & = \pr{E_i} \cdot \pr{E_j}
%    & \text{for all distinct $i$, $j$}
 \\
\pr{E_i \intersect E_j \intersect E_k}
    & = \pr{E_i} \cdot \pr{E_j} \cdot \pr{E_k}
%     & \text{for all distinct $i$, $j$, $k$}
 \\
\pr{E_i \intersect E_j \intersect E_k \intersect E_l}
    & = \pr{E_i} \cdot \pr{E_j} \cdot \pr{E_k} \cdot \pr{E_l}
%    & \text{for all distinct $i$, $j$, $k$, $l$}
 \\
    & \ldots \\
\pr{E_1 \intersect \cdots \intersect E_n} & = \pr{E_1} \cdots \pr{E_n}
\end{align*}
%
As an example, if we toss 100 fair coins and let $E_i$ be the event
that the $i$th coin lands heads, then we might reasonably assume that
$E_1, \dots, E_{100}$ are mutually independent.

\section{DNA Testing}

Assumptions about independence are routinely made in practice.
Frequently, such assumptions are quite reasonable.  Sometimes,
however, the reasonableness of an independence assumption is not so
clear, and the consequences of a faulty assumption can be severe.

For example, consider the following testimony from the O. J. Simpson
murder trial on May 15, 1995:
\begin{description}

\item[Mr. Clarke:] When you make these estimations of frequency---and
I believe you touched a little bit on a concept called independence?

\item[Dr. Cotton:] Yes, I did.

\item[Mr. Clarke:] And what is that again?

\item[Dr. Cotton:] It means whether or not you inherit one allele that
you have is not---does not affect the second allele that you might
get.  That is, if you inherit a band at 5,000 base pairs, that doesn't
mean you'll automatically or with some probability inherit one at
6,000.  What you inherit from one parent is what you inherit from the
other.  \emph{(Got that?---EAL)}

\item[Mr. Clarke:] Why is that important?

\item[Dr. Cotton:] Mathematically that's important because if that
were not the case, it would be improper to multiply the frequencies
between the different genetic locations.

\item[Mr. Clarke:] How do you---well, first of all, are these markers
independent that you've described in your testing in this case?

\end{description}

The jury was told that genetic markers in blood found at the crime
scene matched Simpson's.  Furthermore, they were told that the
probability that the markers would be found in a randomly-selected
person was at most 1 in 170 million.  This astronomical figure was
derived from statistics such as:
%
\begin{itemize}
\item 1 person in 100 has marker $A$.
\item 1 person in 50 marker $B$.
\item 1 person in 40 has marker $C$.
\item 1 person in 5 has marker $D$.
\item 1 person in 170 has marker $E$.
\end{itemize}
%
Then these numbers were multiplied to give the probability that a
randomly-selected person would have all five markers:
%
\begin{align*}
\pr{A \intersect B \intersect C \intersect D \intersect E}
    & = \pr{A} \cdot \pr{B} \cdot \pr{C} \cdot \pr{D} \cdot \pr{E} \\[2pt]
    & = \frac{1}{100} \cdot \frac{1}{50} \cdot \frac{1}{40}
                      \cdot \frac{1}{5} \cdot \frac{1}{170} \\[2pt]
    & = \frac{1}{170,000,000}
\end{align*}
%
The defense pointed out that this assumes that the markers appear
mutually independently.  Furthermore, all the statistics were based on
just a few hundred blood samples.  The jury was widely mocked for
failing to ``understand'' the DNA evidence.  If you were a juror,
would \emph{you} accept the 1 in 170 million calculation?

\section{Pairwise Independence}

The definition of mutual independence seems awfully
complicated---there are so many conditions!  Here's an example that
illustrates the subtlety of independence when more than two events are
involved and the need for all those conditions.  Suppose that we flip
three fair, mutually-independent coins.  Define the following events:
%
\begin{itemize}
\item $A_1$ is the event that coin 1 matches coin 2.
\item $A_2$ is the event that coin 2 matches coin 3.
\item $A_3$ is the event that coin 3 matches coin 1.
\end{itemize}
%
Are $A_1$, $A_2$, $A_3$ mutually independent?

The sample space for this experiment is:
%
\[
    \set{HHH,\, HHT,\, HTH,\, HTT,\, THH,\, THT,\, TTH,\, TTT}.
\]
%
Every outcome has probability $(1/2)^3 = 1/8$ by our assumption that
the coins are mutually independent.

To see if events $A_1$, $A_2$, and $A_3$ are mutually independent, we
must check a sequence of equalities.  It will be helpful first to
compute the probability of each event $A_i$:
%
\begin{align*}
\pr{A_1} & = \pr{HHH} + \pr{HHT} + \pr{TTH} + \pr{TTT} \\[2pt]
         & = \frac{1}{8} + \frac{1}{8} + \frac{1}{8} + \frac{1}{8}\\[2pt]
         & = \frac{1}{2}
\end{align*}
%
By symmetry, $\pr{A_2} = \pr{A_3} = 1/2$ as well.  Now we can begin
checking all the equalities required for mutual independence.
%
\begin{align*}
\pr{A_1 \intersect A_2}
	& = \pr{HHH} + \pr{TTT} \\[2pt]
        & = \frac{1}{8} + \frac{1}{8} \\[2pt]
        & = \frac{1}{4} \\[2pt]
        & = \frac{1}{2} \cdot \frac{1}{2}\\[2pt]
        & = \pr{A_1} \pr{A_2}
\end{align*}
%
By symmetry, $\pr{A_1 \intersect A_3} = \pr{A_1} \cdot \pr{A_3}$ and
$\pr{A_2 \intersect A_3} = \pr{A_2} \cdot \pr{A_3}$ must hold also.
Finally, we must check one last condition:
%
\begin{align*}
\pr{A_1 \intersect A_2 \intersect A_3}      & = \pr{HHH} + \pr{TTT} \\[2pt]
                                & = \frac{1}{8} + \frac{1}{8} \\[2pt]
                                & = \frac{1}{4} \\[2pt]
                                & \neq \pr{A_1} \pr{A_2} \pr{A_3} = \frac{1}{8}
\end{align*}
%
The three events $A_1$, $A_2$, and $A_3$ are not mutually independent
even though any two of them are independent!  This not-quite
mutual independence seems weird at first, but it happens.   It even generalizes:

\begin{definition}\label{kway_independent_events}
  A set $A_1$, $A_2$, \dots, of events is \term{$k$-way independent}
  iff every set of $k$ of these events is mutually independent.  The
  set is \term{pairwise independent} iff it is 2-way independent.
\end{definition}

So the sets $A_1$, $A_2$, $A_3$ above are pairwise independent, but
not mutually independent.  Pairwise independence is a much weaker
property than mutual independence.

For example, suppose that the prosecutors in the
O. J. Simpson trial were wrong and markers $A$, $B$, $C$, $D$, and $E$
appear only \emph{pairwise} independently.  Then the probability
that a randomly-selected person has all five markers is no more than:
%
\begin{align*}
\pr{A \intersect B \intersect C \intersect D \intersect E}
    & \leq \pr{A \intersect E} \\[2pt]
    & = \pr{A} \cdot \pr{E} \\[2pt]
    & = \frac{1}{100} \cdot \frac{1}{170} \\[2pt]
    & = \frac{1}{17,000}
\end{align*}
%
The first line uses the fact that $A \intersect B \intersect C \intersect
D \intersect E$ is a subset of $A \intersect E$.  (We picked out the $A$
and $E$ markers because they're the rarest.)  We use pairwise independence
on the second line.  Now the probability of a random match is 1 in
17,000---a far cry from 1 in 170 million!  And this is the strongest
conclusion we can reach assuming only pairwise independence.

On the other hand, the 1 in 17,000 bound that we get by assuming
pairwise independence is a lot better than the bound that we would
have if there were no independence at all.  For example, if the
markers are dependent, then it is possible that
\begin{quote}
everyone with marker~$E$ has marker~$A$,

everyone with marker~$A$ has marker~$B$,

everyone with marker~$B$ has marker~$C$, and

everyone with marker~$C$ has marker~$D$,
\end{quote}
In such a scenario, the probability of a match is
\begin{equation*}
    \pr{E} = 1/170.
\end{equation*}

So a stronger independence assumption leads to a smaller bound on the
probability of a match.  The trick is to figure out what independence
assumption is reasonable.  Assuming that the markers are
\emph{mutually} independent may well \emph{not} be reasonable unless
you have examined hundred of millions of blood samples.  Otherwise,
how would you know that marker~$D$ does not show up more frequently
whenever the other four markers are simultaneously present?

We will conclude our discussion of independence with one final, and
somewhat famous, example known as the Birth Paradox.

\begin{problems}
\classproblems
\pinput{CP_three_fair_coins}
\end{problems}


\section{The Birthday Paradox}\label{birthday_principle_sec}

Suppose that there are 100 students in a class.  What is the
probability that some birthday is shared by two people?  Comparing 100
students to the 365 possible birthdays, you might guess the
probability lies somewhere around~$1/3$---but you'd be wrong: the
probability that there will be two people in the class with matching
birthdays is actually~$0.999999692\dots$.  In other words, the
probability that all 100 birthdays are different is less than 1
in~3,000,000.

Why is this probability so small?  The answer involves a phenomenon
known as the \term{Birthday Paradox} (or the \term{Birthday
  Principle}), which is surprisingly important in computer science, as
we'll see later.

Before delving into the analysis, we'll need to make some modeling
assumptions:
\begin{itemize}

\item
For each student, all possible birthdays are equally likely.  The idea
underlying this assumption is that each student's birthday is
determined by a random process involving parents, fate, and, um, some
issues that we discussed earlier in the context of of graph theory.
The assumption is not completely accurate, however; a disproportionate
number of babies are born in August and September, for example.

\item
Birthdays are mutually independent.  This isn't perfectly accurate
either.  For example, if there are twins in the class, then their
birthdays are surely not independent.

\end{itemize}
We'll stick with these assumptions, despite their limitations.  Part
of the reason is to simplify the analysis.  But the bigger reason is
that our conclusions will apply to many situations in computer science
where twins, leap days, and romantic holidays are not considerations.
After all, whether or not two items collide in a hash table really has
nothing to do with human reproductive preferences.  Also, in pursuit
of generality, let's switch from specific numbers to variables.  Let
$m$~be the number of people in the room, and let $N$~be the number of
days in a year.

We can solve this problem using the standard fours-step method.
However, a tree diagram will be of little value because the sample
space is so enormous.  This time we'll have to proceed without the
visual aid!

\paragraph{Step 1: Find the Sample Space}

Let's number the people in the room from 1 to~$m$.  An outcome of the
experiment is a sequence $(b_1, \dots, b_m)$ where $b_i$~is the
birthday of the $i$th person.  The sample space is the set of all such
sequences:
\begin{equation*}
    \sspace = \{\, (b_1, \dots, b_m) \suchthat b_i \in \set{1, \dots
      N} \,\}.
\end{equation*}

\paragraph{Define Events of Interest}

Our goal is to determine the probability of the event~$A$, in which
some two people have the same birthday.  This event is a little
awkward to study directly, however.  So we'll use a common trick,
which is to analyze the \term{complementary} event~$\setcomp{A}$,
in which all $m$~people have different birthdays:
\begin{equation*}
    \setcomp{A} = \set{\, (b_1, \dots, b_m) \in \sspace
                    \suchthat \text{all $b_i$ are distinct} \,}.
\end{equation*}
If we can compute $\pr{\setcomp{A}}$, then we can compute what
really want, $\pr{A}$, using the identity
\begin{equation*}
    \pr{A} + \pr{\setcomp{A}} = 1.
\end{equation*}

\paragraph{Assign Outcome Probabilities}

We need to compute the probability that $m$~people have a particular
combination of birthdays ~$(b_1, \dots, b_m)$.  There are $N$~possible
birthdays and all of them are equally likely for each student.
Therefore, the probability that the $i$th person was born on day~$b_i$
is~$1/N$.  Since we're assuming that birthdays are mutually
independent, we can multiply probabilities.  Therefore, the
probability that the first person was born on day~$b_i$, the second
on~$b_2$, and so forth is~$(1/N)^m$.  This is the probability of every
outcome in the sample space, which means that the sample space is
uniform.  That's good news, because, as we have seen, it means that
the analysis will be simpler.

\paragraph{Compute Event Probabilities}

We're interested in the probability of the event~$\setcomp{A}$ in
which everyone has a different birthday:
\begin{equation*}
    \setcomp{A} = \set{\, (b_1, \dots, b_n) \suchthat
                            \text{all $b_i$ are distinct} \,}.
\end{equation*}
This is a gigantic set.  In fact, there are $N$~choices for~$b_i$,
\ $N - 1$ choices for~$b_2$, and so forth.  Therefore, by the
Generalized Product Rule,
\begin{equation*}
\card{\setcomp{A}}
    = \frac{N!}{(N - m)!}
    = N (N - 1) (N - 2) \cdots (N - m + 1).
\end{equation*}
Since the sample space is uniform, we can conclude that
\begin{equation}\label{eqn:15E4}
\pr{\bar{A}}
    = \frac{\card{\bar{A}}}{N^m} \\
    = \frac{N!}{N^m (N - m)!}.
\end{equation}
We're done!

Or are we?  While correct, it would certainly be nicer to have a
closed-form expression for Equation~\ref{eqn:15E4}.  That means
finding an approximation for $N!$ and~$(N - m)!$.  But this is what we
learned how to do in Chapter~\ref{asymptotics_chap}.  In fact, from
Theorem~\ref{thm:stirling}, we know that
\begin{equation}\label{eqn:15E7}
    n! = \sqrt{2\pi n} \paren{\frac{n}{e}}^n e^{a_n}
\end{equation}
where
\begin{equation*}
    \frac{1}{12n + 1} \le a_n \le \frac{1}{12n}
\end{equation*}

\dmj{I changed from $e^x$ to $\exp(x)$ because the sub-subscripts were
  becoming illegible.}
Plugging Equation~\ref{eqn:15E7} in for $N!$ and~$(N - m)!$ in
Equation~\ref{eqn:15E4} and simplifying yields a closed-form
expression for the probability that all $m$~birthdays are different:
\begingroup
\openup4pt
\begin{align}
\pr{\setcomp{A}}
    &= \frac{N!}{N^m (N - m)!} \notag\\
    &= \frac{     \sqrt{2\pi N} \paren{\frac{N}{e}}^N \exp(a_n) }
            { N^m \sqrt{2\pi (N-m)} \paren{\frac{N-m}{e}}^{N-m}
              \exp(a_{N - m}) } \notag\\
     &= \sqrt{\frac{N}{N - m}}
         \frac{ \exp(N \ln(N) - N + a_n )}
              { \exp(m \ln(N)) \exp((N - m) \ln(N - m) - (N - m) + a_{N - m})}
        \notag\\
     &= \sqrt{\frac{N}{N - m}}
         \exp((N - m) \ln(N) - (N - m) \ln (N - m) - m + a_n - a_{N - m} ) \notag\\
       &= \sqrt{\frac{N}{N - m}}
          \exp\left( (N - m) \ln\left(\frac{N}{N - m}\right) - m + a_n
          - a_{N - m} \right) \label{eqn:15E9}
\end{align}
\endgroup

We can  now evaluate Equation~\ref{eqn:15E9} for $m = 100$ and $N =
365$ to find that the probability that all 100 birthdays are different
is\footnote{The contribution of $a_N$ and~$a_{N - m}$ is so small that
  it is lost in the \dots after 3.07.}
\begin{equation*}
    3.07\ldots \cdot 10^{-7}.
\end{equation*}

We can also plug in other values of~$m$ to find the number of people
needed for the probability of a matching birthday to be about~$1/2$.
In particular, for $m = 23$ and $N = 365$, Equation~\ref{eqn:15E9}
reveals that the probability that all the birthdays differ is
0.493\dots.  So if you are in a room with 23 other people, the
probability that some pair of people share a birthday will be a little
better than~$1/2$.  It is because 23 seems such a small number of
people for a match that the phenomenon is called the \term{Birthday
  Paradox}.

\section{Applications to Hashing}

Hashing is frequently used in computer science to map large strings of
data into short strings of data.  In a typical scenario, you have a
set of $m$~items and you would like to assign each item to a number
from 1 to~$N$ where no pair of items is assigned to the same number
and $N$~is as small as possible.  For example, the items might be
messages, addresses, or variables.  The numbers might represent
storage locations, devices, indices, or digital signatures.

If two items are assigned to the same number, then a \term{collision}
is said to occur.  Collisions are generally bad.  For example,
collisions can correspond to two variables being stored in the same
place or two messages being assigned the same digital signature.  In
fact, finding collisions is a common technique in breaking
cryptographic codes.  Such techniques are often referred to as
\term{birthday attacks}.

In practice, the assignment of a number to an item is done using a
hash function
\begin{equation*}
    h: S \to [1, N]
\end{equation*}
where $S$~is the set of items and $m = \card{S}$.  Typically, the
values of~$h(S)$ are assumed to be uniformly selected from~$[1, N]$
and to be mutually independent.

For efficiency purposes, it is generally desirable to make~$N$ as
small as possible to accommodate the hashing of $m$~items without
collisions.  Ideally, $N$~would be only a little larger than~$m$.
Unfortunately, this is not possible for random hash functions.  To see
why, let's take a closer look at Equation~\ref{eqn:15E9}.

Using the Taylor Series expansion for
\begin{equation*}
    \ln(1 - x) = -x - \frac{x^2}{2} - \frac{x^3}{3} - \cdots
\end{equation*}
in Equation~\ref{eqn:15E9}, we find that
\begingroup
\openup2pt
\begin{align*}
\lefteqn{(N - m) \ln\left(\frac{N}{N - m}\right) - m}\qquad &\\
    &= -(N - m) \ln\left(\frac{N - m}{N}\right) - m \\
    &= -(N - m) \ln \left(1 - \frac{m}{N}\right) - m \\
    &= -(N - m)
        \left(-\frac{m}{N} - \frac{m^2}{2N^2} - \frac{m^3}{3N^3} -
                \cdots \right)
        - m \\
    &= \left( m + \frac{m^2}{2N} + \frac{m^3}{3N^2} + \cdots \right)
     - \left( \frac{m^2}{N} + \frac{m^3}{2N^2} + \frac{m^4}{3N^3} +
              \cdots \right)
     - m \\
    &= - \frac{m^2}{2N} - \frac{m^3}{6N^2} - \frac{m^4}{12N^3} - \cdots.
\end{align*}
Hence, for $m = o(N^{2/3})$,
\begin{align*}
\pr{\setcomp{A}}
    &= \sqrt{\frac{N}{N - m}}
        \exp\left(-\frac{m^2}{2N} - \frac{m^3}{6N^2} -
                  \frac{m^4}{12N^3} - \cdots - a_N - a_{N - m}
                  \right)\\
    &\sim e^{-m^2/2N}.
\end{align*}
\endgroup

This means that if
\begin{align*}
m   &= \sqrt{2 \ln(2)} \sqrt{N} \\
    &= 1.177\ldots \sqrt{N}, \\
\end{align*}
then $\pr{A} \sim 1/2$ and there will be a collision with probability
near~$1/2$.

In other words, $N$ needs to grow quadratically with~$m$ in order to
avoid collisions.  This unfortunate fact is known as the
\term{Birthday Principle} and it limits the efficiency of hashing in
practice---either $N$~is quadratic in the number of items being hashed
or you need to be able to deal with collisions.

\problemsection

\endinput
