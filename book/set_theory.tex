\newcommand{\nohalt}{\text{\small{No-halt}}}
\newcommand{\asciibet}{\text{ASCII}}
\newcommand{\asciistr}{\strings{\asciibet}}

\chapter{Infinite Sets}\label{infinite_chap}\label{set_theory_chap}

This chapter is about \idx{infinite} sets and some challenges in
proving things about them.

Wait a minute!  Why bring up infinity in a Mathematics for
\emph{Computer Science} text?  After all, any data set in a computer
is limited by the size of the computer's memory, and there is a bound
on the possible size of computer memory, for the simple reason that
the universe is (or at least appears to be) bounded.  So why not stick
with \emph{finite} sets of some large, but bounded, size?  This
is a good question, but let's see if we can persuade you that dealing
with infinite sets is inevitable.

\iffalse We've run into a lot of computer science students who wonder
why they should care about infinite sets.  They point out that
\fi

You may not have noticed, but up to now you've already accepted the
routine use of the integers, the rationals and irrationals, and
sequences them---infinite sets, all.  Further, do you really want
Physics or the other sciences to give up the real numbers on the
grounds that only a bounded number of bounded measurements can be made
in a bounded universe?  It's pretty convincing---and a lot
simpler---to ignore such big and uncertain bounds (the universe seems
to be getting bigger all the time) and accept theories using real
numbers.

Likewise in computer science, it simply isn't plausible that writing a
program to add nonnegative integers with up to as many digits as, say,
the stars in the sky---billions of galaxies each with billions of
stars)---would be meaninfully different from writing a program that
would add \emph{any} two integers, no matter how many digits they had.
The same is true in designing a compiler: it's neither useful nor
sensible to make use of the fact that in a bounded universe, only a
bounded number of programs will ever be compiled.

\begin{editingnotes}
That's why basic programming data types like integers or
strings, for example, are defined without imposing any bound on the
sizes of data items.  For example, each datum of type
\idx{\texttt{string}} consists of characters from a finite alphabet
and has a finite length, but the data type definition does not require
that there be bound on the sizes of these finite numbers.  So we
accept the fact that conceptually there are an infinite number of item
of data type \texttt{string}---even though in any given
implementation, storage limits would impose overflow bounds.

When we then consider string procedures of type
\idx{\texttt{string-->string}}, not only are there an infinite number
of such procedures, but each procedure generally behaves differently
on different inputs, so that a single \texttt{string-->string}
procedure may embody an infinite number of behaviors.  In short, an
educated computer scientist can't get around having to cope with
infinite sets.
\end{editingnotes}

Infinite sets also provide a nice setting to practice proof methods,
because it's harder to sneak in unjustified steps under the guise of
intuition.  And there has been a truly astonishing outcome of studying
infinite sets.  It led to the discovery of widespread logical limits
on what computers can possibly do.  For example, in
section~\ref{halting_sec}, we'll use reasoning developed for infinite
sets to prove that it's impossible to have a perfect type-checker for
a programming language.

So in this chapter, we ask you to bite the bullet and start learning to
cope with infinity.

\iffalse 
But as a warmup, we'll first examine some basic properties of
\emph{finite} sets.
\fi

\section{Infinite Cardinality}\label{infinite_sec}

In the late nineteenth century, the mathematician Georg Cantor was
studying the convergence of Fourier series and found some series that
he wanted to say converged ``most of the time,'' even though there
were an infinite number of points where they didn't converge.  As a
result, Cantor needed a way to compare the size of infinite sets.  To
get a grip on this, he got the idea of extending the Mapping Rule
Theorem~\ref{maprul_thm} to infinite sets: he regarded two infinite
sets as having the ``same size'' when there was a bijection between
them.  Likewise, an infinite set $A$ should be considered ``as big
as'' a set $B$ when $A \surj B$.  So we could consider $A$ to be
``strictly smaller'' than $B$, which we abbreviate as $A \strict B$,
when $A$ is \emph{not} ``as big as'' $B$:
\begin{definition}\label{def:strict}
$\qquad A \strict B  \qiff \QNOT(A \surj B)$.
\end{definition}
On finite sets, this $\strict$ relation really does mean ``strictly
smaller.''  This follows immediately from the Mapping
Rule Theorem~\ref{maprul_thm}.
\begin{corollary}\label{cor:strict}
For finite sets $A,B$,
\[
A \strict B  \qiff \card{A} < \card{B}. 
\]
\end{corollary}

\begin{proof}
\begin{align*}
A \strict B
  & \qiff \QNOT(A \surj B)
    & \text{(Def~\ref{def:strict})}\\
  & \qiff \QNOT(\card{A} \geq \card{B})
    & \text{(Theorem~\ref{maprul_thm}.\eqref{sur_ge_fincard})}\\
  & \qiff \card{A} < \card{B}.
\end{align*}
\end{proof}
Cantor got diverted from his study of Fourier series by his effort to
develop a theory of infinite sizes based on these ideas.  His theory
ultimately had profound consequences for the foundations of
mathematics and computer science.  But Cantor made a lot of enemies in
his own time because of his work: the general mathematical community
doubted the relevance of what they called ``\idx{Cantor's paradise}''
of unheard-of infinite sizes.

A nice technical feature of Cantor's idea is that it avoids the need
for a definition of what the ``size'' of an infinite set might
be---all it does is compare ``sizes.''

\textbf{Warning}: We haven't, and won't, define what the ``size'' of
an infinite set is.  The definition of infinite ``sizes'' requires the
definition of some infinite sets called \term{ordinals} with special
well-ordering properties.  The theory of ordinals requires getting
deeper into technical set theory than we want to go, and we can get by
just fine without defining infinite sizes.  All we need are the ``as
big as'' and ``same size'' relations, $\surj$ and $\bij$, between
sets.

But there's something else to watch out for: we've referred to $\surj$
as an ``as big as'' relation and $\bij$ as a ``same size'' relation on
sets.  Of course, most of the ``as big as'' and ``same size''
properties of $\surj$ and $\bij$ on finite sets do carry over to
infinite sets, but \emph{some important ones don't}---as we're about
to show.  So you have to be careful: don't assume that $\surj$ has any
particular ``as big as'' property on \emph{infinite} sets until it's
been proved.

Let's begin with some familiar properties of the ``as big as'' and
``same size'' relations on finite sets that do carry over exactly to
infinite sets:
\begin{lemma}\label{surjinjbij_properties}
For any sets, $A,B,C$,
\begin{enumerate}

\item \label{surjvsinj} $A \surj B$ iff $B \inj A$.

\item \label{bigtrans} If $A \surj B$ and $B \surj C$, then $A \surj
  C$.

\item \label{sametrans} If $A \bij B$ and $B \bij C$, then $A \bij C$.

\item\label{sameABA} $A \bij B$ iff $B \bij A$.
\end{enumerate}
\end{lemma}

Part~\ref{surjvsinj}.\ follows from the fact that $R$ has the $[\le
  1\ \text{out}, \ge 1\ \text{in}]$ surjective function property iff
$\inv{R}$ has the $[\ge 1\ \text{out}, \le 1\ \text{in}]$ total,
injective property.  Part~\ref{bigtrans}.\ follows from the fact that
compositions of surjections are surjections.
Parts~\ref{sametrans}.\ and~\ref{sameABA}.\ follow from the first two
parts because $R$ is a bijection iff $R$ and $\inv{R}$ are surjective
functions.  We'll leave verification of these facts to
Problem~\ref{CP_surj_relation}.

Another familiar property of finite sets carries over to infinite
sets, but this time it's not so obvious:
\begin{theorem}\label{S-B_thm} \mbox{}
 [\idx{Schr\"oder-Bernstein}] For any sets $A,B$, if $A \surj B$ and
 $B \surj A$, then $A \bij B$.
\end{theorem}

That is, the Schr\"oder-Bernstein Theorem says that if $A$ is at least
as big as $B$ and conversely, $B$ is at least as big as $A$, then $A$
is the same size as $B$.  Phrased this way, you might be tempted to
take this theorem for granted, but that would be a mistake.  For
infinite sets $A$ and $B$, the Schr\"oder-Bernstein Theorem is
actually pretty technical.  Just because there is a surjective
function $f:A\to B$---which need not be a bijection---and a surjective
function $g:B \to A$---which also need not be a bijection---it's not
at all clear that there must be a bijection $e:A \to B$.  The idea is
to construct $e$ from parts of both $f$ and $g$.  We'll leave the
actual construction to Problem~\ref{CP_Schroeder_Bernstein_theorem}.

Another familiar set property is that for any two sets, either the
first is at least as big as the second, or vice-versa.  For finite
sets this follows trivially from the Mapping Rule.  It's actually
still true for infinite sets, but assuming it was obvious would be
mistaken again.
  \begin{theorem}\label{surj-comparable}
    For \emph{all} sets $A,B$,
    \[
    A \surj B\quad \QOR\quad  B \surj A.
    \]
  \end{theorem}

Theorem~\ref{surj-comparable} lets us prove that another basic
property of finite sets carries over to infinite ones:
\begin{lemma}\label{strict-transitive}
\begin{equation}\label{AstrBstrC}
A \strict B\ \QAND\ B \strict C
\end{equation}
implies
\[
A \strict C
\]
for all sets $A,B,C$.
\end{lemma}

\begin{proof} (of Lemma~\ref{strict-transitive})

Suppose~\ref{AstrBstrC} holds, and assume for the sake of
contradiction that $\QNOT(A \strict C)$, which means that $A \surj C$.
Now since $B \strict C$, Theorem~\ref{surj-comparable} lets us
conclude that $C \surj B$.  So we have
\[
A \surj C\ \QAND\ C \surj B,
\]
and Lemma~\ref{surjinjbij_properties}.\ref{bigtrans} lets us conclude
that $A \surj B$, contradicting the fact that $A \strict B$.
\end{proof}

We're omitting a proof of Theorem~\ref{surj-comparable} because
proving it involves technical set theory---typically the theory of
\idx{ordinals} again---that we're not going to get into.  But since
proving Lemma~\ref{strict-transitive} is the only use we'll make of
Theorem~\ref{surj-comparable}, we hope you won't feel cheated not to
see a proof.

\subsection{Infinity is different}

A basic property of finite sets that does \emph{not} carry over to
infinite sets is that adding something new makes a set bigger.  That
is, if $A$ is a finite set and $b \notin A$, then $\card{A \union
  \set{b}} = \card{A}+1$, and so $A$ and $A \union \set{b}$ are not
the same size.  But if $A$ is infinite, then these two sets \emph{are}
the same size!

\begin{lemma}\label{AUb}
  Let $A$ be a set and $b \notin A$.  Then $A$ is infinite iff $A \bij
  A \union \set{b}$.
\end{lemma}
\begin{proof}
  Since $A$ is \emph{not} the same size as $A \union \set{b}$ when $A$
  is finite, we only have to show that $A \union \set{b}$ \emph{is}
  the same size as $A$ when $A$ is infinite.

That is, we have to find a bijection between $A \union \set{b}$ and
$A$ when $A$ is infinite.  Here's how: since $A$ is infinite, it
certainly has at least one element; call it $a_0$.  But since $A$ is
infinite, it has at least two elements, and one of them must not equal
to $a_0$; call this new element $a_1$.  But since $A$ is infinite, it
has at least three elements, one of which must not equal $a_0$ or
$a_1$; call this new element $a_2$.  Continuing in this way, we
conclude that there is an infinite sequence
$a_0,a_1,a_2,\dots,a_n,\dots$ of different elements of $A$.  Now it's
easy to define a bijection $e: A \union \set{b} \to A$:
\begin{align*}
e(b) & \eqdef a_0,\\ e(a_n) & \eqdef a_{n+1} &\text{ for } n \in
\naturals,\\ e(a) & \eqdef a & \text{ for } a \in A -
\set{b,a_0,a_1,\dots}.
\end{align*}
\end{proof}

\subsection{Countable Sets}
A set, $C$, is \term{countable} iff its elements can be listed in
order, that is, the elements in $C$ are precisely
\[
c_0, c_1, \dots, c_n, \dots.
\]
This means that if we defined a function, $f$, on the nonnegative
integers by the rule that $f(i) \eqdef c_i$, then $f$ would be a total
surjective function from $\naturals$ to $C$, and would be a bijection
if there were no repeats in the list.  More formally,

\begin{definition}
A set, $C$, is \term{countably infinite} iff\  $\naturals \bij C$.  A
set is \term{countable} iff it is finite or countably infinite.
\end{definition}

For example, the most basic countably infinite set is the set,
$\naturals$, itself.  But the set, $\integers$, of \emph{all} integers
is also countably infinite, because the integers can be listed in the
order,
\begin{equation}\label{intlist}
0,-1,1,-2,2,-3,3,\dots.
\end{equation}
In this case, there is a simple formula for the $n$th element of the
list~\eqref{intlist}.  That is, the bijection $f:\naturals \to
\integers$ such that $f(n)$ is the $n$th element of the list can be
defined as:
\[
f(n) \eqdef \begin{cases} n/2 & \text{if $n$ is even},\\ -(n+1)/2 &
  \text{if $n$ is odd}.
           \end{cases} 
\]    
There is also a simple way to list all \emph{pairs} of nonnegative
integers, which shows that $(\naturals \cross \naturals)$ is also
countably infinite (Problem~\ref{MQ_product_of_countables})..  From
that it's a small step to reach the conclusion that the set,
$\rationals^{\ge 0}$, of nonnegative rational numbers is countable.
This may be a surprise---after all, the rationals densely fill up the
space between integers, and for any two, there's another in between,
so it might seem as though you couldn't write them all out in a list,
but Problem~\ref{CP_rationals_are_countable} illustrates how to do it.
More generally, it is easy to show that countable sets are closed
under unions and products (Problems~\ref{TP_union_two_countable}
and~\ref{MQ_product_of_countables}) which implies the countability of
a bunch of familiar sets:
\begin{corollary}\label{countable_examples}
The following sets are countably infinite:
\[\integers^+, 
 \integers, \naturals \cross \naturals, \rationals^+, \integers \cross
 \integers, \rationals.
\]
\end{corollary}

A small modification of the proof of Lemma~\ref{AUb} shows that
countably infinite sets are the ``smallest'' infinite sets, or more
precisely that if $A$ is an infinite set, and $B$ is countable, then
$A \surj B$ (see Problem~\ref{CP_smallest_infinite_set}).

Since adding one new element to an infinite set doesn't change its
size, it's obvious that neither will adding any \emph{finite} number
of elements.  It's a common mistake to think that this proves that you
can throw in infinitely many new elements.  But just because it's ok
to do something any finite number of times doesn't make it OK to do an
infinite number of times.  For example, starting from 3, you can
increment by 1 any finite number of times and the result will be some
integer greater than or equal to 3.  But if you increment an infinite
number of times, you don't get an integer at all.

The good news is that you really can add a \emph{countably} infinite
number of new elements to an infinite set and still wind up with just
a set of the same size; see Problem~\ref{PS_add_countable_elements}.

\subsection{\idx{Power sets} are \idx{strictly bigger}}

Cantor's astonishing discovery was that \emph{not all infinite sets
  are the same size}.  In particular, he proved that for any set, $A$,
the \idx{power set}, $\power(A)$, is ``\idx{strictly bigger}'' than
$A$.  That is,
\begin{theorem}\label{powbig}[Cantor]\mbox{}
For any set, $A$,
\[
A \strict \power(A).
\]
\end{theorem}
\begin{proof}
\iffalse
  First of all, $\power(A)$ is as big as $A$: for example, the partial
  function $f:\power(A) \to A$, where $f(\set{a}) \eqdef a$ for $a \in
  A$ and $f$ is only defined on one-element sets, is a surjection.
\fi
 
To show that $A$ is strictly smaller than $\power(A)$, we have to show
that if $g$ is a function from $A$ to $\power(A)$, then $g$ is \emph{not} a
surjection.  To do this, we'll simply find a subset, $A_g \subseteq A$
that is not in the range of $g$.  The idea is, for any element $a \in
A$, to look at the set $g(a) \subseteq A$ and ask whether or not $a$
happens to be in $g(a)$.  First, define \iffalse mimicking Russell's
Paradox,\fi
  \[
  A_g \eqdef \set{a \in A \suchthat a \notin g(a)}.
  \]
  $A_g$ is now a well-defined subset of $A$, which means it is a
  member of $\power(A)$.  But $A_g$ can't be in the range of $g$,
  because if it were, we would have
\[
A_g = g(a_0)
\]
for some $a_0 \in A$, so by definition of $A_g$,
\[
a \in g(a_0) \qiff a \in A_g \qiff a \notin g(a)
\]
for all $a \in A$.  Now letting $a = a_0$ yields the contradiction
\[
a_0 \in g(a_0) \qiff a_0 \notin g(a_0).
\]
So $g$ is not a surjection, because there is an element in the power
set of $A$, namely the set $A_g$, that is not in the range of $g$.
\end{proof}

Cantor's Theorem immediately implies:

\begin{corollary}
$\power(\naturals)$ is uncountable.
\end{corollary}

The bijection between subsets of an $n$-element set and the length $n$
bit-strings, $\set{0,1}^n$, used to prove
Theorem~\ref{powset_fincard}, carries over to a bijection between
subsets of a countably infinite set and the infinite bit-strings,
$\set{0,1}^{\omega}$.  That is,
\[
\power(\naturals) \bij \set{0,1}^{\omega}.
\]
This immediately implies
\begin{corollary}
$\set{0,1}^{\omega}$ is uncountable.
\end{corollary}

\subsubsection{Larger Infinities}

There are lots of different sizes of infinite sets.  For example,
starting with the infinite set, $\naturals$, of nonnegative integers,
we can build the infinite sequence of sets
\[
\naturals \strict \power(\naturals) \strict \power(\power(\naturals))
\strict \power(\power(\power(\naturals))) \strict \dots.
\]
By Theorem~\ref{powbig}, each of these sets is strictly bigger than
all the preceding ones.  But that's not all: the union of all the sets
in the sequence is strictly bigger than each set in the sequence (see
Problem~\ref{CP_power_set_tower}).  In this way you can keep going
indefinitely, building ``bigger'' infinities all the way.

\subsection{Diagonal Argument}
Theorem~\ref{powbig} and similar proofs are collectively known as
``\index{diagonal argument}{diagonal arguments}.''  The diagonal
aspect might not be immediately obvious, so here is a more intuitive
version of the proof.  First, suppose there were a bijection between
$\naturals$ and $\set{0,1}^{\omega}$.  If such a relation existed, we
would be able to display it as a list of the infinite bit strings in
some countable order or another.  Once we'd found a viable way
to organize this list, any given string in $\set{0,1}^{\omega}$ would
appear in a finite number of steps, just as any integer you can name
will show up a finite number of steps from 0.  This hypothetical list
would look something like the one below, extending to infinity both
vertically and horizontally:
\[\begin{array}{|rc c c c c c c l|}
\hline
A_0 &=  & 1 & 0 & 0 & 0 & 1 & 1 & \cdots\\
A_1 &=  & 0 & 1 & 1 & 1 & 0 & 1 & \cdots\\
A_2 &=  & 1 & 1 & 1 & 1 & 1 & 1 & \cdots\\
A_3 &=  & 0 & 1 & 0 & 0 & 1 & 0 & \cdots\\
A_4 &=  & 0 & 0 & 1 & 0 & 0 & 0 & \cdots\\
A_5 &=  & 1 & 0 & 0 & 1 & 1 & 1 & \cdots\\
\vdots & & \vdots & \vdots & \vdots & \vdots & \vdots &
         \vdots & \ddots\\
\hline
\end{array}\]
But now we can exhibit a sequence that's missing from our allegedly
complete list of all the sequences.  Look at the diagonal in our
sample list:
\[\begin{array}{|rc c c c c c c l|}
\hline
A_0 &=  & \textcolor{blue}{\mathbf{1}} & 0 & 0 & 0 & 1 & 1 & \cdots \\
A_1 &=  & 0 & \textcolor{blue}{\mathbf{1}} & 1 & 1 & 0 & 1 & \cdots \\
A_2 &=  & 1 & 1 & \textcolor{blue}{\mathbf{1}} & 1 & 1 & 1 & \cdots \\
A_3 &=  & 0 & 1 & 0 & \textcolor{blue}{\mathbf{0}} & 1 & 0 & \cdots \\
A_4 &=  & 0 & 0 & 1 & 0 & \textcolor{blue}{\mathbf{0}} & 0 & \cdots \\
A_5 &=  & 1 & 0 & 0 & 1 & 1 & \textcolor{blue}{\mathbf{1}} & \cdots \\
\vdots & & \vdots & \vdots & \vdots & \vdots & \vdots & \vdots & \ddots\\
\hline
\end{array}\]
Here is why the diagonal argument has its name: we can form a sequence
$\textcolor{blue}{D}$ consisting of the bits on the diagonal.
\[\begin{array}{l c c c c c c l}
\textcolor{blue}{D} = & \textcolor{blue}{1} & \textcolor{blue}{1} &
  \textcolor{blue}{1} & \textcolor{blue}{0} & \textcolor{blue}{0} &
  \textcolor{blue}{1} & \textcolor{blue}{\cdots},
\end{array}\]
Then, we can form another sequence by switching the
$\textcolor{blue}{\mathbf{1}}$'s and $\textcolor{blue}{\mathbf{0}}$'s
along the diagonal.  Call this sequence $\textcolor{red}{C}$:
\[\begin{array}{l c c c c c c l}
\textcolor{red}{C} = & \textcolor{red}{0} & \textcolor{red}{0} &
  \textcolor{red}{0} & \textcolor{red}{1} & \textcolor{red}{1} &
  \textcolor{red}{0} & \textcolor{red}{\cdots}.
\end{array}\]
Now if $n$th term of $A_n$ is $\textcolor{blue}{\mathbf{1}}$ then the
$n$th term of $\textcolor{red}{C}$ is $\textcolor{red}{\mathbf{0}}$,
and \emph{vice versa}, which guarantees that $\textcolor{red}{C}$
differs from $A_n$.  In other words, $\textcolor{red}{C}$ has at least
one bit different from \emph{every} sequence on our list.  So
$\textcolor{red}{C}$ is an element of $\set{0,1}^{\omega}$ that does
not appear in our list---our list can't be complete!

This diagonal sequence $\textcolor{red}{C}$ corresponds to the set
$\set{a \in A \suchthat a \notin g(a)}$ in the proof of
Theorem~\ref{powbig}. Both are defined in terms of a countable subset
of the uncountable infinity in a way that excludes them from that
subset, thereby proving that no countable subset can be as big as the
uncountable set.

\iffalse It shows that any function arranging the elements
of~$\set{0,1}^{\omega}$ into a countable list will necessarily
generate a list that is incomplete.  Elements of the codomain will be
omitted from the range of the function, so~$\QNOT (\naturals \surj
\set{0,1}^{\omega})$ which means $\naturals \strict
\set{0,1}^{\omega}$.\fi

\begin{problems}
\practiceproblems
%\pinput{TP_Inclusion_Exclusion}  used in counting chapter
\pinput{TP_union_two_countable}
\pinput{TP_countable_strings}
\pinput{TP_uncountable_example}
\pinput{TP_countable_injection}  %UNHIDE after 3/19/12

\classproblems
\pinput{CP_finite_strings_of_nonneg}
\pinput{CP_smallest_infinite_set}
\pinput{CP_rationals_are_countable}
\pinput{CP_Schroeder_Bernstein_theorem}
%\pinput{CP_countable_surjection}
\pinput{CP_countable_from_surj}

\homeworkproblems
\pinput{PS_add_countable_elements}
\pinput{PS_unit_interval}

\examproblems
\pinput{MQ_countable_union}
\pinput{MQ_product_of_countables}
\pinput{FP_countable_sets}
\pinput{FP_infinite_binary_sequences}
\pinput{FP_countable_quadratics}

\end{problems}

\section{The Halting Problem}\label{halting_sec}

Although towers of larger and larger infinite sets are at best a
romantic concern for a computer scientist, the \emph{reasoning} that
leads to these conclusions plays a critical role in the theory of
computation.  Diagonal arguments are used to show that lots of
problems can't be solved by computation, and there is no getting
around it.

This story begins with a reminder that having procedures operate on
programs is a basic part of computer science technology.  For example,
\emph{\idx{compilation}} refers to taking any given program text
written in some ``high level'' programming language like Java, C++,
Python, \dots, and then generating a program of low-level instructions
that does the same thing but is targeted to run well on available
hardware.  Similarly, \emph{\idx{interpreters}} or \emph{\idx{virtual
    machines}} are procedures that take a program text designed to be
run on one kind of computer and simulate it on another kind of
computer.  Routine features of compilers involve
``\idx{type-checking}'' programs to ensure that certain kinds of
run-time errors won't happen, and ``optimizing'' the generated
programs so they run faster or use less memory.

The fundamental thing that just can't be done by
computation is a \emph{perfect} job of type-checking, optimizing, or
any kind of analysis of the overall run time behavior of programs.  In
this section, we'll illustrate this with a basic example known as the
\term{Halting Problem}.  The general Halting Problem for some
programming language is, given an arbitrary program, to determine whether
the program will run forever if it is not interrupted.  If the program
does not run forever, it is said to \idx{halt}.  Real programs may
halt in many ways, for example, by returning some final value,
aborting with some kind of error, or by awaiting user input.  But it's
easy to detect when any given program will halt: just run it on a
virtual machine and wait till it stops.  The problem comes when the
given program does \emph{not} halt---you may wind up waiting
indefinitely without realizing that the wait is fruitless.  So how
could you detect that the program does \emph{not} halt?  We will use a
diagonal argument to prove that if an analysis program tries to
recognize the non-halting programs, it is bound to give wrong answers,
or no answers, for an infinite number of the programs it is supposed to be
able to analyze!

To be precise about this, let's call a programming procedure
---written in your favorite programming language\iffalse such as C++, or Java,
or Python\fi---a \term{string procedure} when it is applicable to
strings over a standard alphabet---say, the 256 character ASCII
alphabet.  \iffalse When a string procedure applied to an ASCII string
halts, we'll say the procedure halts on the string.  If it runs
forever, then we'll say does not halt on the string.\fi As a simple
example, you might think about how to write a string procedure that
halts precisely when it is applied to a \term{double letter} ASCII
strings, namely, a string in which every character occurs twice in a
row.  For example, \texttt{aaCC33}, and \texttt{zz++ccBB} are double
letter strings, but \texttt{aa;bb}, \texttt{b33}, and \texttt{AAAAA}
are not.

We'll call a set of strings \term{recognizable} if there is a string
procedure that halts when it is applied to any string in that set and
does not halt when applied to any string not in the set.  For example,
we've just agreed that set of double letter strings is recognizable.

Let $\asciistr$ be the set of (finite) strings of ASCII characters.
There is no harm in assuming that every program can be written using
only the ASCII characters; they usually are.  When a string $s
\in \asciistr$ is actually the ASCII description of some string
procedure, we'll refer to that string procedure as $P_s$.  You can
think of $P_s$ as the result of compiling $s$.\footnote{The string, $s
  \in \asciistr$, and the procedure, $P_s$, have to be distinguished
  to avoid a type error: you can't apply a string to string.  For
  example, let $s$ be the string that you wrote as your program to
  recognize the double letter strings.  Applying $s$ to a string
  argument, say \texttt{aabbccdd}, should throw a type exception; what
  you need to do is compile $s$ to the procedure $P_s$ and then apply
  $P_s$ to \texttt{aabbccdd}.  \iffalse This application should result
  in a halting computation, since \texttt{aabbccdd} is a double letter
  string.\fi } It's technically helpful to treat \emph{every} ASCII
string as a program for a string procedure.  So when a string $s \in
\asciistr$ doesn't parse as a proper string procedure, we'll define
$P_s$ to be some default string procedure---say one that never halts
on any input.  \iffalse So if $s$ is an ill-formed string, $P_S$ will
be a recognizer for the empty set of strings.  \fi

Focusing just on string procedures, the general \term{halting problem}
is to decide, given strings $s$ and $t$, whether or not the procedure
$P_s$ halts when applied to $t$.  We'll show that the general problem
can't be solved by showing that a special case can't be solved,
namely, whether or not $P_s$ applied to $s$ halts.  So, let's define
\begin{definition}\label{nohalt_def}
\begin{equation}\label{nohalt_eqdef}
\nohalt \eqdef \set{s \in \asciistr \suchthat P_s \text{ applied to
    $s$ does not halt}}.
\end{equation}
\end{definition}
We're going to prove
\begin{theorem}\label{nohalt_thm}
\nohalt\ is not recognizable.
\end{theorem}

We'll use an argument just like Cantor's in the proof of
Theorem~\ref{powbig}.

\begin{proof}
For any string $s \in \asciistr$, let
% define $f: \asciistr \to \power(\asciistr)$ so that
$f(s)$ be the set of strings recognized by $P_s$:
\[
f(s) \eqdef \set{t \in \asciistr \suchthat P_s \text{ halts when applied to
    $t$}}.
\]
By convention, we associated a string procedure, $P_s$, with every
string, $s \in \asciistr$, which makes $f$ a total function, and by
definition,
\begin{equation}\label{snohaltsnfs}
s \in \nohalt \QIFF\ s \notin f(s),
\end{equation}
for all strings, $s \in \asciistr$.

Now suppose to the contrary that \nohalt\ was recognizable.  This
means there is some procedure $P_{s_0}$ that recognizes \nohalt, which
is the same as saying that
\[
\nohalt = f(s_0).
\]
Combined with~\eqref{snohaltsnfs}, we get
\begin{equation}\label{sfs0iff}
s \in f(s_0) \qiff s \notin f(s)
\end{equation}
for all $s \in \asciistr$.  Now letting $s = s_0$ in~\eqref{sfs0iff}
yields the immediate contradiction
\[
s_0 \in f(s_0) \qiff s_0 \notin f(s_0).
\]

This contradiction implies that \nohalt\ cannot be recognized by any
string procedure.
\end{proof}

So that does it: it's logically impossible for programs in any
particular language to solve just this special case of the general
Halting Problem for programs in that language.  And having proved that
it's impossible to have a procedure that figures out whether an
arbitrary program halts, it's easy to show that it's impossible to
have a procedure that is a perfect recognizer for \emph{any} overall
run time property.\footnote{The weasel word ``overall'' creeps in here
  to rule out some run time properties that are easy to recognize
  because they depend only on part of the run time behavior.  For
  example, the set of programs that halt after executing at most 100
  instructions is recognizable.}

For example, most compilers do ``static'' \idx{type-checking} at compile
time to ensure that programs won't make run-time type errors.  A program
that type-checks is guaranteed not to cause a run-time type-error.  But
since it's impossible to recognize perfectly when programs won't cause
type-errors, it follows that the type-checker must be rejecting programs
that really wouldn't cause a type-error.  The conclusion is that no
type-checker is perfect---you can always do better!

It's a different story if we think about the \emph{practical}
possibility of writing programming analyzers.  The fact that it's
logically impossible to analyze perfectly arbitrary programs does not
mean that you can't do a very good job analyzing interesting programs
that come up in practice.  In fact, these ``interesting'' programs are
commonly \emph{intended} to be analyzable in order to confirm that
they do what they're supposed to do.

In the end, it's not clear how much of a hurdle this theoretical limitation
implies in practice.  But the theory does provide some perspective
on claims about general analysis methods for programs.  The theory
tells us that people who make such claims either

\begin{itemize}
\item are exaggerating the power (if any) of their methods, perhaps to make a
  sale or get a grant, or

\item are trying to keep things simple by not going into technical
  limitations they're aware of, or

\item perhaps most commonly, are so excited about some useful practical
    successes of their methods that they haven't bothered to think about
    the limitations which must be there.
\end{itemize}

So from now on, if you hear people making claims about having general
program analysis/verification/optimization methods, you'll know they can't
be telling the whole story.

One more important point: there's no hope of getting around this by
switching programming languages.  Our proof covered programs written
in some given programming language like Java, for example, and
concluded that no Java program can perfectly analyze all Java
programs.  Could there be a C++ analysis procedure that successfully
takes on all Java programs?  After all, C++ does allow more intimate
manipulation of computer memory than Java does.  But there is no
loophole here: it's possible to write a virtual machine for C++ in
Java, so if there were a C++ procedure that analyzed Java programs,
the Java virtual machine would be able to do it too, and that's
impossible.  These logical limitations on the power of computation
apply no matter what kinds of programs or computers you use.

\begin{problems}

\classproblems
\pinput{CP_N_to_N_diagonal_argument}
\pinput{CP_power_set_tower}
\pinput{CP_undescribable_language}
%\pinput{CP_recognizable_sets}

\homeworkproblems
\pinput{PS_A_to_B_diagonal_argument}

\begin{editingnotes}
Add problem that the $4^n$ time-bounded halting problem requires time
$2^n$.
\end{editingnotes}

\examproblems
\pinput{MQ_uncountable_infinite_sequences}

\end{problems}

\section{The Logic of Sets}\label{set_logic_sec}%\hyperdef{logic}{sets}

\subsection{\idx{Russell's Paradox}}

Reasoning naively about sets turns out to be risky.  In fact, one of the
earliest attempts to come up with precise axioms for sets in the late
nineteenth century by the logician \index{Frege, Gotlob} Gotlob Frege, was
shot down by a three line argument known as \emph{Russell's
  Paradox}\footnote{Bertrand \term{Russell} was a mathematician/logician
  at Cambridge University at the turn of the Twentieth Century.  He
  reported that when he felt too old to do mathematics, he began to study
  and write about philosophy, and when he was no longer smart enough to do
  philosophy, he began writing about politics.  He was jailed as a
  conscientious objector during World War I.  For his extensive
  philosophical and political writing, he won a Nobel Prize for
  Literature.} which reasons in nearly the same way as the proof of
Cantor's Theorem~\ref{powbig}.  This was an astonishing blow to efforts to
provide an axiomatic foundation for mathematics:

\textbox{
\begin{center}
\large Russell's Paradox
\end{center}

\begin{quote}
Let $S$ be a variable ranging over all sets, and define
\[
W \eqdef \set{S \suchthat S \not\in S}.
\]
So by definition,
\[
S \in W  \mbox{  iff  } S \not\in S,
\]
for every set $S$.  In particular, we can let $S$ be $W$, and obtain
the contradictory result that
\[
W \in W  \mbox{  iff  } W \not\in W.
\]
\end{quote}}

The simplest reasoning about sets crashes mathematics!  Russell and his
colleague Whitehead spent years trying to develop a set theory that was
not contradictory, but would still do the job of serving as a solid
logical foundation for all of mathematics.

Actually, a way out of the paradox was clear to Russell and others at
the time: \emph{it's unjustified to assume that $W$ is a set}.  The
step in the proof where we let $S$ be $W$ has no justification,
because $S$ ranges over sets, and $W$ might not be a set.  In fact, the
paradox implies that $W$ had better not be a set!

But denying that $W$ is a set means we must \emph{reject} the very
natural axiom that every mathematically well-defined collection of
sets is actually a set.  The problem faced by Frege, Russell and their
fellow logicians was how to specify \emph{which} well-defined
collections are sets.  Russell and his Cambridge University colleague
Whitehead immediately went to work on this problem.  They spent a
dozen years developing a huge new axiom system in an even huger
monograph called \emph{Principia Mathematica}, but for all intents and
purposes, their approach failed.  It was so cumbersome no one ever
used it, and it was subsumed by a much simpler, and now widely
accepted, axiomatization of set theory by the logicians Zermelo
and Fraenkel.

\subsection{The ZFC Axioms for Sets}\label{ZFC_sec}

A \emph{formula of \idx{set theory}}\footnote{Technically this is
  called a \term{first-order predicate formula} of set theory} is a
predicate formula that only uses the predicates ``$x = y$'' and ``$x
\in y$.''  The domain of discourse is the collection of sets, and ``$x
\in y$'' is interpreted to mean that $x$ and $y$ are variables that
range over sets, and $x$ is one of the elements in $y$.

It's generally agreed that, using some simple logical deduction rules,
essentially all of mathematics can be derived from some formulas of set theory
called the Axioms of \idx{Zermelo-Fraenkel Set Theory} with Choice (\idx{ZFC}).

For example, since $x$ is a subset of $y$ iff every element of $x$ is
also an element of $y$, here's how we can express $x$ being a subset
of $y$ with a formula of set theory:
\begin{equation}\label{xsubeqy}
(x \subseteq y) \eqdef\ \forall z.\, (z \in x\ \QIMPLIES\ z \in y).
\end{equation}
Now we can express formulas of set theory using ``$x \subseteq y$'' as
an abbreviation for formula~\eqref{xsubeqy}.

We're \emph{not} going to be studying the axioms of ZFC in this text,
but we thought you might like to see them---and while you're at it,
get some practice reading quantified formulas:

\begin{description}

\item[\term{Extensionality}.] Two sets are equal if they have the same
  members.
\[
(\forall z.\; z \in x \QIFF z \in y) \QIMPLIES x = y.
\]

\item[\term{Pairing}.] For any two sets $x$ and $y$, there is a set,
     $\set{x,y}$, with $x$ and $y$ as its only elements:
\[
\forall x,y.\; \exists u.\; \forall z.\;
[z \in u \QIFF (z = x \QOR z = y)]
\]

\item[\index{Union axiom}\emph{Union}.]
The union, $u$, of a collection, $z$, of sets is also a set:
\[
\forall z.\, \exists u.\, \forall x.\; (\exists y.\; x \in y \QAND y \in z) \QIFF x \in u.
\]

\item[\index{Infinity axiom}\emph{Infinity}.]  There is an infinite set.
  Specifically, there is a nonempty set, $x$, such that for any set $y \in
  x$, the set $\set{y}$ is also a member of $x$.

\item[\index{Subset axiom}\emph{Subset}.] Given any set, $x$, and any definable property of sets,
  there is a set containing precisely those elements $y \in x$ that
  have the property.
\[
\forall x.\, \exists z.\, \forall y.\, y \in z \QIFF [y \in x \QAND \phi(y)]
\]
where $\phi(y)$ is any assertion about $y$ definable in the notation
of set theory.

\item[\index{Power Set axiom}\emph{Power Set}.]  All the subsets of a set form another set:
\[
\forall x.\; \exists p.\; \forall u.\: u \subseteq x \QIFF u \in p.
\]

\item[\index{Replacement axiom}\emph{Replacement}.]  Suppose a formula, $\phi$,
  of set theory defines the graph of a function, that is,
\[
\forall x, y, z.\, [\phi(x,y) \QAND \phi(x,z)] \QIMPLIES y = z.
\]
Then the image of any set, $s$, under that function is also a set, $t$.  Namely,
\[
\forall s\, \exists t\, \forall y.\, [\exists x.\, \phi(x,y) \QIFF y \in t].
\]

\item[\index{Foundation axiom}\emph{Foundation}.] 
There cannot be an infinite sequence
\[
\cdots \in x_n \in \cdots \in x_1 \in x_0
\]
of sets each of which is a member of the previous one.  This is equivalent
to saying every nonempty set has a ``member-minimal'' element.  Namely, define
\[
\text{member-minimal}(m, x) \eqdef [m \in x \QAND \forall y \in x.\, y \notin m].
\]
Then the Foundation axiom is
\[
\forall x.\ x \neq \emptyset\ \QIMPLIES\ \exists m.\, \text{member-minimal}(m, x).
\]

\item[\index{Choice axiom}\emph{Choice}.]  Given a set, $s$, whose
  members are nonempty sets no two of which have any element in
  common, then there is a set, $c$, consisting of exactly one element
  from each set in $s$.  The formula is given in
  Problem~\ref{CP_axiom_of_choice_formula}.

\iffalse

\begin{tabbing}
$\exists y \, \forall z \, \forall w \,
 \biggl( ($\=$z \in w \,\QAND\, w \in x) \; \QIMPLIES $\\
\> $\exists v \, \exists u \, \Bigl(\exists t \, \bigr((u \in w \, \QAND \, w \in t)$\=$\;\QAND\; (u \in t \,\QAND\, t \in y)\bigl) $\\
\> \> $\QIFF\; u = v\Bigr) \biggr)$
\end{tabbing}
%\begin{editingnotes}

\[\begin{array}{rlll}
\exists y \forall z \forall w & ( (z \in w \QAND w \in x) \QIMPLIES\\
                              &\quad \exists v \exists u (\exists t
                                           ((u \in w \QAND & w \in t)
                                                              & \QAND (u \in t \QAND t \in y))\\
                                                            &&& \QIFF u = v))
\end{array}\]

%\end{editingnotes}
\fi
\end{description}

\subsection{Avoiding \idx{Russell's Paradox}}

These modern ZFC axioms for set theory are much simpler than the system
Russell and Whitehead first came up with to avoid paradox.  In fact, the
ZFC axioms are as simple and intuitive as Frege's original axioms, with
one technical addition: the Foundation axiom.  Foundation captures the
intuitive idea that sets must be built up from ``simpler'' sets in certain
standard ways.  And in particular, Foundation implies that no set is ever
a member of itself.  So the modern resolution of Russell's paradox goes as
follows: since $S \not \in S$ for all sets $S$, it follows that $W$,
defined above, contains every set.  This means $W$ can't be a set---or it
would be a member of itself.

\begin{problems}
%\practiceproblems

\classproblems
\pinput{CP_set_pairing}
\pinput{CP_axiom_of_choice_formula}
\pinput{CP_foundation_axiom}

\homeworkproblems
\pinput{PS_size_n_set_formula}

\end{problems}

\section{Does All This Really Work?}\label{setsreallywork}

So this is where mainstream mathematics stands today: there is a handful
of ZFC axioms from which virtually everything else in mathematics can be
logically derived.  This sounds like a rosy situation, but there are
several dark clouds, suggesting that the essence of truth in mathematics
is not completely resolved.

%
\begin{itemize}

\item The \idx{ZFC axioms} weren't etched in stone by God.  Instead, they
  were mostly made up by Zermelo, who may have been a brilliant logician,
  but was also a fallible human being---probably some days he forgot his
  house keys.  So maybe \idx{Zermelo}, just like \idx{Frege}, didn't get
  his axioms right and will be shot down by some successor to
  \idx{Russell} who will use his axioms to prove a proposition $P$ and its
  negation $\bar{P}$.  Then math would be broken.  This sounds crazy, but
  after all, it has happened before.

  In fact, while there is broad agreement that the ZFC axioms are capable
  of proving all of standard mathematics, the axioms have some further
  consequences that sound paradoxical.  For example, the
  \idx{Banach-Tarski} Theorem says that, as a consequence of the
  \idx{Axiom of Choice}, a solid ball can be divided into six pieces and
  then the pieces can be rigidly rearranged to give \emph{two} solid balls
  of the same size as the original!

\item Some basic questions about the nature of sets remain unresolved.
  For example, Cantor raised the question whether there is a set whose
  size is strictly between the smallest infinite set, $\naturals$ (see
  Problem~\ref{CP_smallest_infinite_set}), and the strictly larger
  set, $\power(\naturals)$?  Cantor guessed not:

  \textbf{Cantor's \term{Continuum Hypothesis}}: There is no set, $A$,
  such that
  \[
  \naturals \strict A \strict \power(\naturals).
  \]

  The Continuum Hypothesis remains an open problem a century later.
  Its difficulty arises from one of the deepest results in modern Set
  Theory---discovered in part by \idx{G\"odel} in the 1930's and Paul
  \idx{Cohen} in the 1960's---namely, the ZFC axioms are not
  sufficient to settle the Continuum Hypothesis: there are two
  collections of sets, each obeying the laws of \idx{ZFC}, and in one
  collection the Continuum Hypothesis is true, and in the other it is
  false.  Until a mathematician with a deep understanding of sets can
  extend ZFC with persuasive new axioms, the Continuum Hypothesis will
  remain undecided.  \iffalse So settling the Continuum Hypothesis
  some new understanding of what Sets should be to arrive at
  persuasive new axioms that extend ZFC and are strong enough to
  determine the truth of the Continuum Hypothesis one way or the
  other.  \fi
\item But even if we use more or different axioms about sets, there
  are some unavoidable problems.  In the 1930's, \idx{G\"odel} proved
  that, assuming that an axiom system like \idx{ZFC} is
  consistent---meaning you can't prove both $P$ and $\bar{P}$ for any
  proposition, $P$---then the very proposition that the system is
  consistent (which is not too hard to express as a logical formula)
  cannot be proved in the system.  In other words, no \idx{consistent}
  system is strong enough to verify itself.
  
\end{itemize}

\subsection{Large Infinities in Computer Science}

If the romance of different-size infinities and continuum hypotheses
doesn't appeal to you, not knowing about them is not going to limit
you as a computer scientist.  These abstract issues about infinite
sets rarely come up in mainstream mathematics, and they don't come up
at all in computer science, where the focus is generally on
``\idx{countable},'' and often just finite, sets.  In practice, only
logicians and set theorists have to worry about collections that are
``too big'' to be sets.  That's part of the reason that the 19th
century mathematical community made jokes about ``\idx{Cantor's
  paradise}'' of obscure infinities.  But the challenge of
reasoning correctly about this far-out stuff led directly to the
profound discoveries about the logical limits of computation described
in Section~\ref{halting_sec}, and that really is something every
computer scientist should understand.

\endinput
