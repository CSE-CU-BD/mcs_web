\documentclass[problem]{mcs}

\begin{pcomments}
  \pcomment{CP_drug_confidence}
  \pcomment{from: S07.cp13w}
\end{pcomments}

\pkeywords{
probability
confidence
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Problem starts here
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{problem}
	
An \emph{International Journal of Epidemiology} has a policy that they
will only publish the results of a drug trial when there were enough
patients in the drug trial to be sure that the conclusions about the
drug's effectiveness hold at the 95\% confidence level.  The editors of
the Journal reason that under this policy, their readership can be
confident that at most 5\% of the published studies will be mistaken.

Later, the editors are astonished and embarrassed to learn that
\emph{every one} of the 20 drug trial results they published during
the year was wrong.  This happened even though the editors and
reviewers had carefully checked the submitted data, and every one of
the trials was \emph{properly performed and reported} in the published
paper.

The editors thought the probability of this was negligible (namely,
$(1/20)^{20} < 10^{-25})$.  Explain what's wrong with their reasoning and
how it could be that all 20 published studies were wrong.

\begin{solution}
The editors have confused the statistical \emph{confidence
level} with \emph{probability}.  It's a mistake to think that because the
conclusion of \emph{particular} drug trial submitted to the journal holds
at the 95\% confidence level, this means its conclusion is wrong with
probability only 1/20.

The conclusion of the particular submitted drug trial is right or
wrong---period.  An assertion of 95\% confidence means that if very
many trails were carried out, we expect that close to 95\% of the
trials would yield a correct conclusion.  So if the results of all the
many trials were all submitted for publication, and the editors
selected 20 of these at random to publish, then they could reasonably
expect that only one of them would be wrong.

But that's not what happens: not all the trials are written up and
submitted, so the confidence level of the trial is not specially
relevant.  For example, there may be more than 400 worthless
``alternative'' drugs being tried by proponents who are genuinely
honest, even if misguided.  When they conduct careful trials with a
95\% confidence level, we can expect that in 1/20 of the 400 trials,
worthless---even damaging---drugs will look helpful.  The remaining
19/20 of the 400 trials would not be submitted for publication by
honest proponents because the trials did not show positive results at
the 95\% level.  But the 20 that mistakenly showed positive results
might well all be submitted with no intention to mislead.

This is why, unless there is an explanation of \emph{why} a therapy works,
scientists and doctors usually doubt results claiming to confirm the
efficacy of some mysterious therapy at a high confidence level.
\end{solution}

\end{problem}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Problem ends here
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\endinput
