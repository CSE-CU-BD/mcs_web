\documentclass[problem]{mcs}

\begin{pcomments}
  \pcomment{CP_drug_confidence}
  \pcomment{from: S07.cp13w}
\end{pcomments}

\pkeywords{
probability
confidence
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Problem starts here
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{problem}
	
An \emph{International Journal of Epidemiology} has a policy of
publishing papers about drug trial results only if the conclusion
about the drug's effectiveness (or lack thereof) holds at the 95\%
\idx{confidence level}.  The editors and reviewers carefully check
that any trial whose results they publish was \emph{properly performed
  and accurately reported}.  They are also careful to check that
trials whose results they publish have been conducted independently of
each other.

The editors of the Journal reason that under this policy, their
readership can be confident that at most 5\% of the published studies
will be mistaken.  Later, the editors are embarrassed ---and
astonished ---to learn that \emph{every one} of the 20 drug trial
results they published during the year was wrong.  The editors thought
that because the trials were conducted independently, the probability
of publishing 20 wrong results was negligible, namely, $(1/20)^{20} <
10^{-25}$.

Write a brief explanation to these befuddled editors explaining what's
wrong with their reasoning and how it could be that all 20 published
studies were wrong.

\hint xkcd comic: ``significant'' \href{http://xkcd.com/882/}{\texttt{xkcd.com/882/}}

\begin{solution}
The editors have confused the statistical \emph{confidence
level} with \emph{probability}.  It's a mistake to think that because the
conclusion of \emph{particular} drug trial submitted to the journal holds
at the 95\% confidence level, this means its conclusion is wrong with
probability only 1/20.

The conclusion of the particular submitted drug trial is right or
wrong ---period.  An assertion of 95\% confidence means that if very
many trails were carried out, we expect that close to 95\% of the
trials would yield a correct conclusion.  So if the results of all the
many trials were all submitted for publication, and the editors
selected 20 of these at random to publish, then they could reasonably
expect that only one of them would be wrong.

But that's not what happens: not all the trials are written up and
submitted.  For example, there may be more than 400 worthless
``alternative'' drugs being tried by proponents who are genuinely
honest, even if misguided.  When they conduct careful trials with a
95\% confidence level, we can expect that in 1/20 of the 400 trials,
worthless ---even damaging ---drugs will look helpful.  The remaining
19/20 of the 400 trials would not be submitted for publication by
honest proponents because the trials did not show positive results at
the 95\% level.  But the 20 that mistakenly showed positive results
might well all be submitted with no intention to mislead.

This is why, unless there is an explanation of \emph{why} a therapy works,
scientists and doctors usually doubt results claiming to confirm the
efficacy of some mysterious therapy at a high confidence level.
\end{solution}

\end{problem}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Problem ends here
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\endinput
