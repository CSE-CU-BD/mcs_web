\documentclass[problem]{mcs}

\begin{pcomments}
  \pcomment{FP_chebyshev_pq}
  \pcomment{generalizes TP_Flipping_coins}
  \pcomment{ARM 5/11/16}
  \pcomment{unused last part good for CP?}
\end{pcomments}

\begin{problem}
You have a biased coin which flips Heads with probability $p$.  You
flip the coin $n$ times.  The coin flips are all mutually independent.

\bparts

\ppart\label{expected} Write a simple expression in terms of $p$ and
$n$ for the expected number of Heads.

\begin{center}
\exambox{0.7in}{0.5in}{0in}
\end{center}

\begin{solution}
$\mathbf{np}$.

\begin{staffnotes}
Not needed for full credit:

Let $X$ denote the number of Heads.  Let $X_{i}$ be the indicator
variable that is 1 if and only if the $i$th coin flip comes out Heads
(and 0 otherwise).  Then
\begin{equation*}
    X = X_{1} + X_{2} + \dots + X_{n}.
\end{equation*}

Hence, by linearity of expectation, 
\[
\expect{X}
    = \expect{X_{1} + X_{2} + \cdots + X_{n}}
    = \expect{X_{1}} + \cdots + \expect{X_{n}}.
\]
The expectation of an indicator variable is the probability it
equals~1\inbook{ by Lemma~\bref{expindic}}.  Hence, $\expect{X_{i}} =
p$.  We conclude that $\expect{X} = n \cdot p$.
\end{staffnotes}
\end{solution}

\ppart Write a simple expression in terms of $p$ and $n$ for the
variance of the number of Heads.

\begin{center}
\exambox{0.7in}{0.5in}{0in}
\end{center}

\begin{solution}
$\mathbf{np(1-p)}$.

\begin{staffnotes}
Not needed for full credit:

By the independence of the $X_{i}$, we know
\[
\Var[X] = 
\Var[X_{1} + \cdots  + X_{n}] = 
\Var[X_{1}]+ \cdots  + \Var[X_{n}].
\]
Finally, we know \inbook{by Corollary~\bref{bernoulli-variance}} the
variance of an indicator with expectation $p$ is $p(1-p)$.
\end{staffnotes}
\end{solution}

\ppart Write a simple expression in terms of $p$ for the upper bound
that Markov's Theorem gives for the probability that the number of
Heads is larger than expected by as much as 1\% of the number of
flips.

\begin{center}
\exambox{0.7in}{0.5in}{0in}
\end{center}

\examspace[1.0in]

\begin{solution}
\[
\frac{100p}{100p+1}.
\]

The Markov bound on the probability of being at least $a\mu$ is $1/a$.  So we want
\[
a(pn) = pn + \frac{n}{100}.
\]
Solving for $a$ gives
\[
a = \frac{100p+1}{100p}.
\]
\end{solution}

\ppart\label{C25bnd} Show that the bound Chebyshev's Theorem gives for
the probability that the number of Heads differs from the expected
number by at least 1\% of the number of flips is
\[
100^2\frac{p(1-p)}{n}.
\]

\examspace[1.0in]

\begin{solution}
Chebyshev's Theorem says that the probability that $X$ deviates
from its expected value by at least $n/100$ is at most
\[
\frac{\variance {X}}{(n/100)^2} = \frac{np(1-p)}{(n/100)^2} %= 100^2\frac{p(1-p)}{n}.
\]
Simplifying the right hand expresssion gives the stated bound.
\end{solution}

\ppart The bound in part~\eqref{C25bnd} implies that if you flip $\geq
m$ times for a certain number $m$, the average of the number of Heads
you flip will have at least a 95\% chance of being within 0.01 of $p$.
Write a simple expression for $m$ in terms of $p$.

\begin{center}
\exambox{0.7in}{0.5in}{0in}
\end{center}

\begin{solution}
$\mathbf{500 p(1-p)}$

The average number Heads is within 0.01 of $p$ iff the total number,
$n$, of flips in with $n/100$ of the expectation $pn$.  So letting $u
\eqdef$ the answer to part~\eqref{C25bnd}.  We need
\[
u \leq \frac{1}{20}.
\]
Solving for $n$, we get
\[
n \geq \frac{100^2 p(1-p)}{20} = 500 p(1-p).
\]
\end{solution}

\eparts

\end{problem}

\endinput
